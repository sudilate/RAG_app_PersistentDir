{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"//Users//amitsarang//LlamaIndex//data\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='57e1cfe5-510e-43f4-84ad-bcd9fa5eb42d', embedding=None, metadata={'page_label': '1', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative AI and Large Language Models for\\nCyber Security: All Insights You Need\\nMohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah, Bilel Cherif, Abdechakour Mechri,\\nand Norbert Tihanyi\\nAbstract —The rapid evolution of cyber threats requires in-\\nnovative approaches to enhance cybersecurity defenses. In this\\npaper, we provide a comprehensive and in-depth review of the\\nfuture of cybersecurity through the lens of Generative AI and\\nLarge Language Models (LLMs). We explore LLM applications\\nacross various cybersecurity domains, including hardware de-\\nsign security, intrusion detection, software engineering, design\\nverification, cyber threat intelligence, malware detection, and\\nphishing and spam detection. We present a detailed overview of\\nLLM evolution and its current state, focusing on advancements\\nin 42 specific models, such as GPT-4o, GPT-4, GPT-3.5, Mixtral-\\n8x7B, BERT, Falcon2, Gemma, Phi-2, Phi-3, and LLaMA. Our\\nanalysis extends to the vulnerabilities inherent in LLMs, in-\\ncluding prompt injection, insecure output handling, training and\\ninference data poisoning, DDoS attacks, and adversarial natural\\nlanguage instructions. We also delve into mitigation strategies to\\nprotect these models, providing a comprehensive look at potential\\nattack scenarios and prevention techniques. Furthermore, we\\nevaluate the performance of 42 LLM models in cybersecurity\\nknowledge and hardware security, highlighting their strengths\\nand weaknesses. Our study thoroughly evaluates cybersecurity\\ndatasets tailored for LLM training and testing, covering the entire\\nlifecycle from data creation to usage and identifying gaps and\\nopportunities for future research. We discuss the challenges and\\nlimitations of employing LLMs in cybersecurity settings, such as\\ndealing with adversarial attacks and ensuring model robustness.\\nIn addition, we review the new strategies for leveraging LLMs,\\nincluding advanced techniques like Half-Quadratic Quantiza-\\ntion (HQQ), Reinforcement Learning with Human Feedback\\n(RLHF), Direct Preference Optimization (DPO), Odds Ratio\\nPreference Optimization (ORPO), GPT-Generated Unified For-\\nmat (GGUF), Quantized Low-Rank Adapters (QLoRA), and\\nRetrieval-Augmented Generation (RAG). These insights aim to\\nenhance real-time cybersecurity defenses and improve the sophis-\\ntication of LLM applications in threat detection and response.\\nOur paper aims to provide a foundational understanding and\\nstrategic direction for integrating LLMs into future cybersecurity\\nframeworks, emphasizing the importance of innovation and\\nrobust model deployment to safeguard against evolving cyber\\nthreats.\\nIndex Terms —Generative AI, LLM, Transformer, Security,\\nCyber Security.\\nM. A. Ferrag is the corresponding author.\\nM. A. Ferrag is with Technology Innovation Institute, 9639 Masdar City,\\nAbu Dhabi, UAE Email: mohamed.ferrag@tii.ae\\nF. Alwahedi is with Technology Innovation Institute, 9639 Masdar City,\\nAbu Dhabi, UAE Email: fatima.alwahedi@tii.ae\\nA. Battah is with Technology Innovation Institute, 9639 Masdar City, Abu\\nDhabi, UAE Email: ammar.battah@tii.ae\\nB. Cherif is with Technology Innovation Institute, 9639 Masdar City, Abu\\nDhabi, UAE Email: bilel.cherif@tii.ae\\nA. Mechri is with Technology Innovation Institute, 9639 Masdar City, Abu\\nDhabi, UAE Email: abdechakour.mechri@tii.ae\\nN. Tihanyi is with Technology Innovation Institute, 9639 Masdar City, Abu\\nDhabi, UAE Email: norbert.tihanyi@tii.aeLIST OF ABBREVIATIONS\\nAI Artificial Intelligence\\nAIGC Artificial Intelligence Generated Content\\nAPT Advanced Persistent Threat\\nCNN Convolutional Neural Network\\nCTG Controllable Text Generation\\nCVE Common Vulnerabilities and Exposures\\nCWE Common Weakness Enumeration\\nFNN Feed-Forward Neural Network\\nFRR False Refusal Rate\\nGPT Generative Pre-trained Transformers\\nGRU Gated Recurrent Units\\nGQA Grouped-Query Attention\\nHPC High-Performance Computing\\nHLS High-Level Synthesis Design Verification\\nHQQ Half-Quadratic Quantization\\nIDS Intrusion Detection System\\nLLM Large Language Model\\nLoRA Low-rank Adapters\\nLSTM Long Short-Term Memory\\nML Machine Learning\\nMLP Multi-Layer Perceptron\\nMQA Multi-Query Attention\\nNIST National Institute of Standards and Technology\\nNLP Natural Language Processing\\nNLU Natural Language Understanding\\nORPO Odds Ratio Preference Optimization\\nPEFT Parameter Efficient Fine-Tuning\\nPLM Pre-trained Language Model\\nPPO Proximal Policy Optimization\\nRAG Retrieval Augmentation Generation\\nRLHF Reinforcement Learning from Human Feedback\\nRNN Recurrent Neural Networks\\nRTL Register-Transfer Level\\nSARD Software Assurance Reference Dataset\\nSFT Supervised Fine-Tuning\\nSVM Support Vector Machine\\nTRPO Trust Region Policy Optimization\\nI. I NTRODUCTION\\nThe history of Natural Language Processing (NLP) dates\\nback to the 1950s when the Turing test was developed. How-\\never, NLP has seen significant advancements in recent decades\\nwith the introduction of Recurrent Neural Networks (RNN)\\n[1], Long Short-Term Memory (LSTM) [2], Gated Recurrent\\nUnits (GRU) [3], and Transformer methods [4]. RNN was first\\nintroduced in the 1990s to model data sequences. LSTM, a\\n1arXiv:2405.12750v1  [cs.CR]  21 May 2024', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3619a38a-4908-43ce-b338-bfeb5822be85', embedding=None, metadata={'page_label': '2', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='variant of RNN, was introduced in 1997, which addressed\\nthe vanishing gradient problem and allowed for longer-term\\nmemory in NLP models. GRU, another variant of RNN, was\\nintroduced in 2014, which reduced the number of parame-\\nters and improved computational efficiency [5]. The latest\\nbreakthrough in NLP was the introduction of Transformers\\nin 2017, enabling parallel processing of sequential data and\\nrevolutionizing tasks like machine translation. These methods\\nhave significantly improved various NLP tasks, including\\nsentiment analysis, language generation, and translation [4],\\n[6], [7].\\nCybersecurity is an ever-evolving field, with threats becom-\\ning increasingly sophisticated and complex. As organizations\\nand individuals rely on digital technologies for communica-\\ntion, commerce, and critical infrastructure, the need for robust\\ncybersecurity measures has never been greater [8]. The scale\\nand diversity of cyber threats make it a daunting challenge\\nfor security professionals to effectively identify, detect, and\\ndefend against them. In this context, Large Language Models\\n(LLMs) have emerged as a game-changing technology with the\\npotential to enhance cybersecurity practices significantly [9]–\\n[13]. These models, powered by advanced NLP and Machine\\nLearning (ML) techniques, offer a new frontier in the fight\\nagainst cyber threats [14], [15]. This article explores the\\nmotivations and applications of LLMs in cybersecurity.\\nCybersecurity professionals often need to sift through a\\nvast amount of textual data, including security alerts, incident\\nreports, threat feeds, and research papers, to stay ahead of\\nevolving threats. LLMs, like Falcon 180b [16], possess natural\\nlanguage understanding capabilities that enable them to parse,\\nsummarize, and contextualize this information efficiently [7],\\n[17], [18]. They can assist in rapidly identifying relevant\\nthreat intelligence, allowing analysts to make more informed\\ndecisions and prioritize responses [19]. LLMs can excel in\\nvarious domains within cybersecurity. Figure 1 highlights the\\ntop nine use cases and applications for LLMs in this field [20].\\n1)Threat Detection and Analysis: LLMs can analyze\\nvast network data in real-time to detect anomalies and\\npotential threats. They can recognize patterns indicative\\nof cyber attacks, such as malware, phishing attempts,\\nand unusual network traffic [19].\\n2)Security Automation: LLMs can facilitate the automa-\\ntion of routine security tasks such as patch management,\\nvulnerability assessments, and compliance checks. This\\nreduces the workload on cybersecurity teams and allows\\nthem to focus on more complex tasks [9].\\n3)Phishing Detection and Response: LLMs can identify\\nphishing emails by analyzing the text for malicious\\nintent and comparing it to known phishing examples.\\nThey can also generate alerts and recommend preventive\\nactions [21].\\n4)Cyber Forensics: LLMs can help in forensic analysis\\nby parsing through logs and data to determine the cause\\nand method of attack, thus aiding in the recovery process\\nand future prevention strategies [22].\\n5)Penetration Testing: LLMs can help generate scripts\\nor modify existing ones to automate certain parts of\\nthe penetration testing process. This includes scripts forvulnerability scanning, network mapping, and exploiting\\nknown vulnerabilities [23].\\n6)Security Protocols Verification: LLMs can help verify\\nthe security of protocols such as TLS/SSL, IPSec, . . . etc.\\n7)Incident Response: During a cybersecurity incident,\\nLLMs can assist by providing rapid analysis of the sit-\\nuation, suggesting mitigation strategies, and automating\\nresponses where applicable [24].\\n8)Chatbots: LLMs significantly enhance the capabilities\\nof chatbots in cybersecurity environments by providing\\nUser Interaction, Incident Reporting and Handling, Real-\\ntime Assistance, Training and Simulations, and FAQ\\nAutomation [25].\\n9)Security Training and Awareness: LLMs can generate\\ntraining materials tailored to an organization’s needs.\\nThey can also simulate phishing attacks and other se-\\ncurity scenarios to train employees to recognize and\\nrespond to security threats [26].\\nThreat Detection\\nand Analysis\\nSecurity\\nAutomation\\nPhishing Detection\\nand Response\\nCyber Forensics\\nPenetration T esting\\n Security Protocols\\nVerification\\nIncident Response\\nChatbots\\nSecurity T raining\\nand Awareness\\nLarge Language Models for Nine Cybersecurity Use Cases\\nFig. 1: LLM Use Cases And Applications for Cybersecurity.\\nThe primary aim of this paper is to provide an in-depth\\nand comprehensive review of the future of cybersecurity using\\nGenerative AI and LLMs, covering all relevant topics in the\\ncyber domain. The contributions of this study are summarized\\nbelow:\\n•We review LLMs’ applications for cybersecurity use\\ncases, such as hardware design security, intrusion de-\\ntection, software engineering, design verification, cyber\\nthreat intelligence, malware detection, phishing, and spam\\ndetection, etc., providing a nuanced understanding of\\nLLM capabilities across different cybersecurity domains;\\n•We present a comprehensive overview of LLMs in cy-\\nbersecurity, detailing their evolution and current state,\\nincluding advancements in 42 specific models, such as\\nGPT-4o, GPT-4, BERT, Falcon, and LLaMA models;\\n•We analyze the vulnerabilities associated with LLMs,\\nincluding prompt injection, insecure output handling,\\ntraining data poisoning, inference data poisoning, DDoS\\nattacks, and adversarial natural language instructions. We\\n2', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90c2083f-f01c-40b1-88fd-e75dac022bb7', embedding=None, metadata={'page_label': '3', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='I. Introduction\\n Cybersecurity use cases\\n Contributions\\nII. Related Reviews\\n A. Applications of LLMs in Hardware Design Security\\n B. Evaluation of LLMs\\n C. Evolution and State of LLMs in AI\\n D. Advancements in PLMs for NLP\\n E. Instruction Fine-T uning for LLMs\\n F. LLMs in Software Engineering\\n G. Multimodal Algorithms\\n H. Alignment Requirements for LLMs\\n I. Knowledge-Enhanced Pre-trained Language Models\\n J. Controllable Text Generation in NLG\\n K. LLM for Cyber Security\\n L. Our survey compared to related surveys\\nIII. Preliminaries of NLP  for Cybersecurity\\nA. Recurrent neural network\\nB. Transformer models\\nIV. LLMS-based models for Cybersecurity\\n A. Recurrent Neural Networks-based models\\nB. Transformer-based modelsV. General LLMs\\n A. Prevalent LLMs\\nB. LLMs performance in the cybersecurity domainVI. Code specific LLMs\\nA. Prevalent LLMs\\nB. Datasets Development for Code-centric LLM Models\\nC. Vulnerabilities Analysis of LLM-Generated CodeVII. Cybersecurity datasets for LLMS\\nA. Cyber Security Dataset Lifecycle\\nB. Software Cyber Security datasetsVIII. LLM V ulnerabilities and Mitigation \\nA. Prompt Injection\\nB. Insecure Output Handling\\nC. Adversarial Natural Language Instructions\\nD. Automatic adversarial prompt generation\\nE. Training Data Poisoning\\nF. Inference Data Poisoning\\nG. Insecure Plugins\\nH. Denial of Service (DoS) attackIX. LLM Cybersecurity Insights, Challenges and Limitations\\nA. Challenges and Limitations\\nB. LLM Cybersecurity Insights\\nX. ConclusionSUR VEY STRUCTURE\\nLLMFig. 2: Survey Structure (From Section I. to Section X.)\\nalso examine the mitigation strategies to safeguard these\\nmodels from such vulnerabilities, providing a compre-\\nhensive look at potential attack scenarios and prevention\\ntechniques;\\n•We evaluated the performance of 42 LLM models in\\ndifferent datasets in the cybersecurity domain.\\n•We thoroughly evaluate cybersecurity datasets tailored\\nfor LLM training and testing. This includes a lifecycle\\nanalysis from dataset creation to usage, covering various\\nstages such as data cleaning, preprocessing, annotation,\\nand labeling. We also compare cybersecurity datasets to\\nidentify gaps and opportunities for future research;\\n•We provide the challenges and limitations of employing\\nLLMs in cybersecurity settings, such as dealing with\\nadversarial attacks and ensuring robustness. We also dis-\\ncuss the implications of these challenges for future LLM\\ndeployments and the development of secure, optimized\\nmodels;\\n•We discuss novel insights and strategies for leveraging\\nLLMs in cybersecurity, including advanced techniques\\nsuch as Half-Quadratic Quantization (HQQ), Reinforce-\\nment Learning with Human Feedback (RLHF), Direct\\nPreference Optimization (DPO), Odds Ratio Preference\\nOptimization (ORPO), GPT-Generated Unified Format(GGUF), Quantized Low-Rank Adapters (QLoRA), and\\nRetrieval-Augmented Generation (RAG). These insights\\naim to enhance real-time cybersecurity defenses and\\nimprove the sophistication of LLM applications in threat\\ndetection and response.\\nThe rest of this paper is organized as follows. Section\\nII presents an in-depth analysis of related reviews in the\\nfield, charting the evolution and state of LLMs in artificial\\nintelligence. Section III delves into the preliminaries of NLP\\napplications for cybersecurity, covering foundational models\\nand their advancements. Section IV discusses LLM-based\\nsolutions specific to cybersecurity. Section V reviews general\\nLLM models. Section VI reviews Code-specific LLMs models.\\nSection VII explores various cybersecurity datasets designed\\nfor LLM training and evaluation, detailing their development\\nlifecycle and specific attributes. Section VIII focuses on the\\nvulnerabilities associated with LLMs and the strategies for\\ntheir mitigation, introducing a classification of potential threats\\nand defense mechanisms. Section IX offers comprehensive in-\\nsights into the challenges and limitations of integrating LLMs\\ninto cybersecurity frameworks, including practical considera-\\ntions and theoretical constraints. Finally, Section X concludes\\nthe paper by summarizing the key findings and proposing\\ndirections for future research in LLMs and cybersecurity. A\\n3', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6a38ac7a-2e0d-4130-87cb-6951e4896a4f', embedding=None, metadata={'page_label': '4', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='brief overview of the paper’s structure is illustrated in Figure 2.\\nII. R ELATED REVIEWS\\nThis section delves into a curated collection of recent\\narticles that significantly contribute to the evolving landscape\\nof LLMs and their multifaceted applications. These reviews\\noffer a comprehensive and insightful exploration into various\\ndimensions of LLMs, including their innovative applications\\nin hardware design security, evaluation methodologies, and\\nevolving role in artificial intelligence. Further, they cover\\ncutting-edge advancements in Pre-trained Language Models\\n(PLMs) for NLP, delve into the intricacies of instruction fine-\\ntuning for LLMs, and explore their impactful integration into\\nsoftware engineering. The section also encompasses an in-\\ndepth look at multimodal algorithms, examines the critical\\naspect of alignment requirements for LLMs, and discusses\\nintegrating external knowledge into PLMs to enhance NLP\\ntasks. Lastly, it sheds light on the burgeoning field of Control-\\nlable Text Generation (CTG) in Natural Language Generation\\n(NLG), highlighting the latest trends and challenges in this\\ndynamic and rapidly advancing area of research [41]–[43].\\nA. Applications of LLMs in Hardware Design Security\\nSaha et al. [27] discussed several key applications of LLMs\\nin the context of hardware design security. The paper illus-\\ntrates how LLMs can intentionally introduce vulnerabilities\\nand weaknesses into RTL (Register-Transfer Level) designs.\\nThis process is guided by well-crafted prompts in natural\\nlanguage, demonstrating the model’s ability to understand and\\nmanipulate complex technical designs. The authors explore\\nusing LLMs to assess the security of hardware designs. The\\nmodel is employed to identify vulnerabilities, weaknesses,\\nand potential threats. It’s also used to pinpoint simple cod-\\ning issues that could evolve into significant security bugs,\\nhighlighting the model’s ability to evaluate technical designs\\ncritically. In this application, LLMs verify whether a hardware\\ndesign adheres to specific security rules or policies. The\\npaper examines the model’s proficiency in calculating secu-\\nrity metrics, understanding security properties, and generating\\nfunctional testbenches to detect weaknesses. This part of the\\nstudy underscores the LLM’s ability to conduct thorough and\\ndetailed verification processes. Finally, the paper investigates\\nhow effectively LLMs can be used to develop countermea-\\nsures against existing vulnerabilities in a design. This aspect\\nfocuses on the model’s capability to solve problems and create\\nsolutions to enhance the security of hardware designs. Overall,\\nthe paper presents an in-depth analysis of how LLMs can be\\na powerful tool in various stages of hardware design security,\\nfrom vulnerability introduction and assessment to verification\\nand countermeasure development.\\nB. Evaluation of LLMs\\nChang et al. [26] offers a comprehensive analysis of LLM\\nevaluations, addressing three key aspects: the criteria for\\nevaluation (what to evaluate), the context (where to evaluate),\\nand the methodologies (how to evaluate). It thoroughly reviewsvarious tasks across different domains to understand the suc-\\ncesses and failures of LLMs, contributing to future research\\ndirections. The paper also discusses current evaluation metrics,\\ndatasets, and benchmarks and introduces novel approaches,\\nproviding a deep understanding of the current evaluation\\nlandscape. Additionally, it highlights future challenges in LLM\\nevaluation and supports the research community by open-\\nsourcing related materials, fostering collaborative advance-\\nments in the field.\\nC. Evolution and State of LLMs in AI\\nZhao et al. [24] provides an in-depth survey of LLMs’\\nevolution and current state in artificial intelligence. It traces\\nthe progression from statistical language models to neural lan-\\nguage models, specifically focusing on the recent emergence\\nof pre-trained language models (PLMs) using Transformer\\nmodels trained on extensive corpora. The paper emphasizes the\\nsignificant advancements achieved by scaling up these mod-\\nels, noting that LLMs demonstrate remarkable performance\\nimprovements beyond a certain threshold and exhibit unique\\ncapabilities not found in smaller-scale models. The survey\\ncovers four critical aspects of LLMs: pre-training, adaptation\\ntuning, utilization, and capacity evaluation, providing insights\\ninto both their technical evolution and the challenges they\\npose. Additionally, the paper discusses the resources available\\nfor LLM development and explores potential future research\\ndirections, underlining the transformative effect of LLMs on\\nAI development and application.\\nD. Advancements in PLMs for NLP\\nMin et al. [28] surveys the latest advancements in leveraging\\nPLMs for NLP, organizing the approaches into three main\\nparadigms. Firstly, the ”Pre-train then Fine-tune” method\\ninvolves general pre-training on large unlabeled datasets fol-\\nlowed by specific fine-tuning for targeted NLP tasks. Secondly,\\n”Prompt-based Learning” uses tailored prompts to transform\\nNLP tasks into formats akin to a PLM’s pre-training, enhanc-\\ning the model’s performance, especially in few-shot learning\\nscenarios. Lastly, the ”NLP as Text Generation” paradigm\\nreimagines NLP tasks as text generation problems, fully capi-\\ntalizing on the strengths of generative models like GPT-2 and\\nT5. These paradigms represent the cutting-edge methods in\\nutilizing PLMs for various NLP applications.\\nE. Instruction Fine-Tuning for LLMs\\nZhang et al. [29] delves into the field of instruction fine-\\ntuning for LLMs, offering a detailed exploration of various\\nfacets of this rapidly advancing area. It begins with an\\noverview of the general methodologies used in instruction\\nfine-tuning, then discusses the construction of commonly-used,\\nrepresentative datasets tailored for this approach. The survey\\nhighlights a range of instruction-fine-tuned models, showcas-\\ning their diversity and capabilities. It also examines multi-\\nmodality techniques and datasets, including those involving\\nimages, speech, and video, reflecting the broad applicability\\nof instruction tuning. The adaptation of LLMs to different\\n4', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='73520771-65fa-430f-903d-47060c1a7745', embedding=None, metadata={'page_label': '5', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE I: Summary of Related Reviews on Large Language Models\\nFocused Area of Study Year Authors Key Points Data. Vuln. Comp. Optim. Hardw.\\nLLMs in Enhancing Hard-\\nware Design Security2023 Saha et\\nal.[27]Discuss applications of LLMs in hardware design security, in-\\ncluding vulnerability introduction, assessment, verification, and\\ncountermeasure development.✘✘✘✘✘\\nComprehensive Evaluation\\nMethodologies for LLMs2023 Chang et\\nal.[26]Provides an analysis of LLM evaluations focusing on criteria,\\ncontext, methodologies, and future challenges.✘✘✘✘✘\\nThe Evolutionary Path of\\nLLMs in AI2023 Zhao et\\nal.[24]Surveys the evolution of LLMs in AI, focusing on pre-training,\\nadaptation tuning, utilization, and capacity evaluation.✘✘✘✘✘\\nRecent Advancements in\\nPLMs for NLP2023 Min et al.\\n[28]Reviews advancements in PLMs for NLP, covering paradigms\\nlike Pre-train then Fine-tune, Prompt-based Learning, and NLP\\nas Text Generation.✘✘✘✘✘\\nExploring Instruction Fine-\\nTuning in LLMs2023 Zhang et\\nal.[29]Explores instruction fine-tuning for LLMs, covering methodolo-\\ngies, datasets, models, and multi-modality techniques.✘✘✘✘✘\\nApplying LLMs in Soft-\\nware Engineering2023 Fan et al.\\n[30]Survey the use of LLMs in Software Engineering, discussing\\napplications, challenges, and hybrid approaches.✘✘✘✘✘\\nUnderstanding Multimodal\\nAlgorithms2023 Wuet al.\\n[31]Provides an overview of multimodal algorithms, covering defini-\\ntion, evolution, technical aspects, and challenges.✘✘✘✘✘\\nDefining Alignment Re-\\nquirements for LLMs2023 Liuet al.\\n[32]Proposes a taxonomy of alignment requirements for LLMs and\\ndiscusses harmful content concepts.✘✘✘✘✘\\nIncorporating External\\nKnowledge in PLMs2023 Huet al.\\n[33]Reviews KE-PLMs, focusing on incorporating different types of\\nknowledge into PLMs for NLP.✘✘✘✘✘\\nAdvances in Controllable\\nText Generation2023 Zhang et\\nal.[34]Reviews CTG in NLG, focusing on Transformer-based PLMs and\\nchallenges in controllability.✘✘✘✘✘\\nLLM for Blockchain Secu-\\nrity2024 Heet al.\\n[35]Analyze existing research to understand how LLMs can improve\\nblockchain systems’ security.✘✘✘\\nLLM for Critical Infras-\\ntructure Protection2024 Yigit et\\nal.[36]Proposing advanced strategies using Generative AI and Large\\nLanguage Models to enhance resilience and security.✘✘✘✘✘\\nSoftware Testing with\\nLarge Language Models2024 Wang et\\nal.[37]Explore how Large Language Models (LLMs) can enhance\\nsoftware testing, examining tasks, techniques, and future research\\ndirections.✘✘✘✘✘\\nMalicious Insider Threat\\nDetection Using Machine\\nLearning Methods2024 Alzaabi et\\nal.[22]Recommends advanced ML methods like deep learning and\\nNLP for better detection and mitigation of insider threats in\\ncybersecurity, emphasizing the need for integrating time-series\\ntechniques.✘✘✘\\nAdvancements in Large\\nLanguage Models2024 Raiaan et\\nal.[25]Reviews the evolution, architectures, applications, societal im-\\npacts, and challenges of LLMs, aiding practitioners, researchers,\\nand experts in understanding their development and prospects.✘✘✘✘✘\\nApplications of LLMs in\\ncybersecurity tasks2024 Xuet al.\\n[38]Highlights the diverse applications of LLMs in cybersecurity\\ntasks such as vulnerability detection, malware analysis, and\\nintrusion and phishing detection.✘✘✘\\nRetrieval-Augmented Gen-\\neration for LLMs2024 Zhao et\\nal.[25]Reviews how RAG has been integrated into various AIGC scenar-\\nios to overcome common challenges such as updating knowledge,\\nhandling long-tail data, mitigating data leakage, and managing\\ncosts associated with training and inference.✘✘✘✘✘\\nProvides an overview of\\nParameter Efficient Fine-\\nTuning (PEFT)2024 Han et al.\\n[39]Reviews various PEFT algorithms, their effectiveness, and the\\ncomputational overhead involved.✘✘✘✘✘\\nLLM for Cyber Security 2024 Zhang et\\nal.[40]The paper conducts a systematic literature review of over 180\\nworks on applying LLMs in cybersecurity.✘✘✘\\nLLM with security and pri-\\nvacy issues2024 Yao et al.\\n[9]Explores the dual impact of LLMs on security and privacy,\\nhighlighting their potential to enhance cybersecurity and data\\nprotection while also posing new risks and vulnerabilities.✘✘✘\\nTHIS SURVEY 2024 Ferrag et\\nal.This paper provides an in-depth review of using Generative AI\\nand Large Language Models (LLMs) in cybersecurity.✔✔✔✔✔\\n✘: Not covered; : Partially covered; ✔: Covered; Data.: Datasets used for training and fine-tuning LLMs for security use cases; Vuln.: LLM Vulnerabilities and Mitigation ;\\nComp.: Experimental Analysis of LLMs Models’ Performance in Cyber Security Knowledge; Optim.: Optimization Strategies for Large Language Models in Cybersecurity;\\nHardw. : Experimental Analysis of LLMs Models’ Performance in Hardware Security.\\ndomains and applications using instruction tuning strategies is\\nreviewed, demonstrating the versatility of this method. Addi-\\ntionally, the survey addresses efforts to enhance the efficiency\\nof instruction fine-tuning, focusing on reducing computational\\nand time costs. Finally, it evaluates these models, including\\nperformance analysis and critical perspectives, offering a holis-\\ntic view of the current state and potential of instruction fine-\\ntuning in LLMs.F . LLMs in Software Engineering\\nFanet al. [30] present a survey on using LLMs in Software\\nEngineering (SE), highlighting their potential applications and\\nopen research challenges. LLMs, known for their emergent\\nproperties, offer novel and creative solutions across various\\nSoftware Engineering activities, including coding, design,\\nrequirements analysis, bug fixing, refactoring, performance\\noptimization, documentation, and analytics. Despite these ad-\\nvantages, the paper also acknowledges the significant technical\\nchallenges these emergent properties bring, such as the need\\n5', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b3206b60-fac4-481d-892a-b0a5c71575dc', embedding=None, metadata={'page_label': '6', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='for methods to eliminate incorrect solutions, notably hallu-\\ncinations. The survey emphasizes the crucial role of hybrid\\napproaches, which combine traditional Software Engineering\\ntechniques with LLMs, in developing and deploying reliable,\\nefficient, and effective LLM-based solutions for Software\\nEngineering. This approach suggests a promising pathway\\nfor integrating advanced AI models into practical software\\ndevelopment processes.\\nG. Multimodal Algorithms\\nWuet al. [31] addresses a significant gap in understand-\\ning multimodal algorithms by providing a comprehensive\\noverview of their definition, historical development, applica-\\ntions, and challenges. It begins by defining multimodal models\\nand algorithms, then traces their historical evolution, offering\\ninsights into their progression and significance. The paper\\nserves as a practical guide, covering various technical aspects\\nessential to multimodal models, such as knowledge represen-\\ntation, selection of learning objectives, model construction,\\ninformation fusion, and prompts. Additionally, it reviews cur-\\nrent algorithms employed in multimodal models and discusses\\ncommonly used datasets, thus laying a foundation for future\\nresearch and evaluation in this field. The paper concludes\\nby exploring several applications of multimodal models and\\ndelving into key challenges that have emerged from their\\nrecent development, shedding light on both the potential and\\nthe limitations of these advanced computational tools.\\nH. Alignment Requirements for LLMs\\nLiuet al. [32] propose a taxonomy of alignment require-\\nments for LLMs to aid practitioners in understanding and ef-\\nfectively implementing alignment dimensions and inform data\\ncollection efforts for developing robust alignment processes.\\nThe paper dissects the concept of ”harmful” generated content\\ninto specific categories, such as harm to individuals (like emo-\\ntional harm, offensiveness, and discrimination), societal harm\\n(including instructions for violent or dangerous behaviors),\\nand harm to stakeholders (such as misinformation impacting\\nbusiness decisions). Citing an imbalance in Anthropic’s align-\\nment data, the paper points out the uneven representation of\\nvarious harm categories, like the high frequency of ”violence”\\nversus the marginal appearance of ”child abuse” and ”self-\\nharm.” This observation supports the argument that alignment\\ntechniques heavily dependent on data cannot ensure that LLMs\\nwill uniformly align with human behaviors across all aspects.\\nThe authors’ own measurement studies reveal that aligned\\nmodels do not consistently show improvements across all\\nharm categories despite the alignment efforts claimed by the\\nmodel developers. Consequently, the paper advocates for a\\nframework that allows a more transparent, multi-objective\\nevaluation of LLM trustworthiness, emphasizing the need for\\na comprehensive and balanced approach to alignment in LLM\\ndevelopment.\\nI. Knowledge-Enhanced Pre-trained Language Models\\nHuet al. [33] offers a comprehensive review of Knowledge-\\nEnhanced Pre-trained Language Models (KE-PLMs), a bur-\\ngeoning field aiming to address the limitations of standardPLMs in NLP. While PLMs trained on vast text corpora\\ndemonstrate impressive performance across various NLP tasks,\\nthey often fall short in areas like reasoning due to the absence\\nof external knowledge. The paper focuses on how incorpo-\\nrating different types of knowledge into PLMs can overcome\\nthese shortcomings. It introduces distinct taxonomies for Nat-\\nural Language Understanding (NLU) and Natural Language\\nGeneration (NLG) to distinguish between these two core\\nareas of NLP. For NLU, the paper categorizes knowledge\\ntypes into linguistic, text, knowledge graph (KG), and rule\\nknowledge. In the context of NLG, KE-PLMs are classified\\ninto KG-based and retrieval-based methods. By outlining these\\nclassifications and exploring the current state of KE-PLMs,\\nthe paper provides not only clear insights into this evolving\\ndomain but also identifies promising future directions for the\\ndevelopment and application of KE-PLMs, highlighting their\\npotential to enhance the capabilities of PLMs in NLP tasks\\nsignificantly.\\nJ. Controllable Text Generation in NLG\\nZhang [34] provides a critical and systematic review of\\nControllable Text Generation (CTG), a burgeoning field in\\nNLG that is essential for developing advanced text generation\\ntechnologies tailored to specific practical constraints. The\\npaper focuses on using large-scale pre-trained language models\\n(PLMs), particularly those based on transformer architecture,\\nwhich have established a new paradigm in NLG due to their\\nability to generate more diverse and fluent text. However,\\nthe limited interpretability of deep neural networks poses\\nchallenges to the controllability of these methods, making\\ntransformer-based PLM-driven CTG a rapidly evolving and\\nchallenging research area. The paper surveys various ap-\\nproaches that have emerged in the last 3-4 years, each targeting\\ndifferent CTG tasks with varying controlled constraints. It\\nprovides a comprehensive overview of common tasks, main\\napproaches, and evaluation methods in CTG and discusses the\\ncurrent challenges and potential future directions in the field.\\nClaiming to be the first to summarize state-of-the-art CTG\\ntechniques from the perspective of Transformer-based PLMs,\\nthis paper aims to assist researchers and practitioners in keep-\\ning pace with the academic and technological developments\\nin CTG, offering them an insightful landscape of the field and\\na guide for future research.\\nK. LLM for Cyber Security\\nZhang et al. [40] examines the integration of LLMs within\\ncybersecurity. Through an extensive literature review involving\\nover 127 publications from leading security and software\\nengineering venues, this paper aims to shed light on LLMs’\\nmultifaceted roles in enhancing cybersecurity measures. The\\nsurvey pinpoints various applications for LLMs in detecting\\nvulnerabilities, analyzing malware, and managing network\\nintrusions and phishing threats. It highlights the current lim-\\nitations regarding the datasets used, which often lack size\\nand diversity, thereby underlining the necessity for more\\nrobust datasets tailored to these security tasks. The paper\\nalso identifies promising methodologies like fine-tuning and\\n6', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90b2b2f3-11c4-4fcb-9119-be1aeabb3089', embedding=None, metadata={'page_label': '7', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='domain-specific pre-training, which could better harness the\\npotential of LLMs in cybersecurity contexts.\\nYao et al. [9] explores the dual role of LLMs in se-\\ncurity and privacy, highlighting their benefits in enhancing\\ncode security and data confidentiality and detailing potential\\nrisks and inherent vulnerabilities. The authors categorize the\\napplications and challenges into ”The Good,” ”The Bad,”\\nand ”The Ugly,” where they discuss LLMs’ positive impacts,\\ntheir use in offensive applications, and their susceptibility to\\nspecific attacks, respectively. The paper emphasizes the need\\nfor further research on threats like model and parameter extrac-\\ntion attacks and emerging techniques such as safe instruction\\ntuning, underscoring the complex balance between leveraging\\nLLMs for improved security and mitigating their risks.\\nL. Our survey compared to related surveys\\nOur paper presents a more specialized and technical explo-\\nration of generative artificial intelligence and large language\\nmodels in cybersecurity than the previous literature review.\\nFocusing on a broad array of cybersecurity domains such\\nas hardware design security, intrusion detection systems, and\\nsoftware engineering, it targets a wider professional audience,\\nincluding engineers, researchers, and industrial practitioners.\\nThis paper reviews 35 leading models like GPT-4, BERT,\\nFalcon, and LLaMA, not only highlighting their applications\\nbut also their developmental trajectories, thereby providing a\\ncomprehensive insight into the current capabilities and future\\npotentials of these models in cybersecurity.\\nThe paper also delves deeply into the vulnerabilities associ-\\nated with LLMs, such as prompt injection, adversarial natural\\nlanguage instructions, and insecure output handling. It presents\\nsophisticated attack scenarios and robust mitigation strategies,\\noffering a detailed analysis crucial for understanding and pro-\\ntecting against potential threats. Additionally, the lifecycle of\\nspecialized cybersecurity datasets—covering creation, clean-\\ning, preprocessing, annotation, and labeling—is scrutinized,\\nproviding essential insights into improving data integrity and\\nutility for training and testing LLMs. This level of detail is\\nvital for developing robust cybersecurity solutions that can\\neffectively leverage the power of LLMs.\\nLastly, the paper examines the challenges associated with\\ndeploying LLMs in cybersecurity contexts, emphasizing the\\nnecessity for model robustness and the implications of adver-\\nsarial attacks. It introduces advanced methodologies such as\\nReinforcement Learning with Human Feedback (RLHF) and\\nRetrieval-Augmented Generation (RAG) to enhance real-time\\ncybersecurity operations. This focus not only delineates the\\ncurrent state of LLM applications in cybersecurity but also\\nsets the direction for future research and practical applications,\\naiming to optimize and secure LLM deployments in an evolv-\\ning threat landscape. This makes the paper an indispensable\\nresource for anyone involved in cybersecurity and AI, bridging\\nthe gap between academic research and practical applications.\\nIII. P RELIMINARIES OF NLP FOR CYBER SECURITY\\nThis section presents the preliminaries of NLP for cyber-\\nsecurity, including recurrent neural networks (LSTMs and\\nGRUs) and transformer models.A. Recurrent neural networks\\nRecurrent Neural Networks (RNNs) [44] are artificial neural\\nnetworks that handle data sequences such as time series or\\nNLP tasks. The RNN model consists of two linked recur-\\nrent neural networks. The first RNN encodes sequences of\\nsymbols into a fixed-length vector, while the second decodes\\nthis vector into a new sequence. This architecture aims to\\nmaximize the conditional probability of a target sequence from\\na given source sequence. When applied to cybersecurity, this\\nmodel could be instrumental in threat detection and response\\nsystems by analyzing and predicting network traffic or log data\\nsequences that indicate malicious activity. Integrating the con-\\nditional probabilities generated by this model could enhance\\nanomaly detection frameworks, improving the identification\\nof subtle or novel cyber threats. The model’s ability to learn\\nmeaningful representations of data sequences further supports\\nits potential to recognize complex patterns and anomalies in\\ncybersecurity environments [45], [46].\\n1) Gated Recurrent Units: GRUs are a recurrent neural\\nnetwork architecture designed to handle the vanishing gradient\\nproblem that can occur with standard recurrent networks.\\nIntroduced by Cho et al. in 2014 [47], GRUs simplify the\\nLSTM (Long Short-Term Memory) model while retaining its\\nability to model long-term dependencies in sequential data.\\nGRUs achieve this through two main gates: the update gate,\\nwhich controls how much a new state overwrites the old\\nstate, and the reset gate, which determines how much past\\ninformation to forget. These gates effectively regulate the\\nflow of information, making GRUs adept at tasks like time\\nseries prediction, speech recognition, and natural language\\nprocessing. The main steps of GRUs are organized as follows:\\n•Update Gate: The update gate determines how much\\ninformation from the previous hidden state should be\\npassed to the new one. The update gate is calculated using\\nthe following formula:\\nzt=σ(Wzxt+Uzht−1) (1)\\nwhere ztis the update gate at time step t,WzandUzare\\nthe weight matrices, xtis the input at time step t, and\\nht−1is the previous hidden state. The sigmoid function,\\nrepresented by σ, squishes the equation’s results between\\n0 and 1. The update gate allows the GRU to decide how\\nmuch of the previous hidden state information should be\\npassed on to the new hidden state. If the update gate\\nis close to 1, it means that a lot of the previous hidden\\nstate information should be passed on, while if the update\\ngate is close to 0, it means that very little of the previous\\nhidden state information should be passed on. The Update\\nGate formula can be explored in the following three\\ndifferent parts:\\nPart 1: Linear combination of inputs:\\nzt=Wrxt+Urht−1 (2)\\nPart 2: Application of the sigmoid function:\\n7', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b63222f2-f2df-43d3-88d3-98ffee2a81a0', embedding=None, metadata={'page_label': '8', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='˜rt=σ(zt) (3)\\nPart 3: Element-wise multiplication of ˜rtand previous\\nhidden state:\\nrt= ˜rt⊙ht−1 (4)\\nWhere ⊙represents the Hadamard product, also known\\nas the element-wise multiplication.\\n•Reset Gate: The reset gate determines how much of the\\nprevious hidden state should be forgotten. The reset gate\\nis calculated using the following formula:\\nrt=σ(Wrxt+Urht−1) (5)\\nwhere rtis the reset gate at time step t,WrandUrare\\nthe weight matrices, xtis the input at time step t, and\\nht−1is the previous hidden state.\\n•Candidate Hidden State: The candidate’s hidden state\\ncombines the input and the previous hidden state, filtered\\nthrough the reset gate. The candidate’s hidden state is\\ncalculated using the following formula:\\n˜ht= tanh( Wxt+U(rt⊙ht−1)) (6)\\nwhere ˜htis the candidate hidden state at time step t,W\\nandUare the weight matrices, xtis the input at time\\nstept, and rtis the reset gate at time step t. In this\\nequation, the input at time step t(xt) is combined with the\\nprevious hidden state ( ht−1) through the weight matrices\\nWandU. The reset gate ( rt) is used to control the extent\\nto which the previous hidden state ( ht−1) is passed to\\nthe candidate hidden state ( ˜ht). The element-wise product\\nbetween the reset gate ( rt) and the previous hidden state\\n(ht−1) is used to create the reset vector ( rt⊙ht−1). The\\nreset vector is combined with the input ( xt) through the\\nweight matrix U. Finally, the result is passed through\\nthe hyperbolic tangent function to calculate the candidate\\nhidden state ( ˜ht).\\n•New Hidden State: The new hidden state combines the\\nprevious and candidate hidden states, filtered through the\\nupdate gate. The new hidden state is calculated using the\\nfollowing formula:\\nht= (1−zt)⊙ht−1+zt⊙˜ht (7)\\nwhere htis the new hidden state at time step t,ztis\\nthe update gate at time step t, and ˜htis the candidate\\nhidden state at time step t. The new hidden state ( ht)\\nis calculated by taking a weighted combination of the\\nprevious hidden state ( ht−1) and the candidate hidden\\nstate ( ˜ht). The weight of the previous hidden state is\\ndetermined by the update gate ( zt) - if the update gate isclose to 1, the new hidden state is primarily influenced\\nby the previous hidden state. If the update gate is close\\nto 0, the candidate’s hidden state primarily influences the\\nnew hidden state. The element-wise product between the\\nupdate gate ( zt) and the candidate hidden state ( ˜ht) is\\nused to create the update vector ( zt⊙˜ht). The element-\\nwise product between the complement of the update gate\\n(1−zt) and the previous hidden state ( ht−1) is used to\\ncreate the reset vector ( (1−zt)⊙ht−1). Finally, the update\\nand reset vectors are added to calculate the new hidden\\nstate ( ht).\\n2) Long Short-Term Memory: The LSTM [2] was designed\\nto overcome the vanishing gradient problem that affects tra-\\nditional recurrent neural networks (RNNs) during training,\\nparticularly over long sequences. By integrating memory cells\\nthat can maintain information over extended periods and gates\\nthat regulate the flow of information into and out of the\\ncell, LSTMs provide an effective mechanism for learning\\ndependencies and retaining information over time. This archi-\\ntecture has proved highly influential, becoming foundational\\nto numerous applications in machine learning that require\\nhandling sequential data, such as natural language processing,\\nspeech recognition, and time series analysis. The impact of\\nthis work has been extensive, as it enabled the practical\\nuse and development of deep learning models for complex\\nsequence modeling tasks. In cybersecurity, LSTMs can be used\\nfor anomaly detection, where they analyze network traffic or\\nsystem logs to identify unusual patterns that may signify a\\nsecurity breach or malicious activity [48]–[50]. Their ability to\\nlearn from long sequences makes them particularly useful for\\ndetecting sophisticated attacks that evolve, such as advanced\\npersistent threats (APTs) and ransomware. The main steps of\\nLSTM models are organized as follows:\\n•Input Gate: The first step in an LSTM-based RNN\\ninvolves calculating the input gate. The input gate deter-\\nmines the extent of new input to be added to the current\\nstate. The formula for the input gate is:\\nit=σ(Wi·[ht−1, xt] +bi) (8)\\nwhere itis the input gate at time step t,Wiis the weight\\nmatrix for the input gate, ht−1is the hidden state from\\nthe previous time step, xtis the input at time step t, and\\nbiis the bias for the input gate. The function σ(x)is the\\nsigmoid activation function. This formula calculates the\\ninput gate itby first concatenating the previous hidden\\nstateht−1with the current input xt. This combined vector\\nis multiplied by the weight matrix Wi, and the bias biis\\nadded. Finally, the sigmoid activation function is applied\\nto produce it, which ranges from 0 to 1 and represents\\nhow much the current input updates the hidden state.\\n•Forget Gate: The second step calculates the forget gate,\\ndetermining how much the previous state should be\\nforgotten. The formula for the forget gate is:\\n8', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1aff56d6-bb3c-4be1-901d-b1a42e915d2d', embedding=None, metadata={'page_label': '9', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ft=σ(Wf·[ht−1, xt] +bf) (9)\\nwhere ftis the forget gate at time step t,Wfis the weight\\nmatrix for the forget gate, ht−1is the hidden state from\\nthe previous time step, xtis the input at time step t, and\\nbfis the bias for the forget gate. The forget gate ftis\\ncalculated like the input gate. It involves concatenating\\nthe previous hidden state ht−1with the current input xt,\\nmultiplying by the weight matrix Wf, and adding the\\nbiasbf. The resulting value is passed through the sigmoid\\nactivation function to determine the forget gate ft, which\\nranges from 0 to 1 and represents the degree to which\\nthe previous hidden state is preserved or forgotten in the\\ncurrent hidden state.\\n•Candidate Memory Cell: The third step calculates the\\ncandidate memory cell, representing the potential mem-\\nory state update. The formula for the candidate memory\\ncell is:\\n˜ct= tanh( Wc·[ht−1, xt] +bc) (10)\\nwhere ˜ctis the candidate memory cell at time step t,\\nWcis the weight matrix for the candidate memory cell,\\nht−1is the hidden state from the previous time step, xt\\nis the input at time step t, and bcis the bias for the\\ncandidate memory cell. The function tanh( x)is the hy-\\nperbolic tangent activation function. In this formula, the\\ncandidate memory cell ˜ctis calculated by concatenating\\nthe previous hidden state ht−1and the input xt, then\\nmultiplying by the weight matrix Wcand adding the bias\\nbc. The result is passed through the hyperbolic tangent\\nactivation function, which ranges from -1 to 1, to control\\nthe magnitude of the memory cell update.\\n•Current Memory Cell: The fourth step calculates the\\ncurrent memory cell, which is the updated state of the\\nmemory cell, combining the effects of the forget and input\\ngates. The formula for the current memory cell is:\\nct=ft·ct−1+it·˜ct (11)\\nwhere ctis the current memory cell at time step t,ftis\\nthe forget gate at time step t,ct−1is the memory cell\\nfrom the previous time step, itis the input gate at time\\nstept, and ˜ctis the candidate memory cell at time step\\nt. This equation represents the new memory cell state as\\na combination of the old state (modulated by the forget\\ngate) and the potential update (modulated by the input\\ngate).\\n•Output Gate: The final step calculates the output gate,\\nwhich determines the amount of information output from\\nthe LSTM cell. The details and formula for the output\\ngate should follow.\\nFig. 3: How Transformer works for Software Security.\\nB. Transformer models\\nThe Transformer architecture proposed by Vaswani et al.\\n[4] in 2017 is a significant advancement in natural language\\nprocessing built entirely around attention mechanisms. These\\nmechanisms allow the model to assess the relevance of dif-\\nferent words in a sentence, independent of their positional\\nrelationships. This foundational technology has enhanced the\\nefficiency of tasks like translation and text summarization\\nand has broad cybersecurity applications. In cybersecurity,\\nTransformer models can detect and respond to threats by ana-\\nlyzing source code patterns and network traffic and identifying\\nanomalies in system logs, as presented in Figure 3. They can\\nalso be used for the automated generation of security policies\\nbased on the evolving landscape of threats and for intelligent\\nthreat hunting, where the system predicts and neutralizes\\nthreats before they cause harm. This makes Transformers\\nversatile in enhancing security protocols and defending against\\ncyber attacks [19]. The main steps of Transformer models are\\norganized as follows:\\n•Attention Mechanism: The attention mechanism in the\\nTransformer model computes attention scores between\\nthe input and output representations. These scores are\\ncalculated using the scaled dot-product of the query and\\nkey representations and then normalized by a softmax\\nfunction. The attention scores are subsequently used to\\ncompute a weighted sum of the value representations,\\nforming the output of the attention mechanism.\\nThe equation defines the attention scores:\\nAttention (Q, K, V ) =softmax\\x12QKT\\n√dk\\x13\\nV\\n(12)\\nWhere:\\n9', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1b9a571b-aa68-4b22-bfb8-7cdac9e711ed', embedding=None, metadata={'page_label': '10', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Q,K, and Vrepresent the matrices of queries, keys, and\\nvalues transformed from the input representations. The\\ndimension of the keys is denoted by dk. The attention\\nmechanism involves computing the dot product of Qand\\nthe transpose of K, which is then scaled by the inverse\\nsquare root of dkto stabilize the gradients. The result\\nis passed through a softmax function to normalize the\\nscores, ensuring they sum to 1. These scores, represent-\\ning attention weights, compute a weighted sum of the\\nvalues in V, resulting in the final attention output. This\\nmechanism allows the model to dynamically focus on\\nthe most relevant parts of the input sequence for making\\npredictions.\\n•Multi-Head Attention: In the Transformer model, mul-\\ntiple attention heads enhance the model’s capability to\\nsimultaneously focus on different parts of the input se-\\nquence. The multi-head attention is calculated as follows:\\nMHead (Q, K, V ) =Concat (hd1, hd 2, . . . , hd h)WO\\n(13)\\nWhere: hdirepresents the output of the i-th at-\\ntention head, computed using the attention formula\\nAttention (Qi, Ki, Vi). Each Qi,Ki, and Viare different\\nlinear projections of the original inputs Q,K, and V.\\nWOis a linear transformation matrix applied to the\\nconcatenated results of all attention heads. The Concat\\nfunction concatenates the outputs of each head along a\\nspecific dimension. The outputs of individual heads, hdi,\\nare each computed using the scaled dot-product attention\\nmechanism:\\nhdi=Attention (Qi, Ki, Vi) =softmax\\x12QiKT\\ni√dk\\x13\\nVi\\n(14)\\nThis approach enables the Multi-Head Attention mech-\\nanism to capture various aspects of the input sequence,\\nsimultaneously focusing on different subspace represen-\\ntations. As a result, it facilitates the model’s capture of\\nmore complex relationships and improves performance\\nacross different types of tasks.\\n•Layer Normalization: In the Transformer model, layer\\nnormalization ensures the input is within a standard\\nrange. The layer normalization can be calculated as\\nfollows:\\nLN(x) =x−mean (x)p\\nvar(x)(15)\\nWhere xis the input to the layer normalization.\\ntextmean (x)andtextvar (x)are the mean and variance\\nofx, respectively. Layer Normalization aims to mitigate\\nthe internal covariate shift, which arises when the distri-bution of activations in a layer changes during training.\\nThe normalization operation is performed by subtracting\\nthe mean of the activations and dividing by the square\\nroot of the variance. This ensures the activations have\\na zero mean and unit variance, leading to more stable\\ntraining.\\n•Position-wise Feed Forward: The position-wise feed-\\nforward network transforms the input and output repre-\\nsentations in the Transformer model. The position-wise\\nfeedforward can be calculated as follows:\\nFFN (x) = max(0 , xW 1+b1)W2+b2(16)\\nWhere: xis the input to the feed-forward network. W1,\\nb1,W2, and b2are the weight and bias parameters of the\\nfeed-forward network. max(0 , x)is the ReLU activation\\nfunction. This equation represents a simple feed-forward\\nneural network (FFN) operation in deep learning models.\\nThe FFN operation is a multi-layer perceptron (MLP)\\nthat transforms the input xinto a new representation by\\npassing it through two fully connected (dense) layers. The\\nfirst layer is followed by a ReLU activation function,\\nwhich applies a non-linear activation to the input by\\nsetting all negative values to zero. This activation function\\nhelps the model learn complex non-linear relationships\\nbetween the input and output. The second layer is a linear\\ntransformation that produces the final output of the FFN.\\nThe weight and bias parameters of the two layers, W1,\\nb1,W2, and b2, are learned during training and allow the\\nmodel to learn different representations of the input data.\\n•Encoder and Decoder Blocks: In the Transformer model,\\nthe encoder and decoder blocks transform the input\\nsequences into the output sequences. The encoder and\\ndecoder blocks can be calculated as follows:\\nEnc(x) =LN(x+MHead (x, x, x )) (17)\\nDec(x, y) =LN(x+MHead (x, y, y )+MHead (x, x, x ))\\n(18)\\nWhere: xis the input to the encoder/decoder block. y\\nis the output from the previous encoder/decoder block.\\nThe Encoder block Enc (x)takes the input xand applies\\nthe Multi-Head Attention mechanism to compute the\\nattention scores between the input and itself. The result\\nis then added to the input and passed through a Layer\\nNormalization operation. The output of the encoder block\\nis the new representation of the input after processing\\nthrough the Multi-Head Attention and Layer Normaliza-\\ntion operations. The Decoder block Dec (x, y)is similar to\\nthe encoder block but also takes the output from the previ-\\nous decoder block, y, as input. The Multi-Head Attention\\nmechanism is applied to compute the attention scores\\n10', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70a8b72e-b6fc-40b7-8e89-fd7b84bfd04d', embedding=None, metadata={'page_label': '11', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='between the input and the previous output and between\\nthe input and itself. The results are added to the input\\nand passed through a Layer Normalization operation. The\\noutput of the decoder block is the new representation\\nof the input after processing through the Multi-Head\\nAttention and Layer Normalization operations.\\nIV. LLM S-BASED MODELS FOR CYBER SECURITY\\nThis section reviews recent studies employing LLM-\\nbased models (i.e., Recurrent Neural Networks-based and\\ntransformer-based models) for threat detection, malware clas-\\nsification, intrusion detection, and software vulnerability de-\\ntection. Tables II, III, and Figure 4 present the LLM-based\\nsolutions for Cyber Security Use Cases.\\nA. Recurrent Neural Networks-based models\\n1) Intrusion Detection: Yinet al. [55] propose a deep learn-\\ning approach for intrusion detection using recurrent neural\\nnetworks (RNN-ID) and study its performance in binary and\\nmulticlass classification tasks. The results show that the RNN-\\nID model outperforms traditional machine learning methods\\nin accuracy. Chawla et al. [67] presented an anomaly-based\\nintrusion detection system that leverages recurrent neural net-\\nworks (RNNs) with gated recurrent units (GRUs) and stacked\\nconvolutional neural networks (CNNs) to detect malicious\\ncyber attacks. The system establishes a baseline of normal\\nbehavior for a given system by analyzing sequences of system\\ncalls made by processes. It identifies anomalous sequences\\nbased on a language model trained on normal call sequences\\nfrom the ADFA dataset of system call traces. The authors\\ndemonstrate that using GRUs instead of LSTMs results in\\ncomparable performance with reduced training times and\\nthat combining GRUs with stacked CNNs leads to improved\\nanomaly detection. The proposed system shows promising\\nresults in detecting anomalous system call sequences in the\\nADFA dataset. However, further research is needed to evaluate\\nits performance in other datasets and real-world scenarios and\\naddress issues related to adversarial attacks.\\nUllah et al. [68] introduce the deep learning models to tackle\\nthe challenge of managing cybersecurity in the growing realm\\nof IoT devices and services. The models utilize Recurrent\\nNeural Networks, Convolutional Neural Networks, and hybrid\\ntechniques to detect anomalies in IoT networks accurately.\\nThe proposed models are validated using various datasets (i.e.,\\nIoT-DS2, MQTTset, IoT-23, and datasets) and achieve high\\naccuracy, precision, recall, and F1 score. However, the models\\nneed to be tested on more extensive and diverse datasets,\\nand further research is necessary to enhance their scalability\\nfor practical applications in cybersecurity. Donkol et al. [69]\\npresents a technique, ELSTM-RNN, for improving security in\\nintrusion detection systems. Using likely point particle swarm\\noptimization (LPPSO) and enhanced LSTM classification, the\\nproposed system addresses gradient vanishing, generalization,\\nand overfitting issues. The system uses an enhanced parti-\\ncle swarm optimization technique to select efficient features,\\nwhich are used for effective classification using an enhanced\\nLSTM framework. The proposed system outperformed othermethods, such as LPBoost and DNNs, in accuracy, precision,\\nrecall, and error rate. The NSL-KDD dataset was used for\\nvalidation and testing, and further verification was done on\\nother datasets. While the paper provides a comprehensive\\nsolution, future research could explore the applicability of the\\nproposed system to other datasets and real-world scenarios.\\nAdditionally, a more detailed analysis of the computational\\ncost of the proposed system compared to other methods could\\nbe beneficial.\\nZhao et al. [70] presents ERNN, an end-to-end RNN\\nmodel with a novel gating unit called session gate, designed\\nto address network-induced phenomena that may result in\\nmisclassifications in traffic detection systems used in cyber-\\nsecurity. The gating unit includes four types of actions to\\nsimulate network-induced phenomena during model training\\nand the Mealy machine to adjust the probability distribution\\nof network-induced phenomena. The paper demonstrates that\\nERNN outperforms state-of-the-art methods by 4% accuracy\\nand is scalable in terms of parameter settings and feature\\nselection. The paper also uses the Integrated Gradients method\\nto interpret the gating mechanism and demonstrates its ability\\nto reduce dependencies on local packets. Althubiti et al.\\n[57] propose a deep learning-based intrusion detection system\\n(IDS) that uses a Long Short-Term Memory (LSTM) RNN\\nto classify and predict known and unknown intrusions. The\\nexperiments show that the proposed LSTM-based IDS can\\nachieve a high accuracy rate of 0.9997. Xu et al. [58] propose\\na novel IDS that consists of a recurrent neural network with\\ngated recurrent units (GRU), multilayer perceptron (MLP),\\nand softmax module. The experiments on the KDD 99 and\\nNSL-KDD data sets show that the system has a high overall\\ndetection rate and a low false positive rate. Ferrag and Lean-\\ndros [59] propose a novel deep learning and blockchain-based\\nenergy framework for smart grids, which uses a blockchain-\\nbased scheme and a deep learning-based scheme for intrusion\\ndetection. The deep learning-based scheme employs recurrent\\nneural networks to detect network attacks and fraudulent\\ntransactions in the blockchain-based energy network. The\\nperformance of the proposed IDS is evaluated using three\\ndifferent data sources.\\nPolat et al. [72] introduce a method for improving the\\ndetection of DDoS attacks in SCADA systems that use SDN\\ntechnology. The authors propose using a Recurrent Neural Net-\\nwork (RNN) classifier model with two parallel deep learning-\\nbased methods: Long Short-Term Memory (LSTM) and Gated\\nRecurrent Units (GRU). The proposed model is trained and\\ntested on a dataset from an experimentally created SDN-\\nbased SCADA topology containing DDoS attacks and regular\\nnetwork traffic data. The results show that the proposed RNN\\nmodel achieves an accuracy of 97.62% for DDoS attack de-\\ntection, and transfer learning further improves its performance\\nby around 5%.\\n2) Software Security: Wang et al. [71] propose a deep\\nlearning-based defense system called PatchRNN to automat-\\nically detect secret security patches in open-source software\\n(OSS). The system leverages descriptive keywords in the\\ncommit message and syntactic and semantic features at the\\nsource-code level. The system’s performance was evaluated\\n11', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='55f677c5-ee9e-4c67-a8b9-4078e42309e4', embedding=None, metadata={'page_label': '12', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE II: LLMs-based models for Cyber Security (Part I).\\nStudy Year Type of Model Dataset Used Domain Key Contributions Open Issues\\nParra et al.\\n[51]2022 Federated\\nTransformer Log\\nLearning ModelHDFS and CTDD datasets Threat detection and\\nforensicsThe interpretability module inte-\\ngrated into the model provides\\ninsightful interpretability of the\\nmodel’s decision-making processThe paper briefly mentions the ap-\\nplicability of the proposed approach\\nin edge computing systems but does\\nnot discuss the scalability of the\\napproach to larger systems\\nZiems et al.\\n[52]2021 Transformer Model,\\nBERT, CANINE,\\nBagging-based\\nrandom transformer\\nforest (RTF)Malware family datasets Malware Classifica-\\ntionDemonstration that transformer-\\nbased models outperform\\ntraditional machine and deep\\nlearning models in classifying\\nmalware familiesThe experiments are conducted\\non preprocessed NIST NVD/SARD\\ndatabases, which may not reflect\\nreal-world conditions\\nWu et al.\\n[53]2022 Robust\\nTransformer-based\\nIntrusion Detection\\nSystem (RTID)CICID2017 and CIC-DDoS2019\\ndatasetsIntrusion Detection The proposed method outperforms\\nclassical machine learning algo-\\nrithms such as support vector ma-\\nchine (SVM) and deep learning al-\\ngorithms (i.e., RNN, FNN, LSTM)\\non the two evaluated datasetsThere is no discussion in the pa-\\nper regarding the scalability of the\\nproposed method, particularly when\\ndealing with large-scale and real-\\ntime network traffic\\nDemirkıran\\net al. [54]2022 Transformer-based\\nmodelsCatak dataset, Oliveira dataset,\\nVirusShare dataset, and VirusSample\\ndatasetMalware classifica-\\ntionThe paper demonstrates that\\ntransformer-based models,\\nspecifically BERT and CANINE,\\noutperform traditional machine and\\ndeep learning models in classifying\\nmalware familiesThe study only focuses on mal-\\nware families that use API call se-\\nquences, which means that it does\\nnot consider other malware types\\nthat may not use API calls\\nYin et al.\\n[55]2017 RNN-ID (Recurrent\\nNeural Network-\\nIntrusion Detection)Benchmark data set Intrusion Detection The proposed model can improve\\nthe accuracy of intrusion detectionOther machine learning algorithms\\nand deep learning models, such as\\nconvolutional neural networks and\\ntransformers are not considered in\\nthe comparison\\nG¨uera et al.\\n[56]2018 Temporal-aware\\nPipeline (CNN and\\nRNN)Large set of deepfake videos collected\\nfrom multiple video websitesDetection of Deep-\\nfake VideosThe proposed method achieves\\ncompetitive results in detecting\\ndeepfake videos while using a sim-\\nple architectureThe proposed approach’s effective-\\nness might be limited to the specific\\ntypes of deepfakes present in the\\ndataset\\nAlthubiti et\\nal.[57]2018 LSTM RNN CSIC 2010 HTTP dataset Web Intrusion De-\\ntectionProposal of LSTM RNN for web\\nintrusion detection. High accuracy\\nrate (0.9997) in binary classifica-\\ntion.The paper only uses the CSIC 2010\\nHTTP dataset, which may not be\\nrepresentative of all types of web\\napplication attacks\\nXu et al.\\n[58]2018 GRU-MLP-Softmax\\n(Gated Recurrent\\nUnit, Multilayer\\nPerceptron,\\nSoftmax)KDD 99 and NSL-KDD data sets Network Intrusion\\nDetectionThe system achieves leading perfor-\\nmance with overall detection rates\\nof 99.42% using KDD 99 and\\n99.31% using NSL-KDD, with low\\nfalse positive ratesThe paper does not provide infor-\\nmation about the scalability of the\\nproposed model\\nFerrag and\\nLeandros\\n[59]2019 DeepCoin\\n(Blockchain and\\nDeep Learning)CICID2017 dataset, Power system\\ndataset, Bot-IoT datasetEnergy framework\\nfor Smart GridsProposal of DeepCoin framework\\ncombining blockchain and deep\\nlearning for smart grid securityThe paper does not address the po-\\ntential scalability issues that may\\narise as the number of nodes in the\\nnetwork increases\\nGhourbi et\\nal.[60]2022 An optimized\\nLightGBM model\\nand a Transformer-\\nbased modelToN-IoT and Edge IIoTset datasets Threat Detection The experimental evaluation of the\\napproach showed remarkable accu-\\nracies of 99%The paper does not discuss the scal-\\nability of the proposed system for\\nlarge-scale healthcare networks\\nThapa et al.\\n[61]2022 Transformer-based\\nlanguage modelsSoftware vulnerability datasets of\\nC/C++ source codesSoftware security\\nand vulnerability\\ndetection in\\nprogramming\\nlanguages,\\nspecifically C/C++The paper highlights the advan-\\ntages of transformer-based language\\nmodels over contemporary modelsThe paper only focuses on detect-\\ning vulnerabilities in C/C++ source\\ncode and does not explore the use\\nof large transformer-based language\\nmodels in detecting vulnerabilities\\nin other programming languages\\nRanade et\\nal.[62]2021 A transformer-based\\nlanguage model,\\nspecifically GPT-2WebText dataset Fake Cyber Threat\\nIntelligenceThe attack is shown to introduce\\nadverse impacts such as returning\\nincorrect reasoning outputsFurther research is needed to ex-\\nplore how to prevent or detect data\\npoisoning attacks on cyber-defense\\nsystem\\nFuet al. [63] 2022 Transformer-\\nbased line-level\\nvulnerability\\nprediction modelLarge-scale real-world dataset with\\nmore than 188k C/C++ functionsSoftware\\nvulnerability\\nprediction in safety-\\ncritical software\\nsystemsThe proposed system is accurate\\nfor predicting vulnerable functions\\naffected by the Top-25 most dan-\\ngerous CWEsThe model’s performance can be\\nchanged when applied to different\\nprogramming languages or software\\nsystems\\nMamede et\\nal.[64]2022 A transformer-based\\ndeep learning modelSoftware Assurance Reference\\nDataset (SARD) project, which\\ncontains vulnerable and non-\\nvulnerable Java filesSoftware security\\nin the context of\\nJava programming\\nlanguageThe proposed system can identify\\nup to 21 vulnerability types and\\nachieved an accuracy of 98.9% in\\nmulti-label classificationThe proposed method cannot be ex-\\ntended to other programming lan-\\nguages and integrated into existing\\nsoftware development processes\\nEvange et\\nal.[65]2021 A transformer-based\\nmodelDNRTI (Dataset for NER in Threat\\nIntelligence)Cybersecurity threat\\nintelligenceThe experimental results demon-\\nstrate that transformer-based tech-\\nniques outperform previous state-\\nof-the-art approaches for NER in\\nthreat intelligenceFurther research is needed to test\\nthe effectiveness of transformer-\\nbased models on larger and more\\ndiverse datasets\\nHashemi et\\nal.[66]2023 Transformer models\\n(including BERT,\\nXLNet, RoBERTa,\\nand DistilBERT)Labeled dataset from vulnerability\\ndatabasesVulnerability Infor-\\nmation ExtractionThe proposed approach outper-\\nforms existing rule-based and CRF-\\nbased modelsThe paper does not address the is-\\nsue of bias in the labeled dataset\\non a large-scale real-world patch dataset and a case study\\non NGINX. The results indicate that the PatchRNN system\\ncan effectively detect secret security patches with a low falsepositive rate.\\n3) Detection of Deepfake Videos: G¨uera et al. [56] propose\\na temporal-aware pipeline that automatically detects deepfake\\n12', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='21fc35fc-3203-4682-acd8-53ccdacbebc7', embedding=None, metadata={'page_label': '13', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE III: LLMs-based models for Cyber Security (Part II).\\nStudy Year Type of Model Dataset Used Domain Key Contributions Open Issues\\nChawla et\\nal.[67]2019 GRU with CNN ADFA (Australian Defence Force\\nAcademy) datasetIntrusion Detection Achieved improved performance by\\ncombining GRUs and CNNsThe proposed system is vulnerable\\nto adversarial attacks\\nUllah et al.\\n[68]2022 LSTM, BiLSTM,\\nand GRUIoT-DS2, MQTTset, IoT-23, and\\ndatasetsIntrusion Detection Validation of the proposed mod-\\nels using various datasets, achieving\\nhigh accuracy, precision, recall, and\\nF1 scoreFurther research is necessary to en-\\nhance their scalability for practical\\napplications in cybersecurity\\nDonkol et\\nal.[69]2023 LSTM CSE-CIC-IDS2018, CICIDS2017, and\\nUNSW-NB15 datasetsIntrusion Detection The proposed system outperformed\\nother methods such as LPBoost and\\nDNNs in terms of accuracy, preci-\\nsion, recall, and error rateFuture research could explore the\\napplicability of the proposed system\\nto other datasets\\nZhao et al.\\n[70]2023 End-to-End Recur-\\nrent Neural NetworkIDS2017 and IDS2018 datasets Intrusion attacks\\nand malware+ Address network-induced phe-\\nnomena that may result in misclas-\\nsifications in traffic detection sys-\\ntems used in cybersecurityThe proposed system is vulnerable\\nto adversarial attacks\\nWang et al.\\n[71]2021 RNN A large-scale patch dataset PatchDB Software Security The PatchRNN system can effec-\\ntively detect secret security patches\\nwith a low false positive rateThe PatchRNN system can only\\nsupport C/C++\\nPolat et al.\\n[72]2022 LSTM and GRU SDN-based SCADA system Detection of DDoS\\nattacksThe results show that the proposed\\nRNN model achieves an accuracy\\nof 97.62% for DDoS attack detec-\\ntionThe paper only focuses on detecting\\nDDoS attacks and does not address\\nother types of cyber threats (e.g., in-\\nsider threats or advanced persistent\\nthreats)\\nLiu et al.\\n[73]2022 Transformer model A commit benchmark dataset that\\nincludes over 7.99 million commits\\nacross 7 programming languagesCommit message\\ngeneration\\n(generation task)\\nand security patch\\nidentification\\n(understanding task)The experimental results demon-\\nstrate that CommitBART signifi-\\ncantly outperforms previous pre-\\ntrained models for codeThe pre-training dataset used in the\\npaper is limited to GitHub commits\\nAhmad et al.\\n[74]2024 Transformer model Set of 15 hardware security bug\\nbenchmark designs from three\\nsources: MITRE website, OpenTitan\\nSystem-on-Chip (SoC) and the\\nHack@DAC 2021 SoCHardware Security\\nBugsBug repair potential demonstrated\\nby ensemble of LLMs, outperform-\\ning state-of-the-art automated toolThe need for designer assistance\\nin bug identification, handling com-\\nplex bugs, limited evaluations due\\nto simulation constraints, and chal-\\nlenges with token limits and repair\\ngeneration using LLMs\\nWan et al.\\n[75]2024 Transformer model Chrysalis dataset, comprising over\\n1,000 function-level HLS designs with\\ninjected logical bugsDesign Verification Creating the Chrysalis dataset\\nfor HLS debugging, and enabling\\nLLM-based bug detection and\\nintegration into development\\nenvironmentsRefining LLM techniques, integrat-\\ning LLMs into development envi-\\nronments, and addressing scalabil-\\nity and generalization challenges\\nJang et al.\\n[76]2024 Transformer model Includes 150K online security articles,\\n7.3K security paper abstracts, 3.4K\\nWikipedia articles, and 185K CVE\\ndescriptions.Threat Detection Pre-trained language model for\\nthe cybersecurity domain, CyBER-\\nTuned incorporates non-linguistic\\nelements (NLEs) such as URLs and\\nhash values commonly found in cy-\\nbersecurity texts.The paper’s limitations include a\\nnarrow focus on specific non-\\nlinguistic element (NLE) types, ac-\\nknowledging the existence of more\\ncomplex NLE types like code\\nblocks and file paths that require\\nfuture exploration\\nBayer et al.\\n[77]2024 Transformer model A dataset consisting of 4.3 million\\nentries of Twitter, Blogs, Paper, and\\nCVEs related to the cybersecurity do-\\nmainIntrusion attacks\\nand malwareCreated a high-quality dataset and\\na domain-adapted language model\\nfor the cybersecurity domain, which\\nimproves the internal representation\\nspace of domain words and per-\\nforms best in cybersecurity scenar-\\niosthe model may not be suitable as\\na replacement for every type of cy-\\nbersecurity model. They also state\\nthat the hyperparameters may not\\nbe generalizable to other language\\nmodels, especially very large lan-\\nguage models\\nShestov et\\nal.[78]2024 Transformer model The dataset comprises 22,945\\nfunction-level source code samples. It\\nincludes 13,247 samples for training,\\n5,131 for validation, and 4,567 for\\ntestingVulnerability detec-\\ntionFinetuning the state-of-the-art code\\nLLM, WizardCoder, increasing its\\ntraining speed without performance\\nharm.The proposed study shows that the\\nmain bottlenecks of the task that\\nlimit performance lie in the field\\nof dataset quality and suggests the\\nusage of the project-level context\\ninformation\\nHe et al.\\n[79]2024 Transformer model Used three datasets: one with over\\n100,000 entries from Ethereum main-\\nnet contracts, another with 892,913\\naddresses labelled across five vulner-\\nability categories, and a third with\\n6,498 smart contracts, including 314\\nassociated with Ponzi schemesblockchain technol-\\nogy and smart con-\\ntractsThe introduction of a novel model,\\nBERT-ATT-BiLSTM, for advanced\\nvulnerability detection in smart\\ncontracts, and the evaluation of its\\nperformance against other modelsInclude the model’s limitation in\\nrecognizing unseen contract struc-\\ntures or novel types of vulnerabil-\\nities, and the need to incorporate\\nsupport for multiple programming\\nlanguages to enhance universality\\nand robustness\\nJamal et al.\\n[21]2024 Transformer model Two open-source datasets, 747 spam,\\n189 phishing, 4825 ham; class imbal-\\nance addressed with ADASYNPhishing and spam\\ndetectionProposing IPSDM, a fine-tuned ver-\\nsion of DistilBERT and RoBERTA,\\noutperforming baseline models and\\nthe demonstration of the effective-\\nness of LLMs in addressing cyber-\\nsecurity challengesClass imbalance, addressed with\\nADASYN, but potential bias re-\\nmains\\nvideos by using a convolutional neural network (CNN) to\\nextract frame-level features and a recurrent neural network\\n(RNN) to classify the videos. The results show that the system\\ncan achieve competitive results in this task with a simple\\narchitecture.Overall, the reviewed studies demonstrate the potential of\\ndeep learning methods, particularly RNNs, for intrusion detec-\\ntion in various domains. The results show that the proposed\\ndeep learning-based models outperform traditional machine\\nlearning methods in accuracy. However, more research is\\n13', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3022adb8-cc4d-4208-8e48-3fca0984fe6a', embedding=None, metadata={'page_label': '14', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Hardware\\nSecurity BugsIntrusion\\nDetection\\nCloud Threat\\nForensics\\nMalware\\nclassification\\nSoftware\\nVulnerability\\nDetectionCyber Threat\\nIntelligenceDesign\\nVerificationPolat et al. (2022)Zhang et al.  (2023)Weimin et al.  (2023)Mohamadreza et al.  (2024)\\nAhmad et al. (2024)\\nGhourbi et al. (2022)\\nAlthubiti et al.  (2018)Zhao et al. (2023)Donkol et al. (2023)\\nYin et al. (2017) \\nChawla et al. (2019)\\nUllah et al. (2022)\\nXu  et al.(2018)\\nFerrag and Leandros (2019)Evange et al. (2021)\\nParra et al. (2022)\\nZiems et al. (2021)Demirkiran et al. (2022)\\nFu et al. (2022)\\nDing et al. (2024)Thapa et al. (2022)\\nLiu et al. (2022)\\nShestov et al. (2024)Mamede et al. (2022)\\nRanade et al. (2021)\\nHashemi et al. (2023)\\nPerrina et al. (2023)Sun et al. (2024)Wan et al.  (2024)LLM-based\\nSolutions for\\nCyber Security\\nUse CasesPhishing and\\nspam detectionKoide et al. (2024)\\nJamal et al. (2024)\\nHeiding et al. (2024)Chataut et al. (2024)\\nProtocols\\nverification\\nRuijie et al.  (2024)Fig. 4: LLM-based Solutions for Cyber Security Use Cases.\\nneeded to address the limitations and challenges associated\\nwith these approaches, such as data scalability and inter-\\npretability.\\nB. Transformer-based models\\n1) Cloud Threat Forensics: Parra et al. [51] proposed\\nan interpretable federated transformer log learning model\\nfor threat detection in syslogs. The model is generated by\\ntraining local transformer-based threat detection models at\\neach client and aggregating the learned parameters to generate\\na global federated learning model. The authors demonstrate\\nthe difference between normal and abnormal log time series\\nthrough the goodness of fit test and provide insights into\\nthe model’s decision-making process through an attention-\\nbased interpretability module. The results from the HDFS and\\nCTDD datasets validate the proposed approach’s effectiveness\\nin achieving threat forensics in real-world operational set-\\ntings. Evange et al. [65] discuss the importance of actionable\\nthreat intelligence in defending against increasingly sophis-\\nticated cyber threats. Cyber Threat Intelligence is available\\non various online sources, and Named Entity Recognition\\n(NER) techniques can extract relevant information from these\\nsources. The paper investigates the use of transformer-based\\nmodels in NER and how they can facilitate the extraction\\nof cybersecurity-related named entities. The DNRTI dataset,\\nwhich contains over 300 threat intelligence reports, tests the ef-fectiveness of transformer-based models compared to previous\\napproaches. The experimental results show that transformer-\\nbased techniques are more effective than previous methods in\\nextracting cybersecurity-related named entities.\\n2) Malware classification: Ziems et al. [52] explore\\ntransformer-based models for malware classification using API\\ncall sequences as features. The study compares the perfor-\\nmance of the traditional machine and deep learning models\\nwith transformer-based models. It shows that transformer-\\nbased models outperform traditional models in terms of F1-\\nscore and AUC score. The authors also propose a bagging-\\nbased random transformer forest (RTF) model that reaches\\nstate-of-the-art evaluation scores on three out of four datasets.\\nDemirkıran et al. [54] proposes using transformer-based mod-\\nels for classifying malware families, better suited for capturing\\nsequence relationships among API calls than traditional ma-\\nchine and deep learning models. The experiments show that\\nthe proposed transformer-based models outperform traditional\\nmodels such as LSTM and pre-trained models such as BERT\\nor CANINE in classifying highly imbalanced malware families\\nbased on evaluation metrics like F1-score and AUC score.\\nAdditionally, the proposed bagging-based random transformer\\nforest (RTF) model, an ensemble of BERT or CANINE,\\nachieves state-of-the-art performance on three out of four\\ndatasets, including a state-of-the-art F1-score of 0.6149 on one\\nof the commonly used benchmark datasets.\\n14', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f3d00812-78b2-4b66-8994-220b9718d59a', embedding=None, metadata={'page_label': '15', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3) Intrusion Detection: Wuet al. [53] proposed an RTID\\nthat reconstructs feature representations in imbalanced datasets\\nto make a trade-off between dimensionality reduction and\\nfeature retention. The proposed method utilizes a stacked\\nencoder-decoder neural network and a self-attention mech-\\nanism for network traffic type classification. The results\\nwith CICID2017 and CIC-DDoS2019 datasets demonstrate\\nthe proposed method’s effectiveness in intrusion detection\\ncompared to classical machine learning and deep learning\\nalgorithms. Ghourbi et al. [60] propose an intrusion and\\nmalware detection system to secure the entire network of the\\nhealthcare system independently of the installed devices and\\ncomputers. The proposed solution includes two components:\\nan intrusion detection system for medical devices installed\\nin the healthcare network and a malware detection system\\nfor data servers and medical staff computers. The proposed\\nsystem is based on optimized LightGBM and Transformer-\\nbased models. It is trained with four different datasets to\\nensure a varied knowledge of the different attacks affecting the\\nhealthcare sector. The experimental evaluation of the approach\\nshowed remarkable accuracies of 99%.\\n4) Software Vulnerability Detection: Thapa et al. [61]\\nexplores the use of large transformer-based language models\\nin detecting software vulnerabilities in C/C++ source code,\\nleveraging the transferability of knowledge gained from nat-\\nural language processing. The paper presents a systematic\\nframework for source code translation, model preparation,\\nand inference. It conducts an empirical analysis of software\\nvulnerability datasets to demonstrate the good performance of\\ntransformer-based language models in vulnerability detection.\\nThe paper also highlights the advantages of transformer-\\nbased language models over contemporary models, such as\\nbidirectional long short-term memory and bidirectional gated\\nrecurrent units, in terms of F1-score. However, the paper does\\nnot discuss the limitations or potential drawbacks of using\\ntransformer-based language models for software vulnerability\\ndetection, and further research is needed in this area. Fu et\\nal.[63] propose an approach called LineVul, which uses a\\nTransformer-based model to predict software vulnerabilities\\nat the line level. The approach is evaluated on a large-scale\\ndataset (i.e., on a large-scale real-world dataset with more\\nthan 188k C/C++ functions). It achieves higher F1-measure\\nfor function-level predictions and higher Top-10 accuracy\\nfor line-level predictions compared to baseline approaches.\\nThe analysis also shows that LineVul accurately predicts\\nvulnerable functions affected by the top 25 most dangerous\\nCWEs. However, the model’s performance can be changed\\nwhen applied to different programming languages or software\\nsystems.\\nMamede et al. [64] presented a transformer-based VS Code\\nextension that uses state-of-the-art deep learning techniques\\nfor automatic vulnerability detection in Java code. The authors\\nemphasize the importance of early vulnerability detection\\nwithin the software development life cycle to promote applica-\\ntion security. Despite the availability of advanced deep learn-\\ning techniques for vulnerability detection, the authors note that\\nthese techniques are not yet widely used in development envi-\\nronments. The paper describes the architecture and evaluationof the VDet tool, which uses the Transformer architecture for\\nmulti-label classification of up to 21 vulnerability types in Java\\nfiles. The authors report an accuracy of 98.9% for multi-label\\nclassification and provide a demonstration video, source code,\\nand datasets for the tool.\\nLiuet al. [73] introduce CommitBART, a pre-trained Trans-\\nformer model specifically designed to understand and generate\\nnatural language messages for GitHub commits. The model is\\ntrained on a large dataset of over 7.99 million commits, cov-\\nering seven different programming languages, using a variety\\nof pre-training objectives, including denoising, cross-modal\\ngeneration, and contrastive learning, across six pre-training\\ntasks. The authors propose a ”commit intelligence” framework\\nencompassing one understanding task and three generation\\ntasks for commits. The experimental results demonstrate that\\nCommitBART significantly outperforms previous pre-trained\\nmodels for code, and the analysis suggests that each pre-\\ntraining task contributes to the model’s performance.\\nDing et al. [80] discuss the effectiveness of code language\\nmodels (code LMs) in detecting vulnerabilities. It identifies\\nsignificant issues in current datasets, such as poor quality,\\nlow accuracy, and high duplication rates, which compromise\\nmodel performance in realistic scenarios. To overcome these\\nchallenges, it introduces the PrimeVul dataset, which uses\\nadvanced data labeling, de-duplication, and realistic evaluation\\nmetrics to represent real-world conditions accurately. The find-\\nings reveal that current benchmarks, like the BigVul, greatly\\noverestimate code LMs’ capabilities, with much lower per-\\nformance observed on PrimeVul. This significant discrepancy\\nhighlights the need for further innovative research to meet the\\npractical demands of deploying code LMs in security-sensitive\\nenvironments.\\n5) Cyber Threat Intelligence: Ranade et al. [62] presented\\na method for automatically generating fake Cyber Threat In-\\ntelligence (CTI) using transformers, which can mislead cyber-\\ndefense systems. The generated fake CTI is used to perform a\\ndata poisoning attack on a Cybersecurity Knowledge Graph\\n(CKG) and a cybersecurity corpus. The attack introduces\\nadverse impacts such as returning incorrect reasoning outputs,\\nrepresentation poisoning, and corruption of other dependent\\nAI-based cyber defense systems. A human evaluation study\\nwas conducted with cybersecurity professionals and threat\\nhunters, which reveals that professional threat hunters were\\nequally likely to consider the generated fake CTI and authentic\\nCTI as true.\\nHashemi et al. [66] propose an alternative approach for\\nautomated vulnerability information extraction using Trans-\\nformer models, including BERT, XLNet, RoBERTa, and Dis-\\ntilBERT, to extract security-related words and terms and\\nphrases from descriptions of vulnerabilities. The authors fine-\\ntune several language representation models similar to BERT\\non a labeled dataset from vulnerability databases for Named\\nEntity Recognition (NER) to extract complex features without\\nrequiring domain-expert knowledge. This approach outper-\\nforms the CRF-based models and can detect new information\\nfrom vulnerabilities with different description text patterns.\\nThe authors conclude that this approach provides a structured\\nand unambiguous format for disclosing and disseminating vul-\\n15', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9dfcbd73-f8f6-47cb-a19f-6cf177c57850', embedding=None, metadata={'page_label': '16', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='nerability information, which is crucial for preventing security\\nattacks.\\n6) Phishing and spam detection: Koide et al. introduced\\n[81], a novel system leveraging LLMs to detect phishing\\nemails. Despite advances in traditional spam filters, significant\\nchallenges such as oversight and false positives persist. The\\nsystem transforms email data into prompts for LLM analy-\\nsis, achieving a high accuracy rate (99.70%) and providing\\ndetailed reasoning for its determinations. This helps users\\nmake informed decisions about suspicious emails, potentially\\nenhancing the effectiveness of phishing detection.\\nJamal et al. [21] explored the potential of LLMs to address\\nthe growing sophistication of phishing and spam attacks. Their\\nwork, IPSDM, is an improved model based on the BERT\\nfamily, specifically fine-tuned to detect phishing and spam\\nemails. Compared to baseline models, IPSDM shows superior\\naccuracy, precision, recall, and F1-score performance on both\\nbalanced and unbalanced datasets while addressing overfitting\\nconcerns.\\nHeiding et al. [82] compared automatically generated phish-\\ning emails by GPT-4, manually designed emails using the V-\\nTriad method, and their combination. Their findings suggest\\nthat emails designed with the V-Triad achieved the highest\\nclick-through rates, indicating the effectiveness of exploiting\\ncognitive biases. The study also evaluated the capability of\\nfour different LLMs to detect phishing intentions, with results\\noften surpassing human detection. Furthermore, they discuss\\nthe economic impact of AI in lowering the costs of orches-\\ntrating phishing attacks.\\nChataut et al. [83] focused on the effectiveness of LLMs\\nin detecting phishing emails amidst threat actors’ constant\\nevolution of phishing strategies. Their study emphasizes the\\nnecessity for continual development and adaptation of detec-\\ntion models to keep pace with innovative phishing techniques.\\nThe role of LLMs in this context highlights their potential to\\nsignificantly enhance email security by improving detection\\ncapabilities.\\n7) Hardware Security Evaluation: Ahmad et al. [74]\\ndelves into leveraging LLMs to automatically repair identified\\nsecurity-relevant bugs present in hardware designs, explicitly\\nfocusing on Verilog code. Hardware security bugs pose signif-\\nicant challenges in ensuring the reliability and safety of hard-\\nware designs. They curated a corpus of hardware security bugs\\nthrough a meticulously designed framework. They explored\\nthe performance of various LLMs, including OpenAI’s Codex\\nand CodeGen, in generating replacement code to fix these\\nbugs. The experiments reveal promising results, demonstrating\\nthat LLMs can effectively repair hardware security bugs, with\\nsuccess rates varying across different bugs and LLM mod-\\nels. By optimizing parameters such as instruction variation,\\ntemperature, and model selection, they achieved successful\\nrepairs for a significant portion of the bugs in their dataset. In\\naddition, the results demonstrate that LLMs, including GPT-\\n4, code-davinci-002, and code-cushman-001, yield successful\\nrepairs for simple security bugs, with GPT-4 achieving a\\nsuccess rate of 67% at variation e, temp 0.5. However, LLMs’\\nperformance varies across bugs, showing success rates over\\n75% with some bugs, while others are more challenging torepair, with success rates below 10%. The study emphasizes\\nthe importance of detailed prompt instructions, with variation\\nd showing the highest success rate among OpenAI LLMs.\\nFurther investigation is needed to evaluate LLMs’ scalability\\nand effectiveness for diverse hardware security bug scenarios.\\nTheir findings underscore the potential of LLMs in automating\\nthe bug repair process in hardware designs, marking a crucial\\nstep towards developing automated end-to-end bug repair tools\\nfor hardware security.\\nMohamadreza et al. [84] explored the potential of using\\nlarge language models to enhance the input generation in the\\nprocess of hardware design verification for security-related\\nbugs. Mohamadreza et al. introduced Chatfuzz, a novel ML-\\nbased hardware fuzzer that leverages LLMs and reinforce-\\nment learning to generate complex and random machine\\ncode sequences for exploring processor security vulnerabil-\\nities. Chatfuzz introduces a specialized LLM model into a\\nhardware fuzzing approach to enhance the input generation\\nquality, outperforming the existing approaches regarding cov-\\nerage, scalability, and efficiency. Utilizing LLMs to understand\\nprocessor language and generate data/control flow entangled\\nmachine code sequences, Chatfuzz integrates RL to guide\\ninput generation based on code coverage metrics. Their ex-\\nperiment on real-world cores, namely RocketCore and BOOM\\ncores, showed significantly faster coverage than state-of-the-art\\nhardware fuzzes. ChatFuzz achieves 75% condition coverage\\nin RocketCore in 52 minutes and 97.02% in BOOM in 49\\nminutes, identifying unique mismatches and new bugs and\\nshowcasing its effectiveness in hardware security testing.\\nWeimin et al. [85] introduces LLM4SECHW, a novel frame-\\nwork for hardware debugging that utilizes domain-specific\\nLarge Language Models. The authors addressed the limitations\\nof out-of-the-shelf LLMs in the hardware security domain by\\ngathering a dataset of hardware design defects and remediation\\nsteps. The collected dataset has been built by leveraging open-\\nsourced hardware designs from GitHub; the data consists of\\ndifferent Hardware Description Language modules with their\\nrespective commits. By harnessing version control informa-\\ntion from open-source hardware projects and processing it\\nto create a debugging-oriented dataset, LLM4SECHW fine-\\ntunes hardware domain-specific language models to locate\\nand rectify bugs autonomously, enhancing bug localization.\\nLLM4SECHW has been evaluated with two objectives: bug\\nidentification and design patching. The authors demonstrated\\nthat non-fine-tuned LLMs lack hardware domain knowledge,\\nwhich makes them incapable of locating bugs in the hard-\\nware design of a popular security-specialized chip project\\nnamed OpenTitan. The based models (falcon 7b, llama 2,\\nBard, chatbot, and stableLM) did not efficiently locate the\\nintroduced hardware bugs. The three fine-tuned models (falcon\\n7b, llama2, stableLM) successfully located the introduced bugs\\nin the hardware design.\\nZhang et al. [85] introduces Hardware Phi-1.5B, a large\\nlanguage model tailored for the hardware domain of the semi-\\nconductor industry, addressing the complexity of hardware-\\nspecific issues. The research focused on developing datasets\\nspecifically for the hardware domain to enhance the model’s\\nperformance in comprehending complex terminologies. The\\n16', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aef90e7a-717d-4f54-b83d-729b3ea4697f', embedding=None, metadata={'page_label': '17', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='authors claim to surpass general code language models and\\nnatural language models like CodeLlama, BERT, and GPT-2\\nin the Hardware understanding tasks.\\nMadhav et al. [86] evaluated the security of the HDL\\ncode generated by ChatGPT. The authors introduced a similar\\ntaxonomy to the NIST CWE []. The authors conducted various\\nexperiments to explore the impact of prompt engineering on\\nthe security of the generated hardware design.\\n8) Hardware design & Verification: Lily et al. [87] in-\\ntroduced the application of LLMs into High-Level Synthesis\\nDesign Verification (HLS). The authors created a dataset\\nnamed Chrysalis to solve the problem of the non-existence of\\nspecialized HLS bug detection and evaluation capabilities. The\\nChrysalis dataset comprises over 1000 function-level designs\\nextracted from reputable sources with intentionally injected\\nknown bugs to evaluate and refine LLM-based HLS bug\\nlocalization. The set of the introduced bugs was selected based\\non the most common human coding errors and has been shaped\\nto elude most of the existing conventional HLS synthesis\\ntools detection mechanisms. The paper’s authors suggest that\\nChrysalis would contribute to the LLM-aided HLS design\\nverification by offering a benchmarking to the existing and\\nspecialized models. The paper also suggests a prompt engi-\\nneering approach that would enhance the efficiency of a large\\nlanguage model on the studied task. The proposed prompt\\nstructure introduces a separation of concern approach, where\\nthe used prompt deals with each class of bugs separately. The\\nprompt starts by explicitly defining the context of the task,\\nthe functional description, the implementation context, and the\\ntask objective. The prompt is implemented through three main\\nsections: context, requirements, and complementary rules. The\\nhighlighted works lay a foundation for a methodological,\\npractical approach to benchmarking, evaluating, and deploying\\nLLM tasks for HLS design verification. While the paper does\\nnot provide any conclusive results about LLMs’ performance\\nin such tasks, the authors believe that such methodology would\\naccelerate the adoption of new techniques to integrate LLMs\\ninto the design verification flow.\\nMingjie et al. [88] evaluated the LLMs’ performance in\\nsolving Verilog related design tasks and generating design\\ntestbenchs by introducing VerilogEval. VerilogEval comprises\\ndifferent hardware design tasks ranging from module imple-\\nmentation of simple combinatorial circuits to complex finite\\nstate machines, code debugging, and testbench construction.\\nVerilogEval suggests an end-to-end evaluation framework that\\nfits better in the context of the hardware design verification\\nprocess benchmarking. The VerilogEval framework validates\\nthe correctness of the prompted tasks by comparing the\\nbehavior simulation to an established golden model of the\\nprompted design. The authors used pass@k metric instead\\nof the generic NLP related metrics like the BLEU score\\nprobability metric. The study demonstrates that pre-trained\\nlanguage models’ Verilog code generation capabilities can be\\nimproved through supervised fine-tuning. The experimental re-\\nsults show that fine-tuning LLMs on the hardware design tasks\\nand using the pass@k metric helps assess the performance of\\nthe resulting models properly. The pass@k metric helps assess\\nthe performance of Large Language Models (LLMs) in Verilogcode generation by quantifying the number of successful code\\ncompletions out of k samples, offering a clear evaluation\\ncriterion. The used metric shows that a fine-tuned model\\ncould have equal or better performance than the state-of-the-\\nart OpenAI models (gpt-3 and gpt-4). VerilogEval highlights\\nthe growing significance of Large Language Models (LLMs)\\nand their application in various domains, emphasizing their\\npotential in Verilog code generation for hardware design and\\nverification. The findings underscore the importance of the\\nproposed benchmarking framework in advancing the state of\\nthe art in Verilog code generation, highlighting the vast poten-\\ntial of LLMs in assisting the hardware design and verification\\nprocess.\\n9) Protocols verification: Ruijie et al. [90] introduced\\nChatAFL, an LLM-based protocol fuzzer. ChatAFL introduces\\nan LLM-guided protocol fuzzing to address the challenge of\\nfinding security flaws in protocol implementations without\\na machine-readable specification. The study suggests three\\nstrategies for integrating an LLM into a mutation-based proto-\\ncol fuzzer, focusing on grammar extraction, seed enrichment,\\nand saturation handling to enhance code coverage and state\\ntransitions. ChatAFL prototype implementation demonstrates\\nthat the LLM-guided stateful fuzzer outperforms state-of-the-\\nart fuzzers like AFLNET [91] and NSFUZZ [92] in terms of\\nprotocol state space coverage and code coverage.\\nThe experiments evaluated CHATAFL’s improvement over\\nthe baselines in terms of transition coverage achieved in\\n24 hours, speed-up in achieving the same coverage, and\\nthe probability of outperforming the baselines in a random\\ncampaign. CHATAFL demonstrated significant efficacy by\\ncovering 47.60% and 42.69% more state transitions, 29.55%\\nand 25.75% more states, and 5.81% and 6.74% more code\\nthan AFLNET and NSFUZZ, respectively.\\nCHATAFL discovered nine unique and previously unknown\\nvulnerabilities in widely used and extensively tested proto-\\ncol implementations on real widely used projects (live555,\\nproFTPD, kamailio). The discovered vulnerabilities encom-\\npass various memory vulnerabilities, including use-after-free,\\nbuffer overflow, and memory leaks, which have potential se-\\ncurity implications such as remote code execution or memory\\nleakage. The study demonstrated the effectiveness of utilizing\\nLLMs for guiding protocol fuzzing to enhance state and code\\ncoverage in protocol implementations.\\nWang et al. [93] introduced LLMIF and LLM-aided fuzzing\\napproach for IoT devices protocols. LLMIF introduces an\\nLLM augmentation-based approach. The developed pipeline\\nincorporates an enhanced seed generation strategy by building\\nan augmentation based on domain knowledge. The domain\\nknowledge structure is extracted from the various specifi-\\ncations of the under-fuzzing protocol. The flow starts by\\nselecting a seed from the extracted augmentation set and\\nthen enriching the extracted seed by exploring the protocol\\nspecification. The enriching process is driven by the various\\nranges of input values extracted during the augmentation\\nphase. Furthermore, LLMIF introduces a coverage approach\\nby mutating the selected seed through the various enrichment\\nand mutation operators that have been selected.\\nThe evaluation part of LLMIF mainly aimed to evaluate\\n17', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a5f5aba-d554-4d73-b240-214d6456e8cb', embedding=None, metadata={'page_label': '18', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='80 \\nQuestions\\n500\\nQuestions2 k\\nQuestions\\n10 k\\nQuestions\\nCyberMetric Dataset\\nInference Model\\nGpt-4-turbo, Gpt-3.5-turbo, Mixtral-8x7B-Instruct,\\nGEMINI-pro (Bard), Falcon-180B-Chat, Flan-T5-XXL,\\nZephyr-7B-beta, Llama 2-70B, Falcon-40B-Instruct,\\nFlan-T5-Base, ....Total Correct \\nTotal Questions\\nAccuracy: %\\nrole_prompt =  \"You are a security expert\\nwho answers questions. \"\\n \\nuser_prompt =  f\"Question:\\n{question}\\\\nOptions: {\\', \\'.join([f\\'{key}) {value}\\'\\nfor key , value in\\nanswers.items()])}\\\\n\\\\nChoose the correct\\nanswer (A, B, C, or D) only . Always return in\\nthis format: \\'ANSWER: X \\'\"\\nLLM prompt\\nengineering\\nQ1: In cryptography , what is the purpose of a message\\nauthentication code (MAC) or digital signature (SIG)?\\nQ2: \"What is the role of Uncoordinated Frequency Hopping\\n(UFH) in anti-jamming broadcast communication?\\n...Fig. 5: LLMs Performance Steps in the cybersecurity domain using CyberMetric Dataset [89].\\nthree axes: code coverage, ablation, and bug identification.\\nThe authors used an out-of-the-shelf popular SOC (CC2530)\\nfor the evaluation. 11 commercial devices have been selected\\nto conduct the various experiments. While the ablation and\\nbug detection could be easily evaluated, the code coverage is\\nimpossible using the custom firmware that ships with the se-\\nlected devices. The authors used an open-source Zigbee stack\\nto demonstrate the coverage capabilities. The authors claimed\\nthat LLMIF outperforms Z-FUZZER [94], and BOOFUZZ\\n[95] in terms of code coverage for the target Zigbee stack.\\nThe authors claim that LLMIF achieved a notable increase\\nin protocol message coverage and code coverage by 55.2%\\nand 53.9%, respectively, outperforming other Zigbee fuzzers\\nin these aspects.\\nLLMIF algorithm successfully uncovered 11 vulnerabilities\\non real-world Zigbee devices, including eight previously un-\\nknown vulnerabilities, showcasing its effectiveness in iden-\\ntifying security flaws in IoT devices. By incorporating the\\nlarge language model into IoT fuzzing, LLMIF demonstrated\\nenhanced capabilities in protocol message coverage and vul-\\nnerability discovery, highlighting its potential for improving\\nthe security testing of IoT devices.\\nV. G ENERAL LLM S\\nTables VI, VII, VIII compare general transformer-based\\nLarge Language Models. LLM models are generally trained on\\na diverse and broad range of data to provide a relatively com-\\nprehensive understanding. They can handle various language\\ntasks like translation, summarization, and question-answering.\\nIn contrast, code-specific LLMs are specialized models trained\\nprimarily on programming languages and related technical\\nliterature, which makes their primary role in understanding\\nand generating programming code well-suited for tasks like\\nautomated code generation, code completion, and bug detec-\\ntion.A. Prevalent LLMs\\n1) GPT-3: GPT-3 (the third version of the Generative Pre-\\ntrained Transformer series by OpenAI) was developed to prove\\nthat scaling language models substantially improves their task-\\nagnostic few-shot performance [96]. Based on transformer\\narchitecture, GPT-3 has eight variants ranging between 125M\\nand 175B parameters, all trained for 300B tokens from datasets\\nlike Common Crawl, WebText, Books, and Wikipedia. Addi-\\ntionally, the models were trained on V100 GPU leveraging\\ntechniques like autoregressive training, scaled cross-entropy\\nloss, and others. GPT-3, especially its most capable 175B\\nversion, has demonstrated strong performance on many NLP\\ntasks in different settings (i.e., zero-shot, one-shot, and few-\\nshots), suggesting it could significantly improve cybersecurity\\napplications if appropriately fine-tuned. This could translate\\nto more effective Phishing Detection through precise language\\nanalysis, faster Incident Response, and other critical applica-\\ntions to enhance digital security measures.\\n2) GPT-4: In 2023, the GPT-4 transformer-based model\\nwas released by OpenAI as the first large-scale multi-\\nmodal model, exhibiting unprecedented performance in var-\\nious benchmarks. The model’s capability of processing image\\nand text inputs has shifted the AI paradigm to a new level,\\nexpanding beyond traditional NLP. [97] declared that GPT-\\n4 was trained using a vast corpus of web-based data and\\ndata licensed from third-party sources with autoregressive\\ntechniques and Reinforcement Learning from Human Feed-\\nback (RLHF). However, other specifics, such as the model\\nsize, data size, and comprehensive training details, remain\\nundisclosed. Although GPT-4 could potentially be leveraged\\nby cybercriminals for a wide range of attacks, such as social\\nengineering, if implemented strategically, it can also help\\nreduce the likelihood of individuals and organizations falling\\nprey to them.\\n3) T5: Motivated by the trend of applying transfer learning\\nfor NLP, researchers of Google have introduced T5 [98], an\\n18', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e400b4b7-fe14-4fd3-b365-0bf75d9988b2', embedding=None, metadata={'page_label': '19', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE IV: Comparison of 19 LLMs Models’ Performance in Hardware Security Knowledge.\\nLLM model Size Design bug detectionHardware CWE Number\\n1245 1221 1224 1298 1254 1209 1223 1234 1231\\nLlama 3-7b-instruct 8B 39.556% Yes No No No Yes No No Yes No\\nMixtral-8x7B-Instruct 8x7B 16.154% No No No No No No No No No\\nDolphin-mistral-7B 7B 16,024% Yes Yes No No No No No No No\\nCodegemma-9b-instruct 9B 10.746% No No No No No No No No Yes\\nCodeQwen-7b-instruct 7B 10.269% No No No No No No No No No\\nWizard-vicuna-uncensored-7b-instruct 7B 9.374% No No No No No No No No No\\nMistral-openorca-7b-instruct 7B 8.241% No No No No No Yes No No No\\nWizardlm2-7b-instruct 7B 5.646% No No No No No No No No No\\nLlama2-uncensored-7b-instruct 7B 2.505% No No No No No No No No No\\nFalcon-40b-instruct 40B 1.620% No No No No No No No No No\\nDeepseek-coder-33b-instruct 33B 1.570% No No No No No No No No No\\nOrca-mini-3b-instruct 3B 1.173% No No Yes No No No No No No\\nQwen2-4b-instruct 4B 0.576% No No No No No No No No No\\nCodeLlama-7b-instruct 7B 0.218% No No No No No No No No No\\nPhi3-4b-instruct 4B 0.019% No No No No No No No No No\\nHardware-Phi 1.5B 0% No No No No No No No No No\\nLlava-13b-instruct 13B 0% No No No No No No No No No\\nGemma-9b-instruct 9B 0% No No No No No No No No No\\nStarcoder2-15b-instruct 15B 0% No No No No No No No No No\\nYes: Detected the CWE sample by MITRE, No: Did not Detect the CWE sample by MITRE. CWE: Common Weakness Enumeration.\\nTABLE V: Comparison of 42 LLMs Models’ Performance in Cyber Security Knowledge.\\nLLM model Company Size LicenseAccuracy\\n80 Q 500 Q 2k Q 10k Q\\nGPT-4o OpenAI N/A Proprietary 96.25% 93.40% 91.25% 88.89%\\nGPT-4-turbo OpenAI N/A Proprietary 96.25% 93.30% 91.00% 88.50%\\nMixtral-8x7B-Instruct Mistral AI 45B Apache 2.0 92.50% 91.80% 91.10% 87.00%\\nFalcon-180B-Chat TII 180B Apache 2.0 90.00% 87.80% 87.10% 87.00%\\nGEMINI-pro 1.0 Google 137B Proprietary 90.00% 85.05% 84.00% 87.50%\\nGPT-3.5-turbo OpenAI 175B Proprietary 90.00% 87.30% 88.10% 80.30%\\nYi-1.5-9B-Chat 01-ai 9B Apache 2.0 87.50% 80.80% 77.15% 76.04%\\nHermes-2-Pro-Llama-3-8B NousResearch 8B Open 86.25% 80.80% 77.95% 77.33%\\nDolphin-2.8-mistral-7b-v02 Cognitive Computations 7B Apache 2.0 83.75% 77.80 % 76.60% 75.01%\\nMistral-7B-OpenOrca Open-Orca 7B Apache 2.0 83.75% 80.20% 79.00% 76.71 %\\nGemma-1.1-7b-it Google 7B Open 82.50% 75.40% 75.75% 73.32%\\nFlan-T5-XXL Google 11B Apache 2.0 81.94% 71.10% 69.00% 67.50%\\nMeta-Llama-3-8B-Instruct Meta 8B Open 81.25 % 76.20% 73.05% 71.25%\\nZephyr-7B-beta HuggingFace 7B MIT 80.94% 76.40% 72.50% 65.00%\\nYi-1.5-6B-Chat 01-ai 6B Apache 2.0 80.00% 75.80% 75.70% 74.84%\\nMistral-7B-Instruct-v0.2 Mistral AI 7B Apache 2.0 78.75% 78.40% 76.40% 74.82%\\nLlama 2-70B Meta 70B Apache 2.0 75.00% 73.40% 71.60% 66.10%\\nQwen1.5-7B Qwen 7B Open 73.75% 60.60% 61.35% 59.79%\\nQwen1.5-14B Qwen 14B Open 71.25% 70.00% 72.00% 69.96%\\nMistral-7B-Instruct-v0.1 Mistral AI 7B Apache 2.0 70.00% 71.80% 68.25% 67.29%\\nLlama-3-8B-Instruct-Gradient-1048k Bartowski 8B Open 66.25% 58.00% 56.30% 55.09%\\nQwen1.5-MoE-A2.7B Qwen 2.7B Open 62.50% 64.60% 61.65% 60.73%\\nPhi-2 Microsoft 2.7B MIT 53.75% 48.00% 52.90% 52.13%\\nLlama3-ChatQA-1.5-8B Nvidia 8B Open 53.75% 52.80% 49.45 % 49.64%\\nDeciLM-7B Deci 7B Apache 2.0 52.50% 47.20% 50.44% 50.75%\\nFlan-T5-Base Google 0.25B Apache 2.0 51.25% 50.40% 48.55% 47.09%\\nDeepseek-moe-16b-chat Deepseek 16B MIT 47.50% 45.80% 49.55% 48.76%\\nMistral-7B-v0.1 Mistral AI 7B Apache 2.0 43.75% 39.40% 38.15% 39.28%\\nQwen-7B Qwen 7B Open 43.75% 58.00% 55.75% 54.09%\\nGemma-7b Google 7B Open 42.50% 37.20% 36.00% 34.28%\\nMeta-Llama-3-8B Meta 8B Open 38.75% 35.80% 37.00% 36.00%\\nGenstruct-7B NousResearch 7B Apache 2.0 38.75% 40.60% 37.55% 36.93%\\nQwen1.5-4B Qwen 4B Open 36.25% 41.20% 40.50% 40.29%\\nLlama-2-13b-hf Meta 13B Open 33.75% 37.00% 36.40% 34.49%\\nDolly V2 12b BF16 Databricks 12B MIT 33.75% 30.00% 28.75% 27.00%\\nDeepseek-llm-7b-base DeepSeek 7B MIT 33.75% 25.20% 27.00% 26.48%\\nCerebras-GPT-2.7B Cerebras 7B Apache 2.0 25.00% 20.20% 19.75% 19.27%\\nGemma-2b Google 2B Open 25.00% 23.20% 18.20% 19.18%\\nStablelm-2-1 6b Stability AI 6B Open 16.25% 21.80% 19.55% 20.09%\\nZySec-7B ZySec-AI 7B Apache 2.0 12.50% 16.40% 15.55% 14.04%\\nPhi-3-mini-4k-instruct Microsoft 3.8B MIT 5.00% 5.00% 4.41% 4.80%\\nPhi-3-mini-128k-instruct Microsoft 3.8B MIT 1.25% 0.20% 0.70% 0.88%\\n19', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a12f8a3c-7ce8-41e8-91f8-6324300709e2', embedding=None, metadata={'page_label': '20', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='encoder-decoder-based model that operates within the unified\\ntext-to-text framework. Multiple variants of T5 with different\\nsizes - ranging between 220M to 11B parameters- were devel-\\noped to broaden the experimental scope and were trained on\\nmassive amounts of data from various sources, including C4,\\nWeb Text, and Wikipedia. Building on the foundation of these\\ndiverse model sizes and rich data sources, multiple approaches\\nand different settings for pre-training and fine-tuning were\\nexamined and discussed, achieving performance that nearly\\nmatched human levels on one of the benchmarks. Considering\\nthat, the model’s potential in cybersecurity applications is\\nparticularly promising. For instance, T5 can be utilized for\\nthreat intelligence by extracting critical information from vast\\nsecurity documents and then summarizing and organizing that\\ninformation.\\n4) BERT: Bidirectional Encoder Representations from\\nTransformers, commonly known as BERT, was presented by\\n[99] to enhance fine-tuning-based approaches in NLP. It is\\navailable in two versions: BERT-Base, with 110M parameters,\\nand BERT-Large, with 340M parameters, trained on 126GB of\\ndata from BooksCorpus and English Wikipedia. During its pre-\\ntraining phase, BERT employed two key techniques: Masked\\nLanguage Modeling (MLM) and Next Sentence Prediction\\n(NSP). Building on these approaches, fine-tuning, and feature-\\nbased methods have led to competitive performance from\\nBERT-Large in particular. Since encoder-only models like\\nBERT are known for their robust contextual understanding,\\napplying such models to tasks like malware detection and\\nsoftware vulnerability can be highly effective in cybersecurity.\\n5) ALBERT: Aiming to address the limitations related to\\nGPU/TPU memory and training time in Large Language\\nModels (LLMs), Google researchers developed A Lite BERT\\n(ALBERT), a modified version of BERT with significantly\\nfewer parameters [100]. And like other LLMs, ALBERT was\\nintroduced in various sizes, with options ranging from 12M\\nto 235M parameters, all trained on data from BooksCorpus\\nand English Wikipedia. Various methods and techniques were\\ndeployed during the pre-training stage, including Factorized\\nEmbedding Parameterization, Cross-layer Parameter Sharing,\\nInter-sentence Coherence Loss, and Sentence Order Prediction\\n(SOP). As a result, one of the models (i.e., ALBERT-xxlarge)\\noutperformed BERT-Large despite having fewer parameters.\\nThus, utilizing ALBERT in cybersecurity applications, such\\nas phishing detection and malware classification, could signif-\\nicantly contribute to advancing cybersecurity infrastructure.\\n6) RoBERTa: RoBERTa, proposed by Meta, is an opti-\\nmized replication of BERT that demonstrates how the choice\\nof hyperparameters can significantly impact the model’s per-\\nformance [101]. RoBERTa has only one version with 355M\\nparameters but is trained and tested in various data sizes\\nand training steps. Similar to BERT, the training data was\\ntaken from the Books corpus and English Wikipedia. However,\\nthe key optimizations in this model were in the training\\ntechniques, which included multiple methods such as Dynamic\\nMasking, training on Full Sentences without NSP loss, using\\nLarge Mini-Batches, and employing a Larger Byte-Level BPE.\\nConsequently, RoBERTa achieved state-of-the-art results in\\nsome of the benchmarks. With proper fine-tuning, RoBERTa’sability to understand, interpret, and generate human-like text is\\nleveraged to automate and enhance various tasks in the realm\\nof cybersecurity.\\n7) XLNet: The advances and limitations of Masked Lan-\\nguage Modeling (MLM) in bidirectional encoders and Au-\\ntoregressive Language Modeling have inspired researchers at\\nCMU and Google AI to develop XLNet [102]. Based on\\nthe Transformer-XL model, XLNet combines aspects of both\\napproaches, enabling the learning of bidirectional contexts\\nwhile addressing common MLM issues, such as neglecting\\ndependencies between masked positions and the discrepancy\\nbetween pretraining and finetuning phases. With 340M pa-\\nrameters, XLNet was pre-trained using data from English\\nWikipedia and utilizing techniques like Permutation Language\\nModeling (PLM), Two-stream attention, Segment Recurrence,\\nand Relative Encoding. Due to the careful design of the model\\nand strategic pre-training techniques, XLNet has achieved\\nsubstantial performance over other popular models like BERT,\\nmaking it -after appropriate fine-tuning- a capable tool for\\nenhancing various aspects of the cybersecurity field.\\n8) ProphetNet: ProphetNet LLM, proposed by Microsoft,\\nis a sequence-to-sequence pre-trained model that aims to\\naddress the issue of overfitting on strong local correlations by\\nleveraging two novel techniques, namely: future n-gram pre-\\ndiction and n-stream self-attention [103]. Built on an encoder-\\ndecoder architecture and trained on 16GB base-scale and\\n160GB large-scale datasets sourced from web data and books,\\nProphetNet, with its 550M parameters, achieved new state-of-\\nthe-art results on multiple benchmarks. The model was also\\nfine-tuned for two downstream tasks, Question Generation and\\nText Summarization, where it achieved the best performance.\\nTherefore, utilizing ProphetNet in cybersecurity tasks such as\\nautomated security incident summarization could significantly\\nenhance efficiency and decision-making.\\n9) Falcon: Falcon LLM, built on decoder-only architec-\\nture, was introduced by the Technology Innovation Institute\\n(TII) as a proof-of-concept that enhancing data quality can\\nsignificantly improve the LLM performance even with purely\\nweb-sourced data [104]. This insight is increasingly relevant as\\nscaling in LLMs, which is becoming more prevalent, requires\\nmore data for processing. The model has three versions (i.e.,\\n7B, 40B, 180B) pre-trained on the “RefinedWeb” dataset\\nproposed by TII. RefinedWeb, sourced exclusively from web\\ndata, was subjected to various filtering and deduplication\\ntechniques to ensure high quality. Autoregressive training,\\nFlash Attention, and ALiBi Positional encoding were the\\nmethods used for pre-training. With further fine-tuning, Falcon\\ncan advance cybersecurity, particularly in threat intelligence\\nand analysis.\\n10) Reformer: Striving to address common memory limi-\\ntations in LLMs, Google proposed the Reformer, an encoder-\\ndecoder memory-efficient LLM [105]. With up to 6B param-\\neters, Reformer was pre-trained on web data using techniques\\nincluding Locality-Sensitive Hashing (LSH) Attention, Chun-\\nked Processing, Shared-QK Attention Heads, and Reversible\\nlayers. These techniques were proven to have a negligible\\nimpact on the training process compared to the standard\\nTransformer, as the Reformer achieved results that matched\\n20', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1ac8b3ef-e046-4ba4-a803-68ae58be2917', embedding=None, metadata={'page_label': '21', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the full Transformer but with much faster processing and\\nbetter memory efficiency. Subsequently, employing Reformer\\nfor tasks like large-scale data analysis could serve the cyberse-\\ncurity field by enabling more efficient processing and analysis\\nof extensive datasets.\\n11) PaLM: Driven by the advancement in machine learning\\nand natural language processing, Google has developed PaLM\\nto examine the impact of scale on few-shot learning [106].\\nPaLM, built on decoder-only architecture, was trained with\\n540B parameters using Pathways, a new system that utilizes\\nhighly efficient training across multiple TPU pods. The model\\nwas trained on 2TB of data from multiple sources, including\\nnews articles, Wikipedia, source code, etc. SwiGLU Activa-\\ntion, Parallel Layers, and other techniques were deployed for\\npre-training three different parameter scales, 8B, 62B, and\\n540B, to understand the scaling behavior better. An observed\\ndiscontinuous improvement indicated that as LLMs reach a\\ncertain level of scale, they exhibit new abilities. Furthermore,\\nthese emerging capabilities continue to evolve and become\\napparent even beyond the scales that have been previously\\nexplored and documented. Subsequently, PaLM achieved a\\nbreakthrough by outperforming the finetuned state-of-the-art\\nand average human on some benchmarks, proving that when\\nscaling is combined with chain-of-thought prompting, basic\\nfew-shot evaluation has the potential to equal or surpass the\\nperformance of fine-tuned state-of-the-art models across a\\nbroad spectrum of reasoning tasks. With such strong capabili-\\nties, utilizing PaLM for tasks like generating security policies\\nand incident response automation can enhance the efficiency\\nand effectiveness of cybersecurity operations.\\n12) PaLM2: PaLM2 is an advanced variant of the PaLM\\nmodel that is more compute-efficient, although it offers better\\nmultilingual and reasoning capabilities [107]. The key en-\\nhancements in the model are the improved dataset mixtures,\\nthe compute-optimal scaling, and architectural and objective\\nimprovements. The significant evaluation results of PaLM2\\nindicate that various approaches could elaborate on the model’s\\nenhancement besides scaling, such as meticulous data selection\\nand efficient architecture/objectives. Moreover, the fact that\\nPaLM2 outperformed the predecessor PaLM despite its signif-\\nicantly smaller size shows that the model quality has a greater\\ninfluence on the performance than the model size as it could\\nenable more efficient inference, reducing serving costs and\\npotentially allowing for broader applications and accessibility\\nto more users.\\n13) LLaMA: Proposed by Meta, the LLaMA decoder-only\\nmodel is a proof-of-concept that it’s possible to achieve state-\\nof-the-art performance by training exclusively on publicly\\navailable data [108]. LLaMA, with multiple variants ranging\\nbetween 7 and 65 billion parameters, was trained on 1400B to-\\nkens of publicly available datasets, including CommonCrawl,\\nC4, arXiv, and others. Interestingly, the techniques used for\\ntraining the model were inspired by multiple popular models\\nlike GPT-3 (Pre-normalization), PaLM (SwiGLU activation\\nfunction), and GPTNeo (Rotary Embedding). As a result of\\nthis incorporation, LLaMA-13B was able to outperform GPT-\\n3(175B) on most benchmarks despite it being more than ten\\ntimes smaller, while LLaMA-65B has shown to be competitivewith Chinchilla-70B and PaLM-540B. Given its relatively\\nsmall size and superior performance, fine-tuning LLaMA on\\ncyber threat intelligence tasks could significantly enhance the\\nsecurity of edge devices.\\n14) LLaMA2: LLaMA2 is an optimized version of LLaMA\\ndeveloped by Meta and a collection of pre-trained and fine-\\ntuned LLMs with sizes ranging from 7 to 70B parameters\\n[109]. In the pre-training, a mixture of publicly available data\\nwas used for up to 2000B training tokens. Moreover, multiple\\ntechniques were used in the predecessor LLaMA, such as Pre-\\nnormalization, SwiGLU activation function, and Rotary posi-\\ntional embeddings. Two additional methods, namely increased\\ncontext length and group-query attention (GQA), were also\\nused. After pre-training, variants of the model (i.e., LLaMA2-\\nChat) were optimized for dialog use cases by supervised\\nfine-tuning and reinforcement learning with human feedback\\n(RLHF). The model evaluation, which focused on helpfulness\\nand safety, showed superiority over the other open-source\\nmodels and competitive performance to some closed-source\\nmodels.\\n15) GShard: GShard LLM was introduced by Google in\\n2020, aiming to address neural network scaling issues related\\nto computation cost and training efficiency [110]. Based on\\na Mixture-of-Experts (MoE) transformer with 600B parame-\\nters, GShard was pre-trained on 1000B tokens of web data.\\nMultiple techniques were deployed for the training stage,\\nsuch as conditional computation, XLA SPMD partitioning,\\nposition-wise MoE, and parallel execution using annotation\\nAPIs. Subsequently, GShard outperformed prior models in\\ntranslation tasks and exhibited a favorable trade-off between\\nscale and computational cost, resulting in a practical and\\nsample-efficient model. These results highlight the importance\\nof considering training efficiency when scaling LLMs, which\\nmakes it more viable in the real world.\\n16) ELECTRA: The extensive computation cost of MLM\\npre-training methods has inspired Google to propose ELEC-\\nTRA LLM, which is a 335M parameters’ encoder-only trans-\\nformer model that utilizes a novel pre-training approach called\\n“replaced token detection” [111]. This technique allows the\\nmodel to learn from the entire sequence rather than just a\\nsmall portion of masked tokens. Given that the quality and\\ndiversity of ELECTRA training data play a pivotal role in\\nits ability to generalize across tasks, the model was trained\\non a vast Books Corpus and English Wikipedia. Pre-training\\ntechniques were utilized, including replaced token detection,\\ngenerator-discriminator framework, token replacement, and\\nweight-sharing. As a result, ELECTRA was able to perform\\ncomparably to popular models like RoBERTa and XLNet\\nwhen using less than 25% of their compute and outperform\\nthem when using equivalent compute. Deploying such a robust\\nmodel in the security field after fine-tuning can provide an\\nefficient solution for detecting and mitigating sophisticated\\ncyber threats, thanks to its nuanced understanding of context\\nand language patterns.\\n17) MPT-30B: MPT-30B LLM is a decoder-only trans-\\nformer introduced by MosaicML after the notable success\\nof MPT-7B [112]. The model has multiple variants, the\\nbase model and two fine-tuned variants, namely MPT-30B-\\n21', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc52d6b8-5412-4bd5-b929-77c0b5cc4796', embedding=None, metadata={'page_label': '22', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Instruct and MPT-30B-Chat. Training the model on a variety\\nof datasets such as C4, CommonCrawl, and arXiv, among\\nothers, besides the strategic selection of pre-training meth-\\nods like FlashAttention and ALiBi positional encoding, have\\ncontributed to a robust performance, surpassing even the\\noriginal GPT-3 benchmarks. MPT-30B has also significantly\\nperformed in programming tasks, outperforming some open-\\nsource models designed specifically for code generation. With\\nthese capabilities, deploying MPT-30B in cybersecurity could\\nsubstantially enhance threat detection and response systems.\\nIts adeptness at understanding and generating programming\\nlanguages promises advancements in automated vulnerability\\nassessment and developing sophisticated security protocols.\\n18) Yi-34B: The newly released LLM Yi-34B developed by\\n01.AI is getting attention as one of the best open-source LLMs\\n[113]. Given the recent release of the model, its technical paper\\nhas not yet been published; hence, the available information\\nis limited. The model has multiple variants: base and chat\\nmodels, some quantized. All variants are trained on a dataset\\ncontaining Chinese and English only, and the chat versions\\nhave gone through supervised fine-tuning, resulting in more\\nefficient models for downstream tasks. The base model out-\\nperformed many open LLMs in certain benchmarks, including\\nrenowned ones like LLaMA2-70B and Falcon-180B. Even the\\nquantized versions have demonstrated impressive performance,\\npaving the way for their deployment in cybersecurity applica-\\ntions, such as edge security solutions.\\n19) Falcon2-11B: Falcon2-11B LLM [114] built by TII, is\\na decoder-only model with 11 billion parameters, trained on\\nan immense corpus of text data totaling over 5,000 billion\\ntokens. In terms of performance, Falcon2-11B showcases\\nimpressive capabilities, supporting 11 languages: English,\\nGerman, Spanish, French, Italian, Portuguese, Polish, Dutch,\\nRomanian, Czech, and Swedish. While it excels in generating\\nhuman-like text, it also carries the biases and stereotypes\\nprevalent in its training data, a common challenge LLMs\\nface. To address this, TII recommends fine-tuning the model\\nfor specific tasks and implementing guardrails for production\\nuse. In the training process of Falcon2-11B, they utilized\\na four-stage strategy with increasing context lengths; in the\\nfinal stage, they reached 8162 context lengths. This stage\\nfocused on enhancing performance using high-quality data.\\nAdditionally, the training leveraged 1024 A100 40GB GPUs\\nand a custom distributed training codebase named Gigatron,\\nwhich employs a 3D parallelism approach combined with\\nZeRO, high-performance Triton kernels, and FlashAttention-2\\nfor efficient and effective training.\\nB. LLMs performance in the hardware cybersecurity\\nTable IV compares 19 publicly available LLMs’ perfor-\\nmance in Hardware design-related bug detection and security\\nissues identification using samples from various sources. A\\nportion of the Chrystalis dataset [87] has been used to evaluate\\nthe performance of the LLM models in bug detection tasks. A\\nset of faults has been injected intentionally into a functional\\ncode and labeled as faulty. The sample size that has been\\nprocessed comprises 10K of hardware design-related codesamples. The prompt that has been used instructs the model\\nto check the concerned code for any issue or bug and respond\\nonly with yes or no. The result is presented as the ratio of\\nthe responses where the model successfully identified a buggy\\ncode from the total samples used. The hardware CWE column\\nevaluates the capability of the models to link a buggy code\\nto its CWE number. The prompt has been designed to ask\\nfor a well-defined CWE number on the buggy design. This\\nevaluation process asses the capability of an LLM model in\\nbug detection and classification into the correct CWE class\\nnumber.\\nThe top performers in this evaluation in terms of design bug\\ndetection are LLama3 and Mixtral. While the LLama3 model\\nperforms better in the bug detection tasks, it lacks the proper\\nidentification of the CWE issue related to the faulty section.\\nMixtral models show less performance at identifying bugs but\\nhigher diversity in identifying a bug’s security impact on the\\noverall design implementation. The outcomes of this experi-\\nment reveal that some models cannot identify the right issues\\nwith the source code, which might require further refinement\\nof the used prompt and/or fine-tuning the general-purpose\\nmodels on bug-locating tasks. The results also show that the\\nmodel size doesn’t greatly impact the model performance at\\nlocating the bugs nor reasoning about their according impact\\n(CWE class identification). While the samples that have been\\npicked do not exceed the context length of the selected models,\\nthe token size of the model itself might reveal a superiority\\nfor the larger models when dealing with large source codes.\\nHowever, superior bug identification and reasoning are also\\nrequired to provide the required performance.\\nIn conclusion, the highlighted results reveal that the existing\\nmodels might be subject to weaknesses in identifying bugs in\\nHardware designs that might lead to security-related issues.\\nThe two-step evaluation process gives better visibility in\\nbuilding more robust dedicated LLMs for Hardware design\\nsecurity evaluation. Models that properly locate bugs do not\\nshow similar performance in classifying the bug’s impact on\\nthe overall design. The outcomes could be evaluated with a\\nlarger sample size and a more dedicated study at a large scale\\nto get conclusive results.\\nC. LLMs performance in the cybersecurity knowledge\\nTable V compares various 42 LLMs performance in the\\ncybersecurity domain using CyberMetric dataset [89]. Figure\\n5 presents the LLMs performance steps. The models are\\nevaluated based on their accuracy across four question sets:\\n80 questions, 500 questions, 2000 questions, and 10,000 ques-\\ntions. The performance is represented in percentage accuracy,\\noffering a comprehensive view of each model’s proficiency in\\nhandling cybersecurity-related queries.\\nThe top performers in this evaluation are the GPT-4 and\\nGPT-4-turbo models by OpenAI. These models demonstrate\\nexceptional performance, with GPT-4 achieving 96.25% ac-\\ncuracy on the 80-question set and maintaining high accuracy\\nwith 88.89% on the 10,000-question set. GPT-4-turbo closely\\nfollows with similar accuracy percentages. Both models are\\nproprietary and developed by OpenAI, indicating a high\\n22', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e081afe6-cec5-46b9-80fa-eeb2a3e9f6fb', embedding=None, metadata={'page_label': '23', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='optimization level for specialized tasks within a controlled\\nenvironment. Another strong performer is the Mixtral-8x7B-\\nInstruct by Mistral AI, which boasts accuracy of 92.50% on the\\n80-question set and 87.00% on the 10,000-question set. This\\nmodel is open-source under the Apache 2.0 license, demon-\\nstrating the potential of community-driven development in\\nachieving high performance. Additionally, GEMINI-pro 1.0 by\\nGoogle shows robust performance, achieving 90.00% accuracy\\non the 80-question set and 87.50% on the 10,000-question set,\\nhighlighting the capabilities of large-scale corporate research\\nand development in LLMs.\\nMid-tier performers include models like Yi-1.5-9B-Chat by\\n01-ai and Hermes-2-Pro-Llama-3-8B by NousResearch. Yi-\\n1.5-9B-Chat performs reasonably well with an 87.50% accu-\\nracy on the 80-question set, tapering to 76.04% on the 10,000-\\nquestion set. Under the Apache 2.0 license, this model shows a\\nbalance between open-source collaboration and performance.\\nHermes-2-Pro-Llama-3-8B achieves 86.25% accuracy on the\\n80-question set and 77.33% on the 10,000-question set, further\\nunderscoring the effectiveness of collaborative research efforts.\\nLower-tier performers include models like Qwen1.5-7B by\\nQwen. Qwen1.5-7B scores 73.75% on the 80-question set,\\ndropping to 59.79% on the 10,000-question set. As an open\\nmodel, Qwen1.5-7B indicates the challenges faced by smaller\\nmodels in maintaining high accuracy with increasing question\\nset sizes. Falcon-40B-Instruct achieves 67.50% accuracy on\\nthe 80-question set and 64.50% on the 10,000-question set.\\nLicensed under Apache 2.0, it highlights the competitive\\nlandscape of open-source LLMs.\\nThe lowest-tier performers include models such as Phi-\\n3-mini-128k-instruct by Microsoft and Stablelm-2-1 6b by\\nStability AI. Phi-3-mini-128k-instruct has the lowest perfor-\\nmance, with only 1.25% accuracy on the 80-question set and\\n0.88% on the 10,000-question set. Despite being from a major\\ncompany like Microsoft and licensed under MIT, this model\\nunderscores the importance of continuous development and\\noptimization in LLMs. Stablelm-2-1 6b scores 16.25% on the\\n80-question set, decreasing to 20.09% on the 10,000-question\\nset, demonstrating smaller models’ difficulties in scaling up\\neffectively.\\nIn conclusion, the table reveals that proprietary models\\nperform better than open-source models, suggesting that con-\\ntrolled environments and dedicated resources may significantly\\nenhance model performance. However, larger models do not\\nalways guarantee higher performance, as seen with some mid\\nand lower-tier performers. Additionally, many models show\\na decline in accuracy as the number of questions increases,\\nhighlighting the challenges in maintaining performance con-\\nsistency across larger datasets. The analysis indicates that\\nwhile top-tier proprietary models lead in performance, there\\nis significant potential within the open-source community\\nto develop competitive models. Continuous improvements in\\nmodel architecture, training data quality, and optimization\\ntechniques are crucial for advancing state-of-the-art cyberse-\\ncurity knowledge within LLMs.VI. C ODE-SPECIFIC LLM S\\nThe rapid evolution of technology and software develop-\\nment has increased the demand for specialized tools that\\naid in coding, debugging, and enhancing software security\\n[115], [116]. Recognizing this need, various organizations\\nhave developed Code-specific LLMs, each offering unique\\nfeatures and capabilities. These models leverage advanced\\nmachine learning techniques to understand, generate, and\\nmanipulate code, thereby revolutionizing the field of software\\ndevelopment [117], [118]. This section delves into several\\nnotable Code-specific LLMs, exploring their architectures,\\ntraining methods, and potential applications in cybersecurity\\nand beyond [119]–[122]. Table IX and Table X compare Code-\\nspecific Large Language Models.\\nA. Prevalent LLMs\\n1) SantaCoder: As part of the BigCode project, Hug-\\ngingFace and ServiceNow have proposed SantaCoder LLM\\n[123]. Based on the decoder-only architecture and with a\\n1.1B parameter, SantaCoder was trained on 268GB of Python,\\nJava, and JavaScript subsets of The Stack dataset. Multiple\\nfiltering techniques were used for the training data with-\\nout much impact except for one (i.e., filtering files from\\nrepositories with 5+ GitHub stars), significantly deteriorat-\\ning the performance on text2code benchmarks. Pre-training\\nmethods included Multi-Query-Attention (MQA) and Fill-in-\\nthe-Middle (FIM). Although these techniques have led to a\\nslight drop in the model’s performance compared to Multi-\\nHead-Attention (MHA) and training without FIM, the model\\ncould still outperform previous multi-lingual code models\\nlike CodeGen0Multi-2.7B and InCoder-6.7B despite being\\nsubstantially smaller. Such performance can be promising if\\ndeployed in cybersecurity for tasks like software vulnerability\\nand secure code generation.\\n2) StarCoder: StarCoder is another decoder-only model\\ndeveloped within the BigCode project [124]. With 15.5B\\nparameters, StarCoder was pre-trained on 1000B tokens from\\nover 80 different programming languages. The pre-training\\nutilized techniques such as FIM and MQA and Learned\\nAbsolute Positional Embeddings. After pre-training, the base\\nmodel was fine-tuned on an additional 35B tokens of Python.\\nCompared to other Code LLMs, StarCoder outperformed all\\nfine-tuning models on Python. Moreover, the base model\\noutperformed OpenAI code-cushman-001. StarCoder’s excep-\\ntional performance in Python and its broad training in multiple\\nprogramming languages position it as a highly versatile tool\\nfor various coding tasks.\\n3) StarChat-Alpha: StarChat Alpha is a variant of Star-\\nCoder fine-tuned to act as a helpful coding assistant that ac-\\ncepts natural language prompting (considering that StarCoder\\nneeds specific structured prompting) [125]. With 16B param-\\neters, the model was fine-tuned on a mixture of oasst1 and\\ndatabricks-dolly-15k datasets. The model has not undergone\\nRLHF or similar methods, which would have helped align it\\nwith human preferences. Nevertheless, the comprehensive pre-\\ntraining of the base model contributed to the model’s ability to\\n23', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc24d475-d2a9-494f-bf7b-49663fd046cc', embedding=None, metadata={'page_label': '24', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='interpret various coding tasks and provide accurate code sug-\\ngestions. This capability makes it an invaluable programming\\ntool, simplifying code development and problem-solving.\\n4) CodeGen-2: Developed by Salesforce AI research,\\nCodeGen2 was proposed as a product of extensive research\\nin the field of LLM aimed at optimizing model architectures\\nand learning algorithms to enhance the efficiency and reduce\\nthe costs associated with LLMs [126]. The final findings were\\nexamined in multiple variants with parameters ranging from\\n1B to 16B, where the 16B model is trained on 400B tokens\\nfrom the Stack dataset. Causal language modeling, cross-\\nentropy loss, and other techniques were used for pre-training,\\nresulting in a robust program synthesis model. CodeGen2’s\\nproficiency in program synthesis makes it a valuable asset\\nin cybersecurity applications, such as aiding in vulnerability\\ndetection and enhancing code security analysis. Its ability to\\nunderstand and generate complex small models can be trained\\nfor multiple epochs with specific settings, efficient security\\nprotocols, and automated threat detection systems.\\n5) CodeGen-2.5: Another version of the CodeGen family is\\nCodeGen 2.5 [127]. The 7B parameters model was introduced\\nto prove that good models don’t necessarily have to be big,\\nespecially with the trend of scaling up LLMs and the data size\\nlimitations. CodeGen 2.5 was trained on 1400B training to-\\nkens from StarCoderData. A strategic selection of pre-training\\ntechniques, such as Flash Attention, Infill Sampling, and Span\\nCorruption, enhanced the model’s performance. Moreover, that\\nled to a good performance that is on par with popular LLMs\\nof larger size. The results indicated that small models can be\\ntrained for multiple epochs with specific settings and achieve\\ncomparable results to bigger models.\\n6) CodeT5+: CodeT5+ is an encoder-decoder transformer\\nproposed by Salesforce AI Research to address some code\\nLLMs limitations [128]. Specifically, those related to the ar-\\nchitecture being either inflexible or serving as a single system\\nand pre-training task limitations related to a limited set of pre-\\ntraining objectives can result in a substantial degradation in\\nperformance. The proposed model has different variants rang-\\ning from 220M to 16B parameters. Trained on 51.5B tokens\\nfrom CodeSearchNet and GitHub code datasets using tech-\\nniques like span denoising, contrastive learning, and others, the\\nmodel achieved new state-of-the-art results on various code-\\nrelated tasks like code generation, code completion, etc. A\\nmodel with such capabilities can be valuable to cybersecurity\\nfor threat intelligence and software vulnerability.\\n7) XGen-7B: Another production of Salesforce AI Re-\\nsearch is XGen-7B LLM, a decoder-only transformer with\\n7B parameters [129]. The model was developed to address\\nthe problem of sequence length constraints in the available\\nopen-source LLMs as many tasks require inference over an\\ninput context. XGen-7B, with up to 8K sequence length, was\\ntrained on 1500B tokens from a mixture of text and code\\ndata. Techniques like standard dense attention and a two-stage\\ntraining strategy were utilized for pre-training. Additionally,\\nthe model was enhanced with instructional tuning, a technique\\nthat refines its responses to align closely with specific user\\ninstructions. As a result, XGen-7B achieved comparable or\\nbetter results than other 7B state-of-the-art open-source LLMs.8) Replit code v1: Proposed by Replit, Inc., the 2.7B\\nparameters causal language model Replit-code-v1-3b, with\\na focus on code completion, was trained on 525B tokens\\nfrom a subset of the stack Dedup v1.2 dataset [130]. The\\nmodel underwent advanced pre-training techniques such as\\nFlash Attention for efficient computation, AliBi positional\\nembeddings for enhanced context interpretation, and the Li-\\nonW optimizer for improved training dynamics. The Replit\\ncode v1 model is also available in two quantization options:\\n8-bit and 4-bit. The Replit-code-v1-3b model’s capabilities\\nin understanding and generating code make it particularly\\nsuited for cybersecurity applications, such as automating the\\ndetection of code vulnerabilities and generating secure coding\\npatterns. Additionally, its quantized versions can be utilized\\nfor edge security.\\n9) DeciCoder-1B: DeciCoder-1B is an open-source 1B\\nparameter decoder-only transformer developed by Deci AI\\nwith a 2048-context window [131]. Subsets of Python, Java,\\nand JavaScript from the StarCoderData dataset were used for\\ntraining. The model architecture was built using Automated\\nNeural Architecture Construction (AutoNAC) developed by\\nthe company, which is a technology designed to automatically\\ncreate and optimize deep learning models, particularly neural\\nnetworks, for specific tasks and hardware environments. More-\\nover, Grouped Query Attention (GQA) and FIM were utilized\\nto pre-train the model. Consequently, the model has shown\\nsmaller memory usage compared to popular code LLMs like\\nStarCoder and outperformed SantaCoder in the languages it\\nwas trained on with remarkable inference speed.\\n10) CodeLLAMA: Based on LLAMA 2, CodeLLAMA was\\nintroduced by Meta as a decoder-only transformer code LLM\\n[132]. With variants ranging from 7 to 34B parameters of\\nbase, python specialized, and instruction-following models, all\\ntrained on text and code from multiple datasets, CodeLLAMA\\nemerges as a comprehensive suite of models, adept at handling\\na wide array of programming-related tasks. Causal infilling,\\nLong-context fine-tuning, and other techniques were utilized\\nfor pre-training and fine-tuning. CodeLLAMA models’ family\\nachieved state-of-the-art performance in multiple benchmarks,\\nindicating their potential for transformative applications in\\ncybersecurity. Their advanced code analysis and generation\\ncapabilities could be crucial in automating threat detection and\\nenhancing vulnerability assessments.\\n11) CodeQwen1.5-7B: CodeQwen1.5-7B-Chat [133] is a\\ntransformer-based decoder-only language model trained on 3\\ntrillion tokens of code data. It supports 92 coding languages\\nand has strong code-generation capabilities. The model can\\nunderstand and generate long contexts of up to 64,000 tokens\\nand has shown excellent performance in text-to-SQL and bug-\\nfixing tasks. It is based on Qwen1.5, which offers eight model\\nsizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B, and 72B dense\\nmodels, and an MoE model of 14B with 2.7B activated.\\n12) DeepSeek Coder-33B-instruct: Deepseek Coder [134]\\nis a series of code language models, with each model trained\\nfrom scratch on 2 trillion tokens, 87% of which are code and\\n13% natural language in English and Chinese. The model\\ncomes in various sizes, ranging from 1B to 33B, with the\\n33B model being fine-tuned on 2 billion tokens of instruction\\n24', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e7b9da4-c898-4f39-97e8-f37445de9e4d', embedding=None, metadata={'page_label': '25', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='data. It achieves state-of-the-art performance among open-\\nsource code models on multiple programming languages and\\nbenchmarks.\\n13) CodeGemma-7B: CodeGemma [135] is a collection of\\nlightweight open code models built on top of Gemma. It is a\\ntext-to-text and text-to-code decoder-only model with 7 billion\\nparameters, specializing in code completion and generation\\ntasks. It can answer questions about code fragments, gener-\\nate code from natural language, or discuss programming or\\ntechnical problems. CodeGemma was trained on 500 billion\\ntokens of primarily English language data from publicly avail-\\nable code repositories, open-source mathematics datasets and\\nsynthetically generated code.\\n14) Granite 8B Code: IBM released a family of Granite\\ncode models [136], including the Granite-8B-Code-Base, to\\nmake coding more accessible and efficient for developers.\\nGranite-8B-Code-Base is a decoder-only code model designed\\nfor code generation, explanation, and fixing. It is trained in\\ntwo phases: first on 4 trillion tokens from 116 programming\\nlanguages, then on 500 billion tokens from a carefully de-\\nsigned mixture of high-quality code and natural language data.\\nThis two-phase training strategy ensures the model can reason\\nand follow instructions while understanding programming\\nlanguages and syntax.\\n15) DeepSeek-V2: DeepSeek-V2 [137] is a mixture-of-\\nexperts (MoE) language model with 236 billion parameters,\\nof which 21 billion are activated for each token. It is a sig-\\nnificant upgrade from the previous DeepSeek model, offering\\nstronger performance while reducing training costs by 42.5%.\\nThe model was pre-trained on a vast and diverse corpus of\\n8.1 trillion tokens, followed by supervised fine-tuning and\\nreinforcement learning to maximise its capabilities. DeepSeek-\\nV2 excels at live coding tasks and open-ended generation,\\nsupporting both English and Chinese.\\nB. Datasets Development for Code-centric LLM Models\\nThe development of large-scale datasets has played a crucial\\nrole in advancing LLM models, especially those focused on\\nunderstanding and generating code. Table XI presents the\\ndatasets used for pre-training foundation models in Coding.\\nDatasets like CodeSearchNet [145] and The Pile [146] have\\nbeen instrumental in bridging the gap between natural lan-\\nguage and code, improving semantic search capabilities, and\\nenhancing language model training across diverse domains.\\nThese datasets provide a rich source of real-world code in\\nmultiple programming languages and include expert annota-\\ntions and natural language queries that challenge and push the\\nboundaries of LLM performance in code-related tasks.\\nOver time, the focus has shifted towards increasing the\\nsize, diversity, and ethical considerations of the data used in\\ntraining AI models. Introducing datasets such as ROOTS and\\nThe Stack v2 [149] reflects a growing emphasis on responsible\\nLLM development. These newer datasets encompass a broader\\nrange of programming languages and coding scenarios, and\\nthey incorporate governance frameworks to ensure the ethical\\nuse of the data. In addition, these datasets are designed to\\naddress the needs of large multilingual language models andthe specific challenges of code generation and comprehension,\\ndemonstrating the evolving landscape of LLM research driven\\nby enhanced dataset quality and scope.\\nC. Vulnerabilities Analysis of LLM-Generated Code\\nThe evolution of LLMs in software development has\\nbrought significant advancements and new security challenges\\n[158]. Table XII presents a comparative analysis of vulnera-\\nbilities in LLM-generated code.\\nSchuster et al. [150] demonstrate how LLMs employed\\nin code autocompletion are susceptible to poisoning attacks,\\nwhich can manipulate the model’s output to suggest insecure\\ncode. This vulnerability is intensified by the ability to target\\nspecific developers or repositories, making the attacks more\\neffective and difficult to detect. Despite defenses against such\\nattacks, their effectiveness remains limited, raising concerns\\nover the secure deployment of these technologies [150].\\nRecent studies, such as those by Asare et al. [151] and\\nSandoval et al. [152], provide an empirical and comparative\\nanalysis of the security aspects of code generated by LLMs\\nlike GitHub’s Copilot and OpenAI Codex. Asare et al. [151]\\nfind that while Copilot occasionally replicates vulnerabilities\\nknown from human-written code, it does not consistently do\\nso across different vulnerabilities. In contrast, Sandoval et\\nal. [152] report a minimal increase in security risks when\\ndevelopers use LLMs in coding, indicating that LLMs do not\\nnecessarily degrade the security of the code more than human\\ndevelopers would.\\nMoreover, Perry et al. [153] reveal a concerning trend where\\nusers interacting with AI code assistants tend to write less\\nsecure code but believe otherwise. Their findings underscore\\nthe need for heightened awareness and better design of user\\ninterfaces to foster critical engagement with the code sugges-\\ntions provided by LLMs [153]. In a similar vein, Hamer et\\nal. [154] emphasize the educational gap among developers\\nregarding the security implications of using code snippets from\\nAI like ChatGPT or traditional sources like StackOverflow,\\nhighlighting that both sources can propagate insecure code.\\nLastly, novel tools like DeV AIC introduced by Cotroneo\\net al. [155] and comprehensive vulnerability evaluations in\\nLLM-generated web application code by T ´oth et al. [156] and\\nTihanyi et al. [157] illustrate ongoing efforts to understand\\nbetter and mitigate the risks associated with AI-generated\\ncode. DeV AIC, for instance, offers a promising approach to\\ndetecting vulnerabilities in incomplete Python code snippets,\\npotentially enhancing the security assessment capabilities for\\nAI-generated code.\\nVII. C YBER SECURITY DATASETS FOR LLM S\\nA. Cyber Security Dataset Lifecycle\\nCreating a cybersecurity dataset for use with LLMs in-\\nvolves several steps that ensure the dataset is comprehensive,\\naccurate, and effective for training or evaluating the models.\\nFigure 6 presents the cyber security dataset lifecycle for LLM\\ndevelopment.\\n25', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fff078c9-4a65-4461-b0f4-7c62494f9585', embedding=None, metadata={'page_label': '26', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE VI: Comparison of Large Language Models\\nModel Architecture Base Model Para-\\nmetersTraining\\nTokensPre-training Corpus\\nVolumeReleased\\nByApplications Use Cases in\\nCybersecurityTraining\\nSchemeKey Training\\nTechniquesQuanti-\\nzationRef\\nGPT-3 Decoder-only NA 175B 300B Books,\\nWeb text,\\nWikipedia,\\nCommon\\nCrawl+570GB Open AI Language\\nModeling, Text\\nCompletion,\\nQAMalware\\nDetection,\\nThreat\\nIntelligence,\\nSocial\\nEngineering\\nDetectionPre-training,\\nIn-context\\nlearningAutoregressive\\ntraining,\\nScaled Cross\\nEntropy Loss,\\nBackpropagation\\nand gradient\\ndescent, Mixed\\nprecision training.NA [96]\\nGPT-4 Decoder-only NA NA NA Web Data,\\nThird-party\\nlicensed dataNA Open AI Language\\nModeling, Text\\nCompletion,\\nQAMalware\\nDetection,\\nThreat\\nIntelligence,\\nSocial\\nEngineering\\nDetectionPre-training,\\nRLHFAutoregressive\\ntrainingNA [97]\\nT5 Encoder-\\ndecoderNA 11B 1000B C4, Web Text,\\nWikipedia750GB Google Language\\nModeling,\\nSumma-\\nrization,\\nTranslationMalware\\nDetection,\\nThreat\\nIntelligence,\\nSocial\\nEngineering\\nDetectionPre-training,\\nFine-tuningText-to-text frame-\\nwork, Denotation-\\nbased pretrainingNA [98]\\nBERT Encoder-only NA 340M 250B BooksCorpus,\\nEnglish\\nWikipedia126GB Google Language\\nModeling,\\nClassification,\\nQA, NERMalware Detec-\\ntion, Threat In-\\ntelligence, Intru-\\nsion Detection,\\nPhishing Detec-\\ntionPre-training Masked\\nLM(MLM),\\nNext-sentence\\nprediction(NSP)NA [99]\\nALBERT Encoder-only BERT 235M +250B\\n(calcu-\\nlated)BooksCorpus,\\nEnglish\\nWikipediaNA Google Language\\nModeling,\\nClassificationMalware Detec-\\ntion, Threat In-\\ntelligence, Intru-\\nsion Detection,\\nPhishing Detec-\\ntionPre-training Factorized\\nembedding\\nparameterization,\\nCross-layer\\nparameter sharing,\\nInter-sentence\\ncoherence loss,\\nSentence order\\nprediction (SOP)NA [100]\\nRoBERTa Encoder-only BERT 355M 2000B BooksCorpus,\\nEnglish\\nWikipediaNA Meta Language\\nModeling,\\nClassification,\\nQA, NERMalware Detec-\\ntion, Threat In-\\ntelligence, Intru-\\nsion Detection,\\nPhishing Detec-\\ntionPre-training Dynamic Masking,\\nFull-Sentences\\nwithout NSP\\nloss, Large mini-\\nbatches, Larger\\nbyte-level BPENA [101]\\nXLNet Encoder-only Transformer-\\nXL340M +2000B\\n(calcu-\\nlated)English\\nWikipedia158GB\\n(calcu-\\nlated)CMU,\\nGoogleLanguage\\nModeling,\\nClassification,\\nQAMalware Detec-\\ntion, Threat In-\\ntelligence, Intru-\\nsion Detection,\\nPhishing Detec-\\ntionPre-training Permutation\\nLM(PLM), Two-\\nstream self-\\nattention, Segment\\nRecurrence and\\nRelative EncodingNA [102]\\nProphetNet Encoder-\\ndecoderNA 550M +260B\\n(calcu-\\nlated)Web Data,\\nBooks160GB Microsoft\\nResearch\\nAsiaLanguage\\nModeling,\\nQuestion\\nGeneration,\\nSummarizationCybersecurity\\nReporting,\\nThreat\\nIntelligencePre-training,\\nFine-tuningMasked Sequence\\ngeneration,\\nAutoregressive\\ntraining, Denoising\\nAutoencoder\\nobjective, Shared\\nParameters\\nbetween encoder\\nand decoder,\\nMaximum\\nLikelihood\\nEstimation (MLE)NA [103]\\nFalcon Decoder-only NA 7-180B 5000B Web Data NA TII Language\\nModeling, Text\\nCompletion,\\nQAMalware\\nDetection,\\nThreat\\nIntelligence,\\nSocial\\nEngineering\\nDetectionPre-training Autoregressive\\ntraining,\\nFlashAttention,\\nALiBi Positional\\nencodingNA [104]\\nReformer Encoder-\\ndecoderNA Up to\\n6B+150B\\n(calcu-\\nlated)Web Data NA Google Language\\nModeling,\\nClassificationMalware Detec-\\ntion, Threat In-\\ntelligence, Intru-\\nsion Detection,\\nPhishing Detec-\\ntionPre-training Locality-Sensitive\\nHashing (LSH)\\nAttention,\\nChunked\\nProcessing,\\nShared-QK\\nAttention Heads,\\nReversible layersNA [105]\\n26', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4f70aa7-975e-4eff-8260-c84cb101cf00', embedding=None, metadata={'page_label': '27', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE VII: Continued\\nModel Architecture Base Model Para-\\nmetersTraining\\nTokensPre-training Corpus\\nVolumeReleased\\nByApplications Use Cases in\\nCybersecurityTraining\\nSchemeKey Training\\nTechniquesQuanti-\\nzationRef\\nPaLM Decoder-only NA 540B 780B Webpages,\\nbooks,\\nWikipedia,\\nnews articles,\\nsource code,\\nsocial media\\nconversations,\\nGitHub2TB Google Language\\nModeling, QA,\\nTranslationThreat\\nIntelligence,\\nSecurity\\nPolicies\\nGenerationPre-training SwiGLU\\nActivation, Parallel\\nLayers, Multi-\\nQuery attention\\n(MQA), RoPE\\nembeddings,\\nShared Input-\\nOutput embeddingNA [106]\\nPaLM2 Decoder-only NA NA NA web\\ndocuments,\\nbooks, code,\\nmathematics,\\nconversational\\ndataNA Google Language\\nModeling, QA,\\nSummarizationThreat\\nIntelligence,\\nSecurity\\nPolicies\\nGenerationPre-training Compute optimal\\nscaling, Canary\\ntoken sequences,\\nControl tokens for\\ninferenceNA [107]\\nLLaMA Decoder-only NA 7-65B 1400B CommonCrawl,\\nC4, GitHub,\\nWikipedia,\\nBooks, arXiv,\\nStackExchange177GB Meta Language\\nModeling, Text\\nCompletion,\\nQAThreat\\nIntelligence,\\nMalware\\nDetectionPre-training Pre-normalization,\\nSwiGLU activation\\nfunction, Rotary\\nEmbedding, Model\\nand sequence\\nparallelismNA [108]\\nLLaMA2 Decoder-only NA 7-70B 2000B Mix of puli-\\ncally available\\ndataNA Meta Language\\nModeling, Text\\nCompletion,\\nQAThreat\\nIntelligence,\\nMalware\\nDetectionPre-training,\\nFine-tuning,\\nRLHFOptimized\\nautoregressive\\ntraining, Grouped\\nQuery Attention\\n(GQA)NA [109]\\nGShard MoE NA 600B 1000B Web Data NA Google Language\\nModelingThreat\\nIntelligence,\\nIntrusion\\nDetection,\\nMalware\\nDetectionPre-training Conditional\\nComputation,\\nLightweight\\nAnnotation APIs,\\nXLA SPMD\\npartitioning,\\nPosition-wise MoENA [110]\\nELECTRA Encoder-only NA 335M +1800B\\n(calcu-\\nlated)BooksCorpus,\\nEnglish\\nWikipedia158GB Google Language\\nModeling,\\nClassificationThreat\\nIntelligence,\\nIntrusion\\nDetection,\\nMalware\\nDetection,\\nPhishing\\nDetectionPre-training,\\nFine-tuningReplaced token\\ndetection,\\nGenerator-\\ndiscriminator\\nframework, Token\\nreplacement,\\nWeight-sharingNA [111]\\nMPT-30B Decoder-only NA 30B 1000B C4, mC4,\\nCommon-\\nCrawl,\\nWikipedia,\\nBooks, arXivNA MosaicML Language\\nModeling, Text\\nCompletion,\\nQAThreat\\nIntelligence,\\nMalware\\nDetection,\\nSoftware\\nVulnerabilityPre-training FlashAttention,\\nALiBi positional\\nencodingNA [112]\\nYi-34B NA NA 34B 3000B Chinese and\\nEnglish datasetNA 01.AI Language\\nModeling,\\nQuestion\\nAnsweringThreat\\nIntelligence,\\nPhishing\\nDetection,\\nVulnerability\\nAssessmentPre-training,\\nFine-tuningNA GPTQ,\\nAWQ[113]\\nPhi-3-mini Decoder-only NA 3.8B 3.3T Phi-3 datasets\\n(Public\\ndocuments,\\nsynthetic, chat\\nformats)NA Microsoft Language\\nModeling, Text\\nCompletion,\\nQAThreat\\nIntelligence,\\nIntrusion\\nDetection,\\nMalware\\nDetectionPre-training,\\nFine-tuningLongRope, Query\\nAttention (GQA)NA [138]\\nMistral 7B Decoder-only NA 7.24B NA NA NA Mistral\\nAILanguage\\nModeling, Text\\nCompletion,\\nQAThreat\\nIntelligence,\\nIntrusion\\nDetection,\\nMalware\\nDetectionPre-training,\\nFine-tuningSliding Window\\nAttention, Query\\nAttention (GQA),\\nByte-fallback BPE\\ntokenizerNA [139]\\nCerebras-\\nGPT 2.7BDecoder-only NA 2.7B 371B The Pile\\nDataset825 GB Cerebras Language\\nModeling, Text\\nCompletion,\\nQAThreat\\nIntelligence,\\nIntrusion\\nDetection,\\nMalware\\nDetectionPre-training standard trainable\\npositional embed-\\ndings and GPT-2\\ntransformer, GPT-\\n2/3 vocabulary and\\ntokenizer blockNA [140]\\nZySec-\\nAI/ZySec\\n7BDecoder-only NA 7.24B NA Trained across\\n30+ domains in\\ncybersecurityNA ZySec AI Language\\nModeling, Text\\nCompletion,\\nQAExpert guidance\\nin cybersecurity\\nissuesPre-training NA NA [141]\\nDeciLM\\n7BDecoder-only NA 7.04 NA NA NA Deci Language\\nModeling, Text\\nCompletion,\\nQAThreat\\nIntelligence,\\nIntrusion\\nDetection,\\nMalware\\nDetectionPre-trained Grouped-Query\\nAttention (GQA)NA [142]\\n27', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e06e60b1-2106-4527-880e-2ea7a5618df6', embedding=None, metadata={'page_label': '28', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE VIII: Continued\\nModel Architecture Base Model Para-\\nmetersTraining\\nTokensPre-training Corpus\\nVolumeReleased\\nByApplications Use Cases in\\nCybersecurityTraining\\nSchemeKey Training\\nTechniquesQuanti-\\nzationRef\\nZephyr 7B\\nBetaDecoder-only Mistral 7B 7.24B NA NA NA Hugging-\\nFaceLanguage\\nModeling, Text\\nCompletion,\\nQAThreat\\nIntelligence,\\nIntrusion\\nDetection,\\nMalware\\nDetectionFine-tuning Flash Attention,\\nDirect Preference\\nOptimization\\n(DPO)NA [143]\\nDolly v2 12B Decoder-only Pythia 12B 12B 3T The Pile\\nDataset825GiB Databricks Language\\nModeling, Text\\nCompletion,\\nQAThreat\\nIntelligence,\\nIntrusion\\nDetection,\\nMalware\\nDetectionFine-tuning NA NA [144]\\nFalcon2 11B Decoder-only NA 11.1B 5T RefinedWeb\\nenhanced\\nwith curated\\ncorpora.NA TII Language\\nModeling, Text\\nCompletion,\\nQAMalware\\nDetection,\\nThreat\\nIntelligence,\\nSocial\\nEngineering\\nDetectionPre-training ZeRO, high-\\nperformance\\nTriton kernels,\\nFlashAttention-2NA [114]\\n1) Define Objectives: Defining the objectives for a cyberse-\\ncurity dataset for LLMs is crucial as it dictates its construction\\nand application. For training purposes, the dataset should cover\\nvarious cybersecurity topics and incorporate various data types\\nlike text, code, and logs, aiming to develop a robust and\\nversatile LLM capable of understanding diverse threats (e.g.,\\nEdge-IIoT dataset [159] for Network Security and FormAI\\ndataset [157], [160] for Software Security). For evaluation,\\nthe focus narrows to specific areas, such as benchmarking the\\nLLMs’ knowledge in cybersecurity (e.g., CyberMetric [89]).\\n2) Scope and Content Gathering: For the scope and content\\ngathering stage of building a cybersecurity dataset aimed at\\ntraining and fine-tuning LLMs, selecting a broad range of\\ntopics is essential to ensure comprehensive coverage. Key\\nareas include network security, malware analysis, software\\nsecurity, cryptographic protocols, cloud security, and incident\\nresponse. The data should be sourced from diverse and reliable\\norigins, such as public and private databases such as Common\\nWeakness Enumeration (CWE) and Common Vulnerabilities\\nand Exposures (CVE) [161], [162].\\n3) Data Cleaning and Preprocessing: This process involves\\nfiltering out irrelevant content to maintain a focus on cy-\\nbersecurity and standardizing formats across the dataset. For\\nexample, processing the Starcoder 2 dataset [149] involves\\nseveral meticulous steps to refine GitHub issues collected\\nfrom GHArchive. Initially, auto-generated texts from email\\nreplies and brief messages under 200 characters are removed,\\nalong with truncating longer comments to maintain a max-\\nimum of 100 lines while preserving the last 20 lines. This\\nstep alone reduced the dataset volume by 17%. The dataset\\nthen undergoes further cleaning to remove comments by bots\\nidentified through specific keywords in usernames, eliminating\\nan additional 3% of the issues. A notable focus is placed on the\\ninteraction quality within the issues; conversations with two or\\nmore users are prioritized, and those with extensive text under\\na single user are preserved if they stay under 7,000 characters.\\nIssues dominated by a single user with more than ten events\\nare excluded, recognizing them as potentially low-quality or\\nbot-driven, resulting in a 38% reduction of the remaining\\ndataset. For privacy, usernames are anonymized by replac-ing them with a sequential participant counter, maintaining\\nconfidentiality while preserving the integrity of conversational\\ndynamics.\\n4) Annotation and Labeling: A sophisticated hybrid ap-\\nproach can be adopted to ensure precision and scalability in\\nthe annotation and labeling stage of developing a cybersecurity\\ndataset for LLMs. Cybersecurity experts manually annotate the\\ndataset, meticulously labeling complex attributes such as threat\\ntype, guaranteeing high accuracy. Concurrently, automated\\ntools like static analyzers (e.g., Clang for C/C++ and Bandit\\nfor Python), formal verification methods (e.g., ESBMC), and\\ndynamic tools are employed to handle the large volume of\\ndata efficiently. These tools initially tag the data, which human\\nexperts carefully review and correct [163].\\nB. Software Cyber Security datasets\\nIn software cyber security, datasets play a crucial role\\nin understanding, detecting, and mitigating vulnerabilities in\\nsoftware systems. This sub-section explores several significant\\nsoftware cybersecurity datasets, each offering unique insights\\nand methodologies for vulnerability analysis in cybersecurity.\\nFrom the extensive BigVul dataset, which links vulnerabilities\\nin the CVE database to specific code commits, to the inno-\\nvative FormAI dataset, leveraging AI-generated C programs\\nand advanced verification methods for precise vulnerability\\nclassification, each dataset contributes uniquely to the field.\\nThese datasets range from manually labeled by security ex-\\nperts to those generated using state-of-the-art automated tools,\\nproviding diverse resources for researchers and practitioners.\\nTable XIII provides an overview of software vulnerability\\ndatasets that can be used for fine-tuning LLMs for software\\nsecurity.\\n1) Sate IV - Juliet dataset: Nist has developed the SARD\\nJuliet dataset to assess the capabilities of static analysis tools\\non C/C++ program code out of many other programming\\nlanguages. The dataset contains the source files, with each test\\ncase containing bad functions and good functions that patch\\nthe vulnerable “bad” code. Test cases are labeled with CWEs\\nto indicate the type of vulnerability exposed in the program.\\n28', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='026b8746-bde6-4645-ab08-add2d64b0c9c', embedding=None, metadata={'page_label': '29', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE IX: Comparison of Code-specific Large Language Models\\nModel Architecture Base Model Para-\\nmetersTraining\\nTokensPre-training Corpus\\nVolumeReleased\\nByApplications Use Cases in\\nCybersecurityTraining\\nSchemeKey Training\\nTechniquesQuanti-\\nzationRef\\nSantaCoder Decoder-only NA 1.1B 236B The Stack\\nv1.1 dataset\\n(Python, Java,\\nand JavaScript)268GB Hugging-\\nFace,\\nServi-\\nceNowCode\\nGeneration,\\nCode\\nCompletion,\\nCode Analysis,\\nQAThreat\\nIntelligence,\\nSoftware\\nVulnerability,\\nSource Code\\nGenerationPre-training Multi Query\\nAttention (MQA),\\nFill-in-the-Middle\\n(FIM)NA [123]\\nStarCoder Decoder-only NA 15.5B PT\\n1000B,\\nFT 35B80+\\nprogramming\\nlanguages,\\nGit commits,\\nGitHub issues,\\nand Jupyter\\nnotebooks+800GB Hugging-\\nFace,\\nServi-\\nceNowCode\\nGeneration,\\nCode\\nCompletion,\\nCode Analysis,\\nQAThreat\\nIntelligence,\\nSoftware\\nVulnerability\\nDetectionPre-training,\\nFine-tuningFill-in-the-Middle\\n(FIM), Multi\\nQuery Attention\\n(MQA), Learned\\nabsolute positional\\nembeddingsNA [124]\\nStarChat Al-\\nphaDecoder-only StarCoder-\\nbase16B NA oasst1 and\\ndatabricks-\\ndolly-15k\\ndatasetsNA Hugging-\\nFace,\\nServi-\\nceNowCode\\nGeneration,\\nCode\\nCompletion,\\nCode Analysis,\\nQAThreat\\nIntelligence,\\nSoftware\\nVulnerabilityFine-tuning NA NA [125]\\nCodeGen2 Decoder-only\\n(causal LM)NA 1-16B 400B Stack dataset\\nv1.1NA Salesforce Program\\nSynthesis,\\nCode\\nGenerationThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training Causal Language\\nModeling, Cross-\\nentropy Loss,\\nFile-level Span\\nCorruption,\\nInfillingNA [126]\\nCodeGen2.5 Decoder-only\\n(causal LM)NA 7B 1400B StarCoderData NA Salesforce Code\\nGeneration,\\nCode\\nCompletion,\\nCode AnalysisThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training Flash Attention,\\nInfill Sampling,\\nSpan CorruptionNA [127]\\nCodeT5+ Encoder-\\ndecoderNA 220M-\\n16B51.5B CodeSearchNet\\ndataset,\\nGitHub code\\ndatasetNA Salesforce Code\\nGeneration and\\nCompletion,\\nMath\\nProgramming,\\nText-to-code\\nRetrieval TasksThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training Span Denoising,\\nContrastive\\nLearning, text-\\ncode Matching,\\nCausal Language\\nModeling (CLM)NA [128]\\nXGen-7B Decoder-only NA 7B 1500B GitHub,\\nSeveral public\\nsources, Apex\\ncode data\\n(mixture of\\nnatural text\\ndata and code\\ndata)NA Salesforce Code Genera-\\ntion, Summa-\\nrizationThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training,\\nFine-tuningStandard Dense\\nAttention, Two-\\nstage Training\\nStrategyNA [129]\\nReplit Code\\nV1Decoder-only\\n(causal LM)NA 2.7B 525B Stack Dedup\\nv1.2 dataset\\n(20 different\\nlanguages)NA Replit,\\nInc.Code\\nCompletion,\\nCode\\nGenerationThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training Flash Attention,\\nAliBi Positional\\nEmbeddings,\\nLionW OptimizerMatrix\\nMultipli-\\ncation[130]\\nDeciCoder-\\n1BDecoder-only NA 1B 446B StarCoderData\\n(Python, Java,\\nand JavaScript)NA Deci Code\\nCompletion,\\nCode\\nGeneration,\\nCode AnalysisThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training Fill-in-the-Middle\\ntraining (FIM),\\nGrouped Query\\nAttention (GQA)NA [131]\\nCodeLLAMA Decoder-only LLaMA2 7-34B 620B Text and code\\nfrom multiple\\ndatasetsNA Meta Code\\nCompletion,\\nCode\\nGeneration,\\nCode AnalysisThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training,\\nFine-tuningCausal Infilling,\\nAutoregressive\\nTraining,\\nRepository-level\\nReasoning, Long-\\ncontext Fine-\\ntuningNA [132]\\nCodeQwen1.5-\\n7BDecoder-only Qwen1.5 7.25B 3T code-related\\ndataNA Qwen Code\\nGeneration,\\nCode\\nCompletion,\\nCode AnalysisThreat\\nIntelligence,\\nSoftware\\nVulnerability,\\nBug fixesPre-training Flash Attention,\\nRoPE, Grouped-\\nQuery Attention\\n(GQA)NA [133]\\nDeepSeek\\nCoder-33B-\\ninstructDecoder-only NA 33.3B 2T Composition\\nof code\\nand natural\\nlanguageNA DeepSeek Code\\nGeneration,\\nCode\\nCompletion,\\nCode AnalysisThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training,\\nLong-\\ncontext\\npre-training,\\nInstruction\\nfine-tuningFlash Attention,\\nRoPE, Grouped-\\nQuery Attention\\n(GQA)NA [134]\\nCodeGemma-\\n7BDecoder-only Gemma 8.54B 500B Code\\nrepositories,\\nMathematics\\ndatasets,\\nSynthetic codeNA Google Code\\ncompletion,\\nCode\\ngeneration,\\nCode chat,\\nInstruction\\nfollowingThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training,\\nFine-tuningFill-in-the-middle\\n(FIM) tasks,\\ndependency graph-\\nbased packing,\\nunit test-based\\nlexical packingNA [135]\\n29', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d2115b6-e916-4cd8-997f-c8145ff20341', embedding=None, metadata={'page_label': '30', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE X: Continued\\nGranite 8B\\nCodeDecoder-only NA 8.05B 4.05T Publicly\\nDatasets\\n(GitHub\\nCode Clean,\\nStarcoder data)NA IBM\\nGraniteCode\\ngeneration,\\nCode\\nexplanation,\\nCode fixing,\\netc.Threat\\nIntelligence,\\nIntrusion\\nDetection,\\nMalware\\nDetectionPre-trained\\nin two\\nphases (the\\nsecond\\nphase for\\nhigh-quality\\ndata)RoPE embedding,\\nGrouped-Query\\nAttention (GQA),\\nContext Length of\\n4096 TokensNA [136]\\nDeepSeek-V2 Decoder-only NA 236B 8.1T Composition\\nof code\\nand natural\\nlanguageNA DeepSeek Code\\nGeneration,\\nCode\\nCompletion,\\nCode AnalysisThreat\\nIntelligence,\\nSoftware\\nVulnerabilityPre-training,\\nSFT, RL,\\nLong\\nContext\\nExtensionMixture-of-\\nExperts (MoE),\\nMulti-head Latent\\nAttention (MLA)NA [137]\\nTABLE XI: Datasets Used for Pre-training Foundation Models in Coding\\nDataset Title Year Purpose Content Significance\\nCodeSearchNet\\n[145]”CodeSearchNet Challenge:\\nEvaluating the State of\\nSemantic Code Search”2019Focuses on bridging natural\\nlanguage and code.Contains about 6 million\\nfunctions from six languages\\nand 2 million automatically\\ngenerated query-like\\nannotations.Advances the semantic code\\nsearch field with a challenge\\nincluding 99 queries and 4k\\nexpert annotations.\\nThe Pile [146]”The Pile: An 800GB Dataset\\nof Diverse Text for Language\\nModeling”2020Designed to train large-scale\\nlanguage models.Comprises 22 high-quality, di-\\nverse text subsets totaling 825\\nGiB.Improves model\\ngeneralization capabilities;\\nevaluates with GPT-2 and\\nGPT-3.\\nCodeParrot1CodeParrot Dataset 2022Facilitates model training in\\ncode understanding and gen-\\neration.Consists of 115M code files\\nfrom GitHub in 32 program-\\nming languages, totaling 1TB.Aids in diverse language and\\nformat model training.\\nThe Stack\\n[147]”The Stack: 3 TB of permis-\\nsively licensed source code”2022Aimed at fostering research\\non AI for code.Features 3.1 TB of code in 30\\nprogramming languages.Demonstrates improved per-\\nformance on text2code bench-\\nmarks; introduces data gover-\\nnance.\\nROOTS [148]”The BigScience ROOTS\\nCorpus: A 1.6TB Composite\\nMultilingual Dataset”2023Supports ethical, multilingual\\nmodel research.Spans 59 languages and fo-\\ncuses on diverse, inclusive\\ndata.Advances large-scale\\nlanguage model research\\nwith an ethical approach.\\nThe Stack v2\\n[149]”StarCoder 2 and The Stack\\nv2: The Next Generation”2024Enhances foundation models\\nfor code.Built from sources including\\n619 programming languages,\\nsignificantly larger than its\\npredecessor.Shows improvements in code\\nLLM benchmarks; ensures\\ntransparency in training data.\\nFig. 6: Cyber Security Dataset Lifecycle for LLM development.\\nThe dataset contains keywords to indicate precisely where vul-\\nnerable and non-vulnerable functions exist. Thus, the dataset\\nneeds careful sanitization /obfuscation. While the dataset has\\nmany vulnerability types and gives concrete examples, they are\\nstill programs purposefully built to demonstrate vulnerabilities\\nrather than naturally occurring ones.\\n2) Draper dataset: Researchers in work [164] leveraged\\na new dataset for vulnerability detection using deep rep-\\nresentation. A custom lexer was used to create a generic\\nrepresentation to capture the essential tokens while minimizing\\nthe token count. It was curated using open-source C/C++ codefrom SATE IV , Github, and Debian and labeled using three\\nstatic analyzers. The dataset is substantial, but the vulnerability\\npercentage is low, standing at roughly 6.8%. The dataset is\\nmulti-labelled, where more than one CWE can exist in a\\ncode sample. The dataset focuses on four main CWEs or\\ncategories, while the rest of the vulnerabilities are grouped\\ninto one class. The researchers mapped the static analyzer\\nfindings to CWEs and binary labels. Furthermore, since the\\nresearchers did the mapping, warnings, and functions flagged\\nby static analyzers that would not typically be exploited were\\nnot flagged as vulnerable. In addition, a strict deduplication\\n30', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7101d90a-3918-47f4-a3dd-1ffc2249d781', embedding=None, metadata={'page_label': '31', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE XII: Comparative Analysis of Vulnerabilities in LLM-Generated Code\\nReference Year Primary Focus Methodology Key Findings\\nSchuster et al. [150] 2021 Poisoning in code autocom-\\npletionExperimental poisoning attacks on autocom-\\npletersDemonstrated effective targeted and untar-\\ngeted poisoning; current defenses are largely\\nineffective.\\nAsare et al. [151] 2023 Security analysis of GitHub’s\\nCopilotEmpirical analysis comparing human and\\nCopilot-generated code vulnerabilitiesCopilot does not consistently replicate human\\nvulnerabilities, showing variable performance\\nacross different types.\\nSandoval et al.\\n[152]2023 Security implications of LLM\\ncode assistants in C program-\\nmingUser study with AI-assisted coding tasks Minimal increase in security risks from LLM\\nassistance compared to control.\\nPerry et al. [153] 2023 Impact of AI code assistants\\non securityLarge-scale user study on security task per-\\nformanceParticipants using AI wrote less secure code\\nbut were overconfident in its security.\\nHamer et al. [154] 2024 Security vulnerabilities in\\nLLM vs. StackOverflow codeEmpirical analysis of code snippets for secu-\\nrity vulnerabilitiesLLM-generated code had fewer vulnerabili-\\nties than StackOverflow, highlighting differ-\\nences in security risks.\\nCotroneo et al.\\n[155]2024 Security assessment tool for\\nAI-generated codeDevelopment and validation of DeV AIC tool DeV AIC effectively identifies vulnerabilities\\nin Python code, outperforming other tools.\\nT´oth et al. [156] 2024 Evaluating security of LLM-\\ngenerated PHP web codeHybrid evaluation using static and dynamic\\nanalysisSignificant vulnerabilities found in AI-\\ngenerated PHP code, emphasizing the need\\nfor thorough testing.\\nTihanyi et al. [157] 2024 Security of LLM-generated C\\ncode from neutral promptsDataset creation and analysis using formal\\nverificationOver 63% of generated C programs were\\nfound vulnerable, with minor variations be-\\ntween different LLMs.\\nprocess was used to refine the dataset. The authors utilize this\\ndataset to train their model after lexing the source code to\\nreduce the code representation and use a limited vocabulary\\nsize. Due to lexing the source code, the approach reduces the\\nneeded vocabulary size compared to the original size required.\\nHowever, the vulnerable portion is minimal compared to\\nthe dataset. Moreover, the labeling considers four categories,\\nwhich are limited compared to other datasets.\\n3) Reveal dataset: Reveal [165] was proposed to provide\\nan accurate dataset that reflects the real world, which is\\nwhy it is also reflected in the imbalance of the samples.\\nTheir work finds that performance drops by more than 50%\\nin real-world prediction, highlighting the need for a dataset\\nsubjected to a realistic setting. The authors focus on two\\nopen-source projects, Linux Debian and Chromium, as they\\nare popular, well-maintained, showcase important domains,\\nand have publicly available vulnerability reports. The data\\nis not used as text but as Code Property Graphs (CPG),\\nwhich are then converted to graph embeddings for training\\na Gated Graph Neural Network (GGNN). The authors use\\nan approach inspired by Zhou et. al [166] to identify the\\nsecurity vulnerabilities in the project, and they remedy the\\nclass imbalance due to the majority of non-vulnerable code\\nthrough SMOTE. Such data was collected from Bugzilla and\\nDebian security tracker. Looking at the vulnerable portion,\\nit constitutes 9.16% out of the 18,169 programs. While the\\ndataset attempts to depict a realistic dataset, relying on two\\nsole projects might limit how well a model trained on the\\ndataset would perform in a real-world prediction case.\\n4) Devign dataset: Researchers of Devign [162] required\\nan accurate dataset to be used in several graph forms, which\\nthey believe is better in reflecting the structural and logical\\naspects of source code. The proposed approach contains a\\ngraph embedding layer that uses Abstract Syntax Tree (AST),\\nControl Flow Graph (CFG), Data Flow Graph (DFG), and\\nNatural Code Sequence (NCS) to generate a joint graph rep-\\nresentation. The rationale behind a joint representation is the\\nability of certain graphs to portray different vulnerability types\\nnot uncovered by others. Motivated to propose a more accuratedataset instead of those generated using static analyzers, the\\nresearchers invested in a security team to manually label\\nthe samples. The data is collected from large open-source\\nprojects: Linux, Wireshark, QEMU, and FFmpeg. The dataset\\nis manually labeled over two rounds, with 600 hours put into\\nlabeling it. While a significant advantage of the dataset is that\\nit is manually labeled and verified, the dataset is only binary\\nlabeled. Also, it is worth noting that only 2 out of the four\\ndatasets are available.\\n5) VUDENC: The VUDENC dataset [167] is comprised\\nof 25,040 vulnerability-fixing commits from 14,686 different\\nGitHub repositories. The commits were filtered only to include\\nthose that changed the code in a limited number of places,\\nensuring that the changed code was related to the commit mes-\\nsage. The dataset covers seven common vulnerability types:\\nSQL injection, cross-site scripting (XSS), command injection,\\ncross-site request forgery (XSRF), remote code execution, path\\ndisclosure, and open redirect. This Python-specific dataset\\nfocuses on improving software systems’ security by identi-\\nfying potentially vulnerable code. Each vulnerability type has\\na dedicated dataset, with the number of repositories ranging\\nfrom 39 to 336 and the number of changed files ranging from\\n80 to 650. The total lines of code across all vulnerability types\\nexceed 200,000, demonstrating the comprehensive nature of\\nthe dataset.\\n6) BigVul dataset: BigVul [168] is a C/C++ vulnerability\\ndataset curated from the CVE database and its relevant open-\\nsource projects. 3,754 code vulnerabilities were collected from\\n348 open-source projects spanning 91 vulnerability types. The\\ndataset links CVEs in the CVE database with code commits\\nand project bug reports. Furthermore, the dataset contains 21\\nfeatures to show changes and where the vulnerability lies.\\nCompared to other datasets, BigVul provides many charac-\\nteristics that can be useful for thoroughly analyzing vulner-\\nabilities and the history of change. Moreover, the diversity\\nof the projects and the vulnerability types expose the models\\nbeing trained on it to several patterns. However, the dataset\\nonly contains 11,823 vulnerable functions as opposed to the\\n253,096 non-vulnerable functions. While it may depict real\\n31', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='48f1f050-913e-44a1-909f-6766e7a94d1a', embedding=None, metadata={'page_label': '32', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='projects, the data is imbalanced, and more vulnerable functions\\nare needed to train large models.\\n7) D2A dataset: A Dataset proposed by IBM [161] is\\ncurated using differential analysis to label issues reported by\\nstatic analysis tools. Bug-fixing commit pairs are extracted\\nfrom open-source projects with a static analyzer running on\\nthem. If issues were detected in the “before” version and\\ndisappeared in the “after” version, then it is assumed to be\\na bug. Compared to other datasets, the bug trace is included\\nin the dataset to determine the type and exact location of the\\nbug. Open-source projects such as FFmpeg, OpenSSL, httpd,\\nlibtiff, libav and NGINX constitute the curated dataset. This\\ndataset also has a limited number of vulnerable samples, and a\\nmanual validation experiment shows that the results are better\\nthan those of regular differential analysis. However, it is still\\nnot at the desired accuracy, with manual validation showing\\nan accuracy of 53%. The paper’s authors applied the dataset\\nto build a classifier to identify false alarms in static analyzers\\nto reduce the false positive rate.\\n8) CVEfixes: CVEfixes [169] is a dataset built using the\\nmethod proposed by the authors to curate vulnerability datasets\\nbased on CVEs. The automated tool was used to release\\nCVEfixes, a dataset covering CVEs up to 9 June 2021. The\\ndataset is organized in a relational database, which can be\\nused to extract data with the desired information. It contains\\nthe code changes in several levels, namely on the repository,\\ncommit, file, and method levels. The dataset contains 5495\\nvulnerability fixing commits with 5365 CVE records, covering\\n1754 open-source projects. The mining tool is shared, and the\\nmost recent CVE records can be mined.\\n9) CrossVul: The CrossVul dataset [170] encompasses a\\ndiverse range of programming languages, exceeding 40 in\\ntotal, and comprises vulnerable source files. The dataset was\\ncurated by extracting data from GitHub projects referenced\\nby the National Vulnerability Database (NVD), specifically\\nfocusing on files modified through git-diff. Files preceding\\nthe commit are tagged as vulnerable, while those following\\nthe commit are designated as non-vulnerable. Organized by\\nCommon Weakness Enumerations (CWEs)/Common Vulnera-\\nbilities and Exposures (CVEs), as well as language types, the\\ndataset offers a comprehensive classification of vulnerabilities.\\nIt encompasses 1675 GitHub projects, spanning 5877 commits\\nand 27,476 files, with an equal distribution of 13,738 files\\nmarked as vulnerable and non-vulnerable, respectively. A\\nsupplementary dataset containing the commit messages for\\neach sample is provided.\\n10) SySeVR dataset: SySeVR framework was proposed\\nin [171], which builds on the previous work in VulDeeP-\\necker [172]. While VulDeePecker only considers library/ API\\nfunction calls, SySeVR covers a variety of vulnerabilities.\\nFurthermore, SySeVR utilized a unique approach using the\\nnotions of syntax-based Vulnerability Candidates(SyVCs) and\\nSemantics-based Vulnerability Candidates (SeVCs) to rep-\\nresent programs as vectors that accommodate syntax and\\nsemantic information. Their approach results show a reduced\\nfalse-negative rate. The dataset is collected from the National\\nVulnerability Database (NVD) and Software Assurance Refer-\\nence Dataset (SARD). The NVD dataset contains 19 popularC/C++ open source products and the SARD data comprises\\n126 vulnerability types. There are 1,591 programs from open-\\nsource projects, of which 874 are vulnerable. As for SARD,\\nthere are 14,000 programs, with 13,906 being vulnerable.\\nWhile this dataset uses the existing datasets published by\\nNIST, the datasets would need further processing in most\\ncases. For example, many vulnerable SARD programs contain\\nthe vulnerable snippet and its patch. Not separating them into\\ndifferent samples might yield unwanted results depending on\\nthe application.\\n11) DiverseVul dataset: DiverseVul [173] is proposed as\\na new vulnerable source code dataset that covers 295 than\\nthe previous datasets combined. Furthermore, the dataset is\\n60% bigger than previous open-source C/C++ datasets. The\\ndata is collected by crawling security issue websites and ex-\\ntracting the commits. The security-based commits are labeled\\nvulnerable before and non-vulnerable for the version after\\nthe commit. DiverseVul covers over 797 projects and 7,514\\ncommits with more than 130 CWEs. The MD5 hashes are\\nused to de-duplicate functions, yielding 18,495 vulnerable and\\n330,492 non-vulnerable functions. The authors conduct several\\nexperiments to validate the dataset, combining their dataset\\nwith previous datasets and showing insights and possibilities of\\ntheir use. The paper shows that using their dataset along with\\nthe previous datasets yields the best result in their experiments,\\nas opposed to using a single dataset.\\n12) FormAI dataset: The FormAI dataset [160] represents\\na significant advancement in cybersecurity and LLM, featuring\\nan extensive collection of 112,000 AI-generated, independent,\\nand compilable C programs. This dataset is unique because\\nit utilizes dynamic zero-shot prompting techniques to create\\nvarious programs, ranging from complex tasks like network\\nmanagement and encryption to simpler ones like string manip-\\nulation. These programs were generated using GPT-3.5-turbo,\\ndemonstrating the ability of Large Language Models (LLMs)\\nto produce diverse and realistic code samples. A standout\\nfeature of the FormAI dataset is its meticulous vulnerability\\nclassification. Each program is thoroughly analyzed for vulner-\\nabilities, with the type of vulnerability, the specific line num-\\nber, and the name of the vulnerable function clearly labeled.\\nThis precise labeling is achieved using the Efficient SMT-\\nbased Bounded Model Checker (ESBMC), an advanced formal\\nverification method. ESBMC employs techniques like model\\nchecking, abstract interpretation, constraint programming, and\\nsatisfiability modulo theories to rigorously assess safety and\\nsecurity properties in the programs. This approach ensures that\\nvulnerabilities are definitively detected, providing a formal\\nmodel or counterexample for each finding and effectively\\neliminating false positives.\\n13) Chrysalis-HLS: Chrysalis-HLS [75] dataset, a helpful\\nresource for improving Large Language Models’ performance\\nin hardware and software design. This comprehensive dataset\\ntargets functional verification and code debugging in High-\\nLevel Synthesis (HLS). It offers a realistic evaluation envi-\\nronment with over 1,000 function-level designs and up to 45\\ninjected bug combinations. Named ”Chrysalis” to symbolize\\ncode transformation, it includes diverse HLS applications with\\nvarious error types. Created with GPT-4 and curated prompts,\\n32', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2c49bb18-94ec-4e1e-970a-5a13cbee2f8c', embedding=None, metadata={'page_label': '33', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chrysalis-HLS is a valuable resource for advancing LLM\\ncapabilities in HLS verification and debugging, enhancing\\nhardware engineering.\\n14) ReFormAI: The ReFormAI dataset [174] is a large-\\nscale dataset of 60,000 independent SystemVerilog designs\\nwith varied complexity levels, targeting different Common\\nWeakness Enumerations (CWEs). The dataset was generated\\nby four different LLMs and features a unique set of designs\\nfor each of the 10 CWEs evaluated. The designs were labeled\\nbased on the vulnerabilities identified by formal verification\\nwith unbounded proof. The LLMs evaluated include GPT-3.5-\\nTurbo, Perplexity AI, Text-Davinci-003, and LLaMA. The re-\\nsults indicate that at least 60% of the samples from the 60,000\\nSystemVerilog designs are vulnerable to CWEs, highlighting\\nthe need for caution when using LLM-generated code in real-\\nworld projects.\\n15) PrimeVul: PrimeVul [80] dataset is a benchmark\\ndataset based on existing open-source datasets. Mainly taking\\ninto consideration BigVul [168], CrossVul [170], CVEfixes\\n[169] and DiverseVul [173]. The proposed pipeline consists of\\nmerging, de-duplication, and labeling through 1) PRIMEVUL-\\nONEFUNC and 2) PRIMEVUL-NVDCHECK. ONEFUNC\\nselects only single functions that are associated with security\\ncommits. NVDCHECK is the compartment where a commit\\nis linked to its CVE and checked for in the NVD database.\\nThe function is labeled vulnerable if the description precisely\\nmentions the function. The other case is the description\\ncontaining the file name and the function being the single\\nfunction changed by a security commit. After such a process,\\nthe yielded dataset consists of 7k vulnerable functions and\\n228,800 benign functions. The dataset spans 755 projects and\\ncontains 6,827 commits. Their work also assesses the label\\nquality of their dataset and other related datasets, showing\\nlow label errors in PrimeVul.\\n16) X1: X1 [78] dataset is constructed from several open-\\nsource vulnerability datasets: CVEFixes, a Manually-Curated\\nDataset, and VCMatch. The dataset contains standalone func-\\ntions labeled as either vulnerable or non-vulnerable. The label-\\ning process involves extracting functions from vulnerability-\\nfixing commits, assuming pre-change versions are vulnerable\\nand post-change versions are non-vulnerable. A modified\\ndataset (X1) is created to address potential false positives,\\ncontaining only functions that were the sole change in a\\ncommit. The final dataset consists of X1 without P3, which\\nhas 1334 samples, and X1 with P3, which has 22945 samples.\\nX1 without P3 is balanced, with a 1:1 ratio of positive to\\nnegative classes, while X1 with P3 is imbalanced, reflecting\\nthe real-world distribution of vulnerable functions with a 1:34\\nratio. The dataset size is relatively small, which may limit its\\nrepresentativeness of the real vulnerability distribution.\\nVIII. LLM VULNERABILITIES AND MITIGATION\\nThis section reviews the OWASP Top 10 for LLM Appli-\\ncations project [175], a comprehensive initiative designed to\\nincrease awareness about LLM security vulnerabilities. This\\nproject targets a wide audience, including developers, design-\\ners, architects, managers, and organizations that deploy andmanage LLMs. Its core deliverable lists the top 10 most critical\\nsecurity vulnerabilities commonly found in LLM applications.\\nIn addition, we include other LLM vulnerabilities not included\\nin the OWASP project, as presented in Table XIV and 7.\\nA. Prompt Injection\\nIntegrating LLMs into various digital platforms has brought\\nto light the critical issue of prompt injection [176]. This\\ncybersecurity concern involves crafting inputs that manipulate\\nLLMs, potentially leading to unauthorized system exploitation\\nor sensitive information disclosure. As LLMs become more\\nprevalent, understanding and countering prompt injection at-\\ntacks is paramount for safeguarding the integrity and security\\nof these systems [177].\\n1) Nature of Prompt Injection Attacks: Prompt injection\\nattacks in LLMs can manifest in various forms. One common\\nmethod involves manipulating the model to retrieve private\\ninformation. Attackers may craft inputs that subtly direct the\\nLLM to divulge confidential data. Another technique involves\\nembedding hidden prompts in web pages, which can solicit\\nsensitive information from unsuspecting users [178]. In addi-\\ntion, attackers might embed specific prompts in documents,\\nsuch as resumes, to alter the LLM’s output for deceptive pur-\\nposes. Finally, the risk of web plugins being exploited through\\nrogue website instructions leads to unauthorized actions by the\\nLLM [179].\\n2) Mitigation Strategies: To combat these threats, several\\nmitigation strategies can be employed. First, operational re-\\nstrictions are vital; limiting the LLM’s capabilities to es-\\nsential functions significantly reduces the risk of malicious\\nexploitation. Requiring user consent for sensitive operations\\nis another critical measure [180]. This approach ensures that\\nhigh-risk activities or operations involving sensitive data only\\noccur with explicit user approval. Therefore, the influence\\nof untrusted or unfamiliar content on user prompts should\\nbe minimized to prevent indirect manipulations. Establishing\\nclear trust boundaries within the system is also crucial. These\\nboundaries maintain user control and prevent unauthorized\\nactions, safeguarding the system from external manipulations\\n[181].\\n3) Potential Attack Scenarios: The scenarios for prompt\\ninjection attacks are diverse and concerning. One scenario\\ninvolves adversarial prompt injections on websites, leading\\nto unauthorized actions by the LLM. Another potential threat\\nis hidden prompt injections in documents like resumes, de-\\nsigned to manipulate the LLM’s output [182]. Furthermore,\\nthere’s the risk of direct user control over the LLM through\\nprompt injections, where malicious users craft inputs to gain\\nundue influence over the model’s responses. By understanding\\nthese risks and implementing robust prevention strategies,\\ndevelopers and users of LLMs can protect against potential\\nexploitations [183].\\nB. Insecure Output Handling\\nThis issue arises when an application or plugin blindly\\ntrusts LLM outputs, funneling them into client-side or backend\\noperations. Such oversight can lead to critical security risks\\n33', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bdd4d4f7-dcae-4d07-9d20-3a43d6a6abd6', embedding=None, metadata={'page_label': '34', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE XIII: Overview of Software Vulnerability Datasets that can be used for fine-tuning LLMs for software security.\\nDataset Year Lang Source Multi-\\nclassType Samples Labelling Method Classification\\nMethodChallenges/Limitations\\nSate IV -\\nJuliet2012 C,\\nC++\\n&\\nJavaSARD Yes Synthetic Approx 60k\\n(C/C++) &\\n29k (Java) test\\ncasesTestcases are vulner-\\nable by design, with\\ncorresponding patchCWE Designed to be vulnerable,\\nmight not accurately depict\\nreal-world projects.\\nDraper\\n[164]2018 C Open-source Yes Real Total: 1.27M\\nV: 82K NV:\\n1.19MStatic analyzers CWE Small percentage of vulnera-\\nble samples. Limited to four\\ncategories.\\nReveal\\n[165]2018 C/C++ Open-source No Real Total: 18k V:\\n1.6k NV: 16kVulnerability-fixing\\ncommits identified by\\nsecurity termsBinary classes Imbalance in sample distribu-\\ntion and only binary labeled.\\nLimited to two projects.\\nDevign\\n[162]2019 C Open-source No Real Total: 26K V:\\n12K NV: 14KBinary Manual label-\\ningBinary classes Binary labeled. Partial dataset\\nrelease.\\nVUDENC\\n[167]2019 Python Open-source Yes Real 1,009 commits\\nfrom 812\\nrepositoriesVulnerability-fixing\\ncommits from GitHub\\nrepositoriesVulnerability\\ntypeRelatively small Dataset, No\\nguarantee that the commits\\nfixed vulnerabilities.\\nBigVul\\n[168]2020 C/C++ Open-source Yes Real Total: 264k\\nV: 11.8k NV:\\n253kVulnerability-fixing\\ncommits from CVE\\ndatabaseCVE/CWE Significant class imbalance.\\nLack of CWEs/categories for\\nall samples.\\nD2A [161] 2021 C/C++ Open-source Yes Real Total: 1.3M\\nV: 18.6k NV:\\n1.27MVunerability-fixing\\ncommits with static\\nanalyzersCategories\\nbased on static\\nanalyzerSmall percentage of vulnera-\\nble samples. Manual valida-\\ntion shows low accuracy.\\nCVEfixes\\n[169]2021 27\\nlan-\\nguagesOpen-source Yes Real 5,495\\ncommits,\\n50k methodsVulnerability-fixing\\ncommits from CVE\\ndatabaseCVE/CWE Labelling accuracy needs en-\\nhancement and dataset size in-\\ncreased (only limited to CVE\\nrecords).\\nCrossVul\\n[170]2021 40+\\nlan-\\nguagesOpen-source Yes Real 5,877\\ncommits,\\n27k (13,738\\nV/NV) filesVulnerability-fixing\\ncommits from CVE\\ndatabaseCVE/CWE Labelling accuracy needs en-\\nhancement and dataset size in-\\ncreased. Takes the whole file\\nwithout pinpointing functions.\\n(only limited to CVE records).\\nSySeVR\\n[171]2022 C/C++ SARD/NVD Yes Semi-\\nSyntheticTotal: 15.6k\\nV: 14.8k NV:\\n811Extracted from exist-\\ning databases NVD\\nand SARDCVE/CWE Limited subset of\\nSARD/NVD. SARD is\\nsynthetic, while NVD is\\nlimited in the number of\\nlabeled vulnerabilities.\\nDiverseVul\\n[173]2023 C/C++ Open-source Yes Real Total: 349K\\nV: 18.9k NV:\\n330KVulnerability-fixing\\ncommits from\\nsecurity trackersCWE Labelling accuracy needs en-\\nhancement and dataset size in-\\ncreased (specifically vulnera-\\nble functions).\\nFormAI\\n[160]2023 C AI-generated Yes Artificial Total: 112k V:\\n57k NV: 55KFormal verification\\nBounded Model\\ncheckerCustom\\ncategoriesBounded formal verification\\ndoes not cover all types of\\nvulnerabilities and depth.\\nChrysalis-\\nHLS [75]2024 C++ Open-source Yes Synthetic Over 1,000\\nfunction-level\\nHLS designsPredefined errors Bug Type Addressing scalability and\\ngeneralization challenges\\nFormAI v2\\n[157]2024 C AI-generated Yes Artificial Total: 265k V:\\n168k NV: 23kFormal verification\\nBounded Model\\ncheckerCustom\\ncategoriesBounded formal verification\\ndoes not cover all vulnerabil-\\nities and depth.\\nReFormAI\\n[174]2024 System\\nVer-\\nilogAI-generated Yes Artificial Total: 60k V:\\n60k NV: 0Formal verification\\nBounded Model\\ncheckerCWE Formal verification with an\\nunbounded proof.\\nPrimeVul\\n[80]2024 C/C++ Open-source Yes Real Total: 236k V:\\n7k NV: 229kSingle function se-\\nlection and extraction\\nfrom NVDCWE Limited vulnerable samples\\ndue filtering existing samples\\nand specific function selec-\\ntion.\\nX1 [78] 2024 Java Open-source Yes Real Total: 22.9k\\nV: 0.6k NV:\\n22.3kAnalyzing\\nvulnerability-fixing\\ncommitsBinary classes Imbalanced, small, and may\\nnot represent the true vulner-\\nability distribution.\\nV : Vulnerable , NV: Non Vulnerable\\nlike Cross-Site Scripting (XSS), Cross-Site Request Forgery\\n(CSRF), Server-Side Request Forgery (SSRF), privilege esca-\\nlation, and remote code execution.\\n1) Nature of Insecure Output Handling Vulnerabilities:\\nThe core of the problem lies in the unverified acceptance\\nof LLM outputs. For example, if LLM-generated content,\\nsuch as JavaScript or Markdown, is directly processed by a\\nbrowser or a backend function, it can lead to XSS or remote\\ncode execution. This highlights the danger of assuming LLM\\noutputs are safe by default, emphasizing the need for thorough\\nvalidation and sanitization.\\n2) Prevention Strategies: Preventing these vulnerabilities\\ninvolves two key strategies. Firstly, implementing stringentvalidation for LLM outputs before interacting with backend\\nfunctions can help identify and neutralize potential threats.\\nSecondly, encoding LLM outputs before they reach the end\\nuser can prevent misinterpretation of the code, thereby reduc-\\ning the risk of malicious executions.\\n3) Potential Attack Scenarios: The scenarios for exploita-\\ntion are varied. They range from an application inadvertently\\nallowing LLM-generated responses to manipulate internal\\nfunctions, leading to unauthorized actions, to an LLM-powered\\ntool capturing and transmitting sensitive data to malicious\\nentities. Other risks include allowing users to generate unvetted\\nSQL queries through an LLM, which could result in data\\nbreaches and the potential for LLMs to create and execute\\n34', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e24e747-e426-4cca-a666-861536490150', embedding=None, metadata={'page_label': '35', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Fig. 7: LLM vulnerabilities included in the OWASP project.\\nharmful XSS payloads.\\nC. Adversarial Natural Language Instructions\\nWu et al. [184] proposed presented DeceptPrompt, high-\\nlighting a critical vulnerability in Code LLMs: their sus-\\nceptibility to adversarial natural language instructions. These\\ninstructions are designed to appear benign while leading\\nCode LLMs to produce functionally accurate code containing\\nhidden vulnerabilities. The DeceptPrompt algorithm utilizes a\\nsophisticated evolution-based methodology with a fine-grained\\nloss design, crafting deceptive instructions that maintain the\\nappearance of normal language inputs while introducing se-\\ncurity flaws into the generated code. This vulnerability is\\nexacerbated by the challenges in preserving the code’s func-\\ntionality, targeting specific vulnerabilities, and maintaining the\\nsemantics of the natural language prompts.\\n1) Prevention Strategies: The study suggests a set of pre-\\nvention strategies to counter these threats. This involves in-\\ntegrating advanced code validation mechanisms within LLMs\\nto identify and mitigate potential vulnerabilities in the gen-\\nerated code. Enriching the training of LLMs with adversarial\\nexamples produced by DeceptPrompt is recommended to boost\\ntheir defense against security threats. Furthermore, continuous\\nupdates and security patches, informed by the latest cybersecu-\\nrity research, are crucial for maintaining the LLMs’ defensesagainst new adversarial techniques. Addressing these chal-\\nlenges involves preserving the code’s functionality, targeting\\nspecific vulnerabilities, and maintaining the semantics of the\\nnatural language prompts used in the generation process.\\n2) Potential Attack Scenarios: The authors highlight vari-\\nous potential attack scenarios that could exploit the vulnera-\\nbilities exposed by DeceptPrompt. These scenarios include at-\\ntackers using crafted natural language prompts to induce Code\\nLLMs into generating code with vulnerabilities, leading to data\\nbreaches, unauthorized access, or system compromises. The\\neffectiveness of DeceptPrompt in real-world settings under-\\nscores the urgency for robust security measures in Code LLMs,\\ngiven their increasing use in critical systems and infrastructure.\\nThe challenges in preserving the code’s functionality, targeting\\nspecific vulnerabilities, and maintaining the semantics of the\\nnatural language prompts add complexity to these potential\\nattack scenarios, amplifying the need for enhanced security\\nprotocols in Code LLMs.\\nD. Automatic adversarial prompt generation\\nIn the highlighted study, Zou et al. [185] address the chal-\\nlenge of automatic adversarial prompt generation in aligned\\nlanguage models. Their method focuses on crafting a specific\\nsuffix that maximizes the likelihood of generating objection-\\nable content when attached to various queries directed at an\\n35', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='878c9923-b868-4a7d-b4b5-8887f29d1ef6', embedding=None, metadata={'page_label': '36', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE XIV: Overview of LLM vulnerabilities and Mitigation\\nVulnerabilities Nature of the Vulnerability Examples Mitigation Strategies Potential Attack Scenarios\\nPrompt Injection Manipulation of LLMs through\\ncrafted inputs leading to unautho-\\nrized exploitation or sensitive in-\\nformation disclosure.•Hidden prompts in web\\npages\\n•Deceptive documents\\n•Rogue web plugin in-\\nstructions•Operational restrictions\\n•User consent for sensitive op-\\nerations\\n•Trust boundaries establishment•Adversarial injections on web-\\nsites\\n•Hidden prompts in documents\\n•Direct user control through\\ncrafted inputs\\nInsecure Output\\nHandlingBlind trust in LLM outputs lead\\nto security risks like XSS, CSRF,\\nSSRF, etc.•Direct processing\\nof LLM-generated\\nJavaScript or Markdown•Validation of LLM outputs\\n•Encoding outputs before reach-\\ning end-users•LLM responses manipulating\\ninternal functions\\n•Generating unvetted SQL\\nqueries\\n•Creating harmful XSS pay-\\nloads\\nInference Data Poi-\\nsoningStealthy activation of malicious\\nresponses under specific opera-\\ntional conditions such as token-\\nlimited output.•Conditions based on\\ntoken-output limits in\\nuser settings\\n•Stealthily altered outputs\\nwhen cost-saving modes\\nare enabled•Monitoring and anomaly de-\\ntection systems specifically de-\\nsigned for conditional outputs\\n•Regular audits of outputs under\\nvarious token limitations•Manipulated responses under\\ntoken limitations leading to\\nmisinformation\\n•Triggered malicious behavior\\nin cost-sensitive environments\\nAdversarial\\nNatural Language\\nInstructionsCode LLMs produce functionally\\naccurate code with hidden vulner-\\nabilities due to adversarial instruc-\\ntions.•DeceptPrompt algorithm\\ncreating deceptive\\ninstructions•Advanced code validation\\n•Training LLMs with adversar-\\nial examples\\n•Continuous updates and secu-\\nrity patches•Crafted prompts leading to\\ncode with vulnerabilities\\n•Unauthorized access or system\\ncompromises\\nAutomatic\\nAdversarial Prompt\\nGenerationAutomated methods to generate\\nprompts that bypass LLM align-\\nment measures.•Crafting specific suffixes\\nfor objectionable content\\ngeneration•Developing advanced align-\\nment algorithms\\n•Real-time monitoring\\n•Training models with new ad-\\nversarial examples•Bypassing alignment measures\\nleading to the generation of ob-\\njectionable content\\nTraining Data Poi-\\nsoningManipulation of training data to\\nskew LLM learning, introducing\\nbiases or vulnerabilities.•Injecting biased or harm-\\nful data into training sets•Verifying data sources\\n•Employing dedicated models\\n•Sandboxing, input filters\\n•Monitoring for poisoning signs•Misleading outputs spreading\\nbiased opinions\\n•Injection of false data into\\ntraining\\nInsecure Plugins Vulnerabilities in plugin design\\nand interaction with external sys-\\ntems or data sources.•Inadequate input valida-\\ntion\\n•Overprivileged access\\n•Insecure API interactions•Rigorous input validation\\n•Adherence to least privilege\\n•Secure API practices\\n•Regular security audits•Exploiting input handling vul-\\nnerabilities\\n•Overprivileged plugins for\\nprivilege escalation\\n•SQL injections\\nDenial of Service\\n(DoS) AttackAttempts to make a system inac-\\ncessible by overwhelming it with\\ntraffic or triggering crashes.•V olume-based attacks\\n•Protocol attacks\\n•Application layer attacks•Rate limiting\\n•Robust infrastructure\\n•Continuous monitoring and\\nrapid response•Overloading servers\\n•Disrupting communication be-\\ntween users and services\\n•Straining system resources\\nLLM. This approach differs significantly from previous meth-\\nods by employing automated techniques, including greedy\\nand gradient-based search strategies. The novelty lies in the\\nmethod’s ability to bypass the alignment measures designed\\nto prevent undesirable outputs from LLMs, effectively cir-\\ncumventing the safeguards put in place. Therefore, the study’s\\nfindings underscore the need for robust prevention strategies\\nagainst these sophisticated adversarial attacks. Enhanced secu-\\nrity measures are required to safeguard aligned LLMs. These\\ncould include developing more advanced alignment algorithms\\nresistant to adversarial manipulations and implementing real-\\ntime monitoring systems to detect and neutralize such attacks.\\nIn addition, continuously updating and training models with\\nnew adversarial examples can help build resilience against\\nthese evolving threats. The focus should be on creating sys-\\ntems that can quickly adapt and respond to the changing tactics\\nof adversarial prompt generation.E. Training Data Poisoning\\nTraining Data Poisoning in LLMs represents a critical\\nsecurity and ethical issue, where malicious actors manipulate\\nthe training dataset to skew the model’s learning process. This\\nmanipulation can range from introducing biased or incorrect\\ndata to embedding hidden, harmful instructions, compromising\\nmodel integrity and reliability. The impact is profound, as\\npoisoned LLMs may produce biased, offensive, or inaccurate\\noutputs, raising significant challenges in detection due to the\\nvast and complex nature of training datasets [186].\\n1) Nature of Training Data Poisoning: Training data poi-\\nsoning in LLMs occurs when an attacker deliberately ma-\\nnipulates the training data or fine-tuning processes. This\\nmanipulation introduces vulnerabilities, backdoors, or biases,\\nsignificantly compromising the model’s security, effectiveness,\\nand ethical behavior. Examples include intentionally including\\ntargeted, inaccurate documents, training models using unver-\\n36', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2aea000b-e75f-4bd4-a3f6-b340263be878', embedding=None, metadata={'page_label': '37', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ified data, or allowing unrestricted dataset access, leading to\\nloss of control. Such actions can detrimentally affect model\\nperformance, erode user trust, and harm brand reputation\\n[187].\\n2) Prevention Strategies: To combat training data poison-\\ning, several prevention strategies are essential. Firstly, verify-\\ning the supply chain of training data and the legitimacy of data\\nsources is crucial. This step ensures the integrity and quality\\nof the data used for training models. Employing dedicated\\nmodels for specific use cases can help isolate and protect\\ndifferent applications from a compromised data source [185].\\nAnother effective strategy is implementing sandboxing and\\ninput filters and ensuring adversarial robustness. In addition,\\nregularly monitoring for signs of poisoning attacks through\\nloss measurement and model analysis is vital in identifying\\nand mitigating such threats.\\nThe prevention of training data poisoning in LLMs can\\nbe significantly bolstered by incorporating advanced strategies\\nbefore and after the training phase. The pre-training defense\\nis a dataset-level strategy that filters suspicious samples from\\nthe training data. This method assumes that text and image\\npairs (i.e., Multimodal data) in a dataset should be relevant\\nto each other. The post-training defense is another crucial\\nstrategy, which involves ”sterilizing” a poisoned model by\\nfurther fine-tuning it on clean data, thus maintaining its utility.\\nThis is conducted by fine-tuning the poisoned models on a\\nclean dataset (e.g., the VG dataset in the study) with a specific\\nlearning rate [186].\\n3) Potential Attack Scenarios: Several potential attack sce-\\nnarios arise from training data poisoning. These include the\\ngeneration of misleading LLM outputs that could spread\\nbiased opinions or even incite hate crimes. Malicious users\\nmight inject false data into training, intentionally skewing\\nthe model’s outputs [188]. Adversaries could also manipulate\\na model’s training data to compromise its integrity. Such\\nscenarios highlight the need for stringent security measures\\nin the training and maintaining LLMs, as the implications of\\ncompromised models extend beyond technical performance to\\nsocietal impacts and ethical considerations.\\nF . Inference Data Poisoning\\n1) Nature of Inference Data Poisoning: Inference data\\npoisoning targets LLMs during their operational phase, unlike\\ntraining-time attacks that tamper with a model’s training\\ndataset. This attack subtly alters the input data to trigger\\nspecific, often malicious behaviors in a model without any\\nmodifications to the model itself. The approach detailed by\\nHe et al. [189] utilizes a novel method where the poison is\\nactivated not by obvious, fixed triggers but by conditions re-\\nlated to output token limitations. Such conditions are generally\\noverlooked as they are a part of normal user interactions aimed\\nat managing computational costs, thus enhancing the stealth\\nand undetectability of attacks.\\n2) Prevention Strategies: Preventing inference data poison-\\ning requires a multi-faceted approach. Firstly, robust anomaly\\ndetection systems can be implemented to scrutinize input\\npatterns and detect deviations from typical user queries. Reg-\\nular audits of model responses under various conditions canalso help identify any inconsistencies that suggest poisoning.\\nImplementing stricter input handling controls and limiting\\nthe impact of token limitation settings could also reduce\\nvulnerabilities.\\n3) Potential Attack Scenarios: The potential scenarios for\\ninference data poisoning are varied and context-dependent.\\nFor example, in a cost-sensitive environment where users\\nfrequently limit token outputs to manage expenses, an attacker\\ncould leverage this setting to trigger harmful responses from\\nthe model. Such scenarios could include delivering incorrect or\\nbiased information, manipulating sentiment in text generation,\\nor generating content that could lead to reputational damage or\\nlegal issues. The BrieFool framework [189] effectively exploits\\nthis vector, demonstrating high success rates in controlled\\nexperiments, highlighting the need for heightened security\\nmeasures in environments where LLMs are deployed.\\nG. Insecure Plugins\\n1) Nature of Insecure Plugins: The nature of insecure\\nplugins in LLMs revolves around several key vulnerabilities\\nthat stem from how these plugins are designed, implemented,\\nand interact with external systems or data sources. These\\nvulnerabilities can compromise the security, reliability, and\\nintegrity of both the LLM and the systems it interacts with.\\nThe primary issues associated with insecure plugins in LLMs\\ninclude inadequate input validation, overprivileged access,\\ninsecure API interactions, SQL injection, and database vul-\\nnerabilities.\\n2) Prevention Strategies: To counter the Insecure Plugins,\\na multi-faceted approach to security is essential. Implementing\\nrigorous input validation, including type-checking, sanitiza-\\ntion, and parameterization, is crucial, especially in data query\\nconstruction. Adhering to the principle of least privilege is key\\nin plugin design; each plugin should only access necessary\\nresources and functionalities. Ensuring secure API practices\\nand avoiding direct URL construction from user inputs is vital.\\nEmploying parameterized queries for SQL interactions helps\\nprevent injection attacks. In addition, regular security audits\\nand vulnerability assessments are necessary to identify and\\naddress potential weaknesses proactively.\\n3) Potential Attack Scenarios: Various attack scenarios\\nemerge from Insecure Plugins. For instance, an attacker could\\nexploit input handling vulnerabilities to extract sensitive data\\nor gain unauthorized system access. Overprivileged plugins\\ncould be used for privilege escalation, allowing attackers to\\nperform restricted actions. Manipulation of API calls can lead\\nto redirection to malicious sites, opening doors to further\\nsystem exploits. SQL injection through plugin queries can\\ncompromise database integrity and confidentiality, leading to\\nsignificant data breaches.\\nH. Denial of Service (DoS) attack\\n1) Nature of DoS attack: A Denial of Service (DoS) attack\\nis a malicious attempt to disrupt the normal functioning of a\\ntargeted system, making it inaccessible to its intended users.\\nThe attack typically involves overwhelming the target with a\\nflood of internet traffic. This could be achieved through various\\n37', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0671e508-1bcd-4863-89a9-991f5db22fe4', embedding=None, metadata={'page_label': '38', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='means, such as sending more requests than the system can\\nhandle or sending information that triggers a crash. In the\\ncontext of services like LLM, a DoS attack could bombard the\\nservice with a high volume of complex queries, significantly\\nslowing down the system or causing it to fail [190].\\n2) Potential Attack Scenarios: The DoS attacks against\\nLLM can be divided into three categories: volume-based\\nattacks, protocol attacks, and application layer attacks.\\n•V olume-based Attacks: This is the most straightforward\\nkind of DoS attack, where the attacker attempts to sat-\\nurate the bandwidth of the targeted system. For LLM,\\nthis could involve sending many requests simultaneously,\\nmore than what the servers are equipped to handle,\\nleading to service disruption [191].\\n•Protocol Attacks: These attacks exploit weaknesses in the\\nnetwork protocol stack layers to render the target inacces-\\nsible. They could involve, for instance, manipulating the\\ncommunication process between the user and the LLM\\nservice in a way that disrupts or halts service [192].\\n•Application Layer Attacks: These are more sophisticated\\nand involve sending requests that appear to be legitimate\\nbut are designed to exhaust application resources. For\\nLLM, this could involve complex queries requiring ex-\\ntensive processing power or memory, thereby straining\\nthe system [193].\\n3) Prevention Strategies: To combat DoS attacks in LLM\\nservices, the following prevention strategies can be applied:\\n•Rate Limiting: Implementing a rate-limiting strategy is\\ncrucial. This involves limiting the number of requests a\\nuser can make within a given timeframe, which helps\\nprevent an overload of the system.\\n•Robust Infrastructure: A robust and scalable server in-\\nfrastructure can help absorb the influx of traffic during\\nan attack. This could involve using load balancers, re-\\ndundant systems, and cloud-based services that can scale\\ndynamically in response to increased traffic.\\n•Monitoring and Rapid Response: Continuous traffic mon-\\nitoring can help quickly identify unusual patterns in-\\ndicative of a DoS attack. Once detected, rapid response\\nmeasures, such as traffic filtering or rerouting, can be\\nemployed to mitigate the attack.\\nIX. LLM C YBERSECURITY INSIGHTS , CHALLENGES AND\\nLIMITATIONS\\nA. Challenges and Limitations\\n1) Adapting to Sophisticated Phishing Techniques: The\\nincreasing sophistication of phishing attacks, especially those\\nenhanced by AI, presents a major challenge for LLMs in\\ncybersecurity. These models need to evolve to identify and\\ncounteract these threats effectively continuously. The chal-\\nlenge lies in the need for regular updates and training to keep\\npace with the advanced tactics of attackers, which demands\\nsubstantial resources and expertise. For example, a large com-\\npany implemented an LLM-based security system to detect\\nphishing emails. Initially, the system was highly effective,\\nidentifying and blocking 95% of phishing attempts. However,attackers quickly adapted, using AI to generate more con-\\nvincing phishing emails that mimicked the company’s official\\ncommunication style and included personalized information\\nabout the customers. The company’s LLM struggled to keep\\nup with these advanced tactics. Phishing emails have become\\nso sophisticated that they can bypass traditional detection\\nmethods, significantly increasing the number of successful\\nattacks. Hence, evolving and adapting LLMs in cybersecurity\\nto combat AI-enhanced phishing threats is an open challenge.\\n2) Managing Data Overload in Enterprise Applications:\\nWith the proliferation of enterprise applications, IT teams are\\noverwhelmed by the sheer volume of data they need to manage\\nand secure, often without corresponding increases in staffing.\\nLLMs are expected to assist in managing this data deluge\\nefficiently. However, ensuring these models can process vast\\namounts of data accurately and identify threats amidst this\\ncomplexity is daunting, necessitating high levels of efficiency\\nand accuracy in the LLMs. The corporation faced a situation\\nwhere the LLM failed to recognize a sophisticated cyber-\\nattack hidden within the massive influx of data. This oversight\\noccurred because the model hadn’t been trained with the latest\\nattack patterns, highlighting a gap in its learning. The incident\\nunderscored the need for LLMs to process data efficiently and\\nmaintain high accuracy and adaptability in threat detection.\\n3) Training Data Availability and Quality: A critical chal-\\nlenge for AI-based cyber defense is the lack of high-quality,\\naccessible training data, as organizations generally hesitate\\nto share sensitive information. The effectiveness of LLMs in\\ncybersecurity depends heavily on the quality and availability of\\ntraining data. Overcoming this data gap remains a significant\\nhurdle, whether through synthetic data generation or other\\nmeans.\\n4) Developing and Training Custom Models for Unique\\nCybersecurity Domains: Certain specialized areas in cyber-\\nsecurity require custom models due to their unique vocab-\\nularies or data structures, which standard LLMs might not\\naddress adequately. Unique V ocabularies and Data Structures:\\nCybersecurity domains, such as network security, malware\\nanalysis, and threat intelligence, have their terminologies,\\ndata formats, and communication protocols. Standard LLMs,\\ntypically trained on general datasets, might not be familiar with\\nthese specialized terms and structures, leading to ineffective\\nor inaccurate threat detection and response. Customizing and\\ntraining these models to handle specific cybersecurity scenar-\\nios is complex and demands substantial resources, presenting\\na significant challenge in the field.\\n5) Real-Time Information Provision by Security Copilots:\\nSecurity copilots powered by LLMs need to provide accurate,\\nup-to-date information in real-time to be effective in the\\ndynamic threat landscape of cybersecurity. Ensuring the rele-\\nvance and accuracy of information provided by these models\\nin real-time is challenging but essential for effective responses\\nto cybersecurity threats.\\nB. LLM Cybersecurity Insights\\nTable XV presents various facets of LLM integration into\\ncybersecurity, providing insights into architectural nuances,\\n38', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='81609c47-5553-4be0-ae54-f192016987ec', embedding=None, metadata={'page_label': '39', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE XV: LLM Cybersecurity Insights.\\nAspect Details Tools/Methods Applications\\nArchitecture Focus on model components such as\\ntokenization, attention mechanisms, and\\noutput generation.•Paper: Attention Is All You NeedThreat Detection and Analysis, Security Au-\\ntomation, Cyber Forensics, Penetration Test-\\ning, Security Training and Awareness, and\\nChatbots.\\nCyber Security\\nDatasetCreation of prompt-response pairs that\\nsimulate cyber threats using synthetic\\ndata.•OpenAI API for synthetic data\\n•Evol-Instruct for data refinement\\n•Regex filtering for uniquenessBuilding datasets that mirror real-world\\nthreats for training and refining LLMs.\\nPre-training Models Training on large-scale datasets com-\\nprising billions of tokens, filtered and\\naligned with cybersecurity lexicon.•Megatron-LM for handling large\\ndatasets\\n•gpt-neox for sequential data handling\\n•Distributed training toolsPreparing LLMs to understand and predict\\ncybersecurity-specific content accurately.\\nSupervised Fine-\\nTuningIncorporating specialized cybersecurity\\ndatasets into pre-trained models for tai-\\nlored applications.•LoRA for parameter-efficient adjust-\\nments\\n•QLoRA for quantization and efficient\\nmemory managementEnhancing LLMs to address unique cyberse-\\ncurity threats and scenarios specifically.\\nCyber Security\\nEvaluationSetting up specialized frameworks and\\ndatasets to test LLMs against potential\\ncyber threats.•Bespoke cybersecurity benchmarks\\n•Authoritative datasets for threat detec-\\ntionEvaluating how well LLMs detect, under-\\nstand, and respond to cyber threats.\\nAdvanced LLM\\nTechniquesImplementing techniques like RAG and\\nRLHF to augment LLMs with real-time\\ndata and expert-aligned feedback.•RAG for context retrieval from\\ndatabases\\n•RLHF with specialized preference\\ndatasets and reward modelsImproving response relevance and accuracy\\nin cybersecurity applications.\\nLLM Deployments Adopting deployment strategies that\\nrange from local installations to large-\\nscale server setups.•Platforms like Gradio and Streamlit\\nfor prototyping\\n•Cloud services for robust deployment\\n•Edge deployment strategies for\\nresource-limited environmentsDeploying LLMs in various environments\\nto ensure accessibility and responsiveness\\nacross devices.\\nSecuring LLMs Addressing vulnerabilities unique to\\nLLMs such as prompt hacking and train-\\ning data leakage.•Security measures like prompt injec-\\ntion prevention\\n•Red teaming\\n•Continuous monitoring systemsPreventing and mitigating security threats to\\nmaintain data integrity and model reliability\\nin LLMs.\\nOptimizing LLMs Implementing strategies to reduce mem-\\nory and computational requirements\\nwhile maintaining output quality.•Model quantization\\n•Use of bfloat16 data formats\\n•Optimization of attention mechanismsEnabling efficient LLM operation on various\\nhardware, making them scalable and practical\\nfor diverse applications.\\ndataset creation, pre-training, fine-tuning methodologies, eval-\\nuation metrics, advanced techniques, deployment strategies,\\nsecurity measures, and optimization approaches.\\n1) LLM architecture: A cyber security scientist venturing\\ninto utilizing LLMs must understand the architecture’s nuances\\n(presented in Section III) to tailor these tools for security ap-\\nplications effectively. Understanding the architecture of LLMs,\\nincluding their ability to process and generate language-based\\ndata, is crucial for detecting phishing attempts, deciphering\\nmalicious scripts, or identifying unusual patterns in network\\ntraffic that may indicate a breach. Knowledge of how these\\nmodels tokenize input data, their attention mechanisms to\\nweigh information, and their output generation techniques\\nprovide the foundational skills necessary to tweak models for\\noptimized threat detection and response [194].\\n2) Building Cyber Security dataset: Building a robust cy-\\nbersecurity dataset using LLMs involves generating and refin-\\ning intricate prompt-response pairs to mirror real-world cyber\\nthreats. Employing synthetic data generation via the OpenAI\\nAPI allows for diverse cybersecurity scenarios, while advanced\\ntools like Evol-Instruct [195] enhance dataset quality byadding complexity and removing outdated threats. Techniques\\nsuch as regex filtering and removing near-duplicates ensure\\nthe data’s uniqueness and relevance. In addition, familiarizing\\nwith various prompt templates like Alpaca [196] is essential\\nfor structuring this data effectively, ensuring that the LLM\\ncan be finely tuned to respond to the nuanced landscape of\\ncybersecurity challenges efficiently.\\n3) Pre-training models: Pre-training a model for cyberse-\\ncurity tasks involves a complex and resource-intensive pro-\\ncess to prepare a language model to understand and predict\\ncybersecurity-specific content. This requires a massive dataset\\ncomprising billions or trillions of tokens, which undergo\\nrigorous processes like filtering, tokenizing, and aligning with\\na pre-defined vocabulary to ensure relevance and accuracy.\\nTechniques such as causal language modeling, distinct from\\nmasked language modeling, are employed, where the loss\\nfunctions and training methodologies, such as those used in\\nMegatron-LM [197] or gpt-neox [198], are optimized for han-\\ndling sequential data predictively. Understanding the scaling\\nlaws is crucial, as these laws help predict how increases in\\nmodel size, dataset breadth, and computational power can\\n39', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8856d441-e2d3-4630-8471-1841bf32ba61', embedding=None, metadata={'page_label': '40', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Fig. 8: Parameter Efficient Fine-Tuning (PEFT) provides an efficient approach by minimizing the number of parameters needed\\nfor fine-tuning and reducing memory consumption comparable to that of traditional fine-tuning.\\nproportionally enhance model performance [199]. While in-\\ndepth knowledge of High-Performance Computing (HPC) isn’t\\nnecessary for using pre-trained models, it becomes essential\\nwhen building a large-scale language model for cyber security\\nfrom scratch, requiring an understanding of hardware capabil-\\nities and managing distributed workloads effectively.\\nMost pre-training LLM models are trained using smdis-\\ntributed libraries, proposed by AWS SageMaker, which of-\\nfer robust solutions for distributed training machine learning\\nmodels, enhancing efficiency on large-scale deployments. The\\nsmdistributed.dataparallel library supports data parallelism,\\noptimizing GPU usage by partitioning the training data across\\nmultiple GPUs, thus speeding up the learning process and\\nminimizing communication overhead. On the other hand,\\nsmdistributed.modelparallel is tailored for model parallelism,\\nallowing large models to be split across multiple GPUs when\\na single model cannot fit into the memory of one GPU. These\\ntools seamlessly integrate with frameworks like TensorFlow,\\nPyTorch, and MXNet, simplifying the implementation of com-\\nplex distributed training tasks.\\n4) Supervised Fine-Tuning: Supervised fine-tuning (SFT)\\nof pre-trained Large Language Models for cybersecurity ap-\\nplications enables these models to move beyond basic next-\\ntoken prediction tasks, transforming them into specialized\\ntools tailored to specific cybersecurity needs. This fine-tuning\\nprocess allows for incorporating proprietary or novel datasets\\nthat have not been previously exposed to models like Fal-\\ncon 180b, providing a significant edge in addressing unique\\nsecurity challenges. Figure 8 outlines a comprehensive three-\\nstep process for training a large language model specialized\\nin cybersecurity, beginning with unsupervised pre-training on\\na vast corpus of cybersecurity-related texts, including diverse\\ndata such as malware, network security, and dark web content.\\nFollowing this, the model undergoes traditional fine-tuning\\nusing a smaller, targeted dataset to refine its capabilities for\\nspecific cybersecurity tasks. However, the Parameter-Efficient\\nFine-Tuning (PEFT) [200] involves freezing the original model\\nweights and fine-tuning a small set of new parameters, enhanc-\\ning the model’s adaptability and efficiency while minimizing\\nthe risk of overfitting, thus preparing the LLM to tackleadvanced cybersecurity challenges efficiently.\\nTechniques such as LoRA (Low-rank Adapters) [201] offer\\na parameter-efficient approach by adjusting only a subset of the\\nmodel’s parameters, thus optimizing computational resources\\nwhile maintaining performance. More advanced methods like\\nQLoRA [202] enhance this by quantizing the model’s weights\\nand managing memory more efficiently, making executing\\nthese operations even on limited platforms like Google Colab\\nwith only one GPU A100. In addition, tools like Axolotl and\\nDeepSpeed [203], [204] facilitate the deployment of these fine-\\ntuned models across various hardware setups, ensuring that\\nthe enhanced models can be scaled efficiently for real-world\\ncybersecurity tasks, ranging from intrusion detection to real-\\ntime threat analysis. This strategic fine-tuning enhances model\\nspecificity and significantly boosts their utility in practical\\ncybersecurity applications.\\n5) Cyber Security Evaluation: To evaluate the code gen-\\neration models, Hugging Face uses the following 7 code\\ngeneration Python tasks: DS-1000, MBPP, MBPP+, APPS, In-\\nstructHumanEval, HumanEval+, and HumanEval [211]–[217].\\nIn cybersecurity, evaluating large language models demands\\na specialized framework considering such applications’ unique\\nsecurity and accuracy needs. When setting up evaluation met-\\nrics for cybersecurity-focused LLMs, test cases should closely\\nmimic potential security scenarios to assess how well the\\nmodel detects, understands, and responds to cyber threats. This\\ninvolves configuring the LLM with tailored inputs, expected\\noutputs, and security-specific contextual data [26]. For in-\\nstance, IBM’s D2A dataset [161] and Microsoft’s dataset [218]\\naids in evaluating AI models’ capability to identify software\\nvulnerabilities using specific metrics such as accuracy.\\nTable XVI compares benchmarks for evaluating LLMs in\\ncybersecurity knowledge. CyberMetric [89] is a benchmark\\ndataset designed explicitly for evaluating large language mod-\\nels in knowledge cybersecurity. It consists of 10,000 questions\\nderived from various authoritative sources within the field. The\\ndataset is used to measure the knowledge of LLMs across\\na spectrum of cybersecurity topics, facilitating direct com-\\nparisons between human expertise and machine capabilities.\\nThis unique dataset aids in understanding the strengths and\\n40', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='758c363c-207d-44e2-b7c4-d4869449b839', embedding=None, metadata={'page_label': '41', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE XVI: Comparison of Benchmarks for Evaluating LLMs in Cybersecurity Knowledge\\nBenchmark Source Year Description Key Features and Metrics\\nCyberSecEval 1 Meta\\n[205]2023 A benchmark tests LLMs across two critical security do-\\nmains—generating insecure code and compliance with re-\\nquests to assist in cyberattacks.It measures the frequency and\\nconditions LLMs propose insecure\\ncode solutions under.\\nSecQA Liu et al.\\n[206]2023 A dataset of multiple-choice questions designed to evaluate\\nthe performance of LLMs in computer security. Features two\\nversions of varying complexity and tests LLMs in both 0-shot\\nand 5-shot learning settings.Evaluates understanding and appli-\\ncation of security principles.\\nCyberMetric Tihanyi et\\nal. [89]2024 A dataset designed for evaluating LLMs in cybersecurity\\nknowledge, consisting of 10,000 questions from various au-\\nthoritative sources. Used to measure the spectrum of cyber-\\nsecurity topics covered by LLMs.Direct comparison between human\\nexpertise and LLMs.\\nCyberSecEval 2 Meta\\n[207]2024 Focuses on quantifying security risks associated with LLMs,\\nsuch as prompt injection and code interpreter abuse. It high-\\nlights challenges in mitigating attack risks and introduces the\\nFalse Refusal Rate (FRR) metric.Testing areas: prompt injection,\\ncode interpreter abuse; Metric:\\nFRR.\\nWMDP-Cyber Li et al.\\n[208]2024 Consists of 3,668 multiple-choice questions designed to mea-\\nsure LLMs’ knowledge in biosecurity, cybersecurity, and\\nchemical security. Excludes sensitive and export-controlled\\ninformation.Covers biosecurity, cybersecurity,\\nand chemical security.\\nLLM4Vuln Sun et al.\\n[209]2024 A unified evaluation framework for assessing the vulnerability\\nreasoning capabilities of LLMs, using 75 verified high-risk\\nsmart contract vulnerabilities in 4,950 scenarios across three\\nLLMs.Focuses on vulnerability reasoning\\nin LLMs.\\nCyberBench Liu et al.\\n[210]2024 A domain-specific, multi-task benchmark for assessing LLM\\nperformance in cybersecurity tasks.Includes diverse tasks such as vul-\\nnerability detection, threat analysis,\\nand incident response.\\nlimitations of LLMs in cybersecurity, providing a foundation\\nfor further development and specialized training of these\\nmodels in this critical area.\\nSimilar to the CyberMetric benchmark, Meta proposed the\\nCyberSecEval 2 benchmark [207] to quantify security risks\\nassociated with LLMs such as GPT-4 and Meta Llama 3 70B-\\nInstruct. They highlight new testing areas, notably prompt\\ninjection and code interpreter abuse, revealing that mitigating\\nattack risks in LLMs remains challenging, with significant\\nrates of successful prompt injections. The study also explores\\nthe safety-utility tradeoff, proposing the False Refusal Rate\\n(FRR) to measure how conditioning LLMs to avoid unsafe\\nprompts might reduce their overall utility by also rejecting\\nbenign requests. Additionally, the research assesses LLMs’\\ncapabilities in automating core cybersecurity tasks, suggesting\\nthat models with coding abilities perform better in exploit\\ngeneration tasks. The benchmark code is open source to\\nfacilitate further research2.\\nLiu et al. introduced SecQA [206], a novel dataset designed\\nto evaluate the performance of LLMs in computer security.\\nThe dataset, generated by GPT-4, consists of multiple-choice\\nquestions to assess LLMs’ understanding and application of\\nsecurity principles. SecQA features two versions with varying\\ncomplexity to challenge LLMs across different difficulty lev-\\nels. The authors comprehensively evaluated prominent LLMs,\\nincluding GPT-3.5-Turbo, GPT-4, Llama-2, Vicuna, Mistral,\\nand Zephyr, in both 0-shot and 5-shot learning settings.\\nThe findings from the SecQA v1 and v2 datasets reveal\\ndiverse capabilities and limitations of these models in han-\\ndling security-related content. Li et al. [208] introduced the\\nWeapons of Mass Destruction Proxy (WMDP) benchmark.\\nThis publicly available dataset consists of 3,668 multiple-\\n2https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarkschoice questions designed to measure LLMs’ knowledge in\\nbiosecurity, cybersecurity, and chemical security, ensuring the\\nexclusion of sensitive and export-controlled information. Sun\\net al. [209] introduced LLM4Vuln, a unified evaluation frame-\\nwork designed to precisely assess the vulnerability reasoning\\ncapabilities of LLMs independent of their other functions such\\nas information seeking, knowledge adoption, and structured\\noutput. This framework aims to determine how enhancing\\nthese separate capabilities could boost LLMs’ effectiveness in\\nidentifying vulnerabilities. To test the efficacy of LLM4Vuln,\\ncontrolled experiments were conducted with 75 verified high-\\nrisk smart contract vulnerabilities sourced from Code4rena\\naudits conducted between August and November 2023. These\\nvulnerabilities were tested in 4,950 scenarios across three\\nLLMs: GPT-4, Mixtral, and Code Llama.\\n6) Advanced LLM techniques (RAG and RLHF): Advanced\\ntechniques like Retrieval Augmentation Generation (RAG)\\ncan significantly enhance Language Model performance by\\nenabling the model to access external databases for additional\\ncontext and information, making it highly effective in special-\\nized fields such as cybersecurity. In cybersecurity applications,\\nRAG can dynamically retrieve up-to-date information from\\nwell-known databases such as CVE (Common Vulnerabilities\\nand Exposures), CWE (Common Weakness Enumeration), and\\nthe NIST (National Institute of Standards and Technology)\\ndatabase [227]. This capability allows the model to offer\\ncurrent and specific advice regarding vulnerabilities, threat\\nintelligence, and compliance standards. Integrating real-time\\ndata from these authoritative sources into the response gener-\\nation process allows RAG to empower Language Models to\\ndeliver precise and contextually relevant cybersecurity insights\\nwithout extensive retraining, thus enhancing decision-making\\nin critical security operations [228].\\nReinforcement Learning from Human Feedback (RLHF) is\\n41', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d625b769-a137-42cd-ac20-c00ffb641241', embedding=None, metadata={'page_label': '42', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE XVII: Optimization Strategies for Large Language Models in Cybersecurity\\nOptimization\\nStrategyDescription Key Benefits Cybersecurity Use Case Scenarios\\nAdvanced Attention\\nMechanismsImplements techniques like Flash Attention [219]\\nto optimize self-attention layers, reducing compu-\\ntation times, particularly effective for long input\\nsequences.Speeds up processing saves com-\\npute resources.Efficient processing of long log files and network\\ntraffic data for anomaly detection.\\nBitsnbytes Introduces k-bit quantization (notably 8-bit) us-\\ning block-wise methods to maintain performance\\nwhile halving memory usage.Halves memory usage without\\nloss in performance.Efficient real-time malware analysis and intrusion\\ndetection on edge devices.\\nGPTQ [220] A novel quantization method for GPT models\\nthat reduces bit width to 3 or 4 bits per weight,\\nenabling the operation of large models on single\\nGPUs with minimal accuracy loss.Compresses model size, mini-\\nmizes accuracy loss.Deploying large-scale threat prediction models on\\nconsumer-grade hardware.\\nGGUF Quantization Optimized for quick model loading and saving,\\nmaking LLM inference more efficient. Supported\\nby Hugging Face Hub.Enhances efficiency of model de-\\nployment.Rapid deployment of updated models to respond\\nto emerging threats and vulnerabilities.\\nQLoRA [202] Enables training using memory-saving techniques\\nwith a small set of trainable low-rank adaptation\\nweights.Preserves performance with re-\\nduced memory.Training complex cybersecurity models on sys-\\ntems with limited memory resources.\\nLower-precision\\ndata FormatsUses formats like bfloat16 instead of float32 for\\ntraining and inference to optimize resource usage\\nwithout compromising performance accuracy.Reduces computational\\noverhead.Enhancing the speed and efficiency of continuous\\ncybersecurity monitoring systems.\\nFSDP-QLoRA Combines Fully Sharded Data Parallelism (FSDP)\\nwith 4-bit quantization and LoRA to shard model\\nparameters, optimizer states, and gradients across\\nGPUs.Scales up model training across\\nmultiple GPUs.Enabling the collaborative training of security\\nmodels across different organizations without re-\\nquiring top-tier hardware.\\nHalf-Quadratic\\nQuantization\\n(HQQ) [221]A model quantization technique that enables the\\nquantization of large models rapidly and accu-\\nrately without the need for calibration data.Works efficiently with\\nCUDA/Triton kernels and\\naims for seamless integration\\nwithtorch.compile .HQQ can be employed in cybersecurity to pro-\\ntect models by reducing the precision of model\\nweights, making it harder for attackers to reverse\\nengineer or tamper with the models..\\nMulti-token Predic-\\ntion [222]A new training approach for large language mod-\\nels where models predict multiple future tokens\\nsimultaneously rather than the next token only.Models trained with 4-token pre-\\ndictions can achieve up to 3x\\nfaster inference speeds, even with\\nlarge batch sizes.Multi-token prediction can enhance the modeling\\nof sophisticated cyber attack patterns.\\nTrust Region\\nPolicy Optimization\\n(TRPO) [223]An advanced policy gradient method in reinforce-\\nment learning that addresses the inefficiencies of\\nstandard policy gradient methods.TRPO enhances training stability\\nby using trust regions to prevent\\noverly large updates that could\\ndestabilize the policy.In environments with dynamic and evolving\\nthreats, TRPO can help maintain a stable and\\neffective response mechanism, adjusting policies\\nincrementally to handle new types of malware.\\nProximal Policy\\nOptimization (PPO)\\n[224]A reinforcement learning technique designed to\\nimprove training stability by cautiously updating\\npolicies.Prevents ”falling off the cliff”\\nscenarios where a policy update\\nis too large could irreversibly\\ndamage the policy’s effective-\\nness.By limiting the extent of policy updates, PPO\\nhelps maintain a steady adaptation to evolving cy-\\nbersecurity threats, reducing the risk of overfitting\\nto specific attack patterns.\\nDirect Preference\\nOptimization\\n(DPO) [225]A fine-tuning methodology for foundation mod-\\nels optimize policies directly using a Kull-\\nback–Leibler divergence-constrained framework,\\nremoving the need for a separate reward model.Requires significantly less data\\nand compute resources than pre-\\nvious methods like PPO.Reduces the computational and data demands of\\ncontinuously training cybersecurity models, allow-\\ning for more scalable solutions.\\nOdds Ratio Pref-\\nerence Optimization\\n(ORPO) [226]An algorithm designed for supervised fine-tuning\\n(SFT) of language models that optimizes prefer-\\nence alignment without the need for a separate\\nreference model.Eliminates the need for an\\nadditional preference alignment\\nphase, simplifying the fine-\\ntuning process.Enables dynamic adaptation of security models\\nto new and evolving cyber threats by optimizing\\npreference alignment efficiently.\\nan advanced method to enhance LLMs tailored for cybersecu-\\nrity applications, focusing on aligning the model’s responses\\nwith expert expectations in the security domain. This involves\\nutilizing specialized preference datasets, which contain re-\\nsponses ranked by cybersecurity professionals, presenting a\\nmore challenging production process than typical instructional\\ndatasets. Techniques like Proximal Policy Optimization (PPO)\\nleverage a reward model to evaluate how well text outputs\\nalign with security expert rankings, refining the model’s\\ntraining through adjustments based on KL divergence [224].\\nDirect Preference Optimization (DPO) further optimizes this\\nby framing it as a classification challenge, using a stable\\nreference model that avoids the complexities of training reward\\nmodels and requires minimal hyperparameter adjustments\\n[229]. These methods are crucial for reducing biases, fine-\\ntuning threat detection accuracy, and enhancing the overall\\neffectiveness of cybersecurity-focused LLMs.\\nIn practical cybersecurity applications, the integration of\\nRAG can be facilitated by orchestrators like LangChain,\\nLlamaIndex, and FastRAG, which connect Language Modelsto relevant tools, databases, and other resources. These orches-\\ntrators ensure efficient information flow, enabling Language\\nModels to seamlessly access and incorporate essential cyberse-\\ncurity information [230]. Advanced techniques such as multi-\\nquery retrievers and HyDE are used to optimize the retrieval of\\nrelevant cybersecurity documents and adapt user queries into\\nmore effective forms for document retrieval. Furthermore, in-\\ncorporating a memory system that recalls previous interactions\\nallows these models to provide consistent and context-aware\\nresponses over time. This amalgamation of advanced retrieval\\nmechanisms and memory enhancement through RAG signif-\\nicantly boosts the efficacy of Language Models in handling\\ncomplex and evolving cybersecurity challenges, making them\\ninvaluable tools for tracking vulnerabilities, managing risks,\\nand adhering to industry standards in the cybersecurity domain\\n[231].\\n7) LLM deployments: Deploying LLMs offers a range of\\napproaches tailored to the scale and specific needs of different\\napplications. At one end of the spectrum, local deployment of-\\nfers enhanced privacy and control, utilizing platforms like LM\\n42', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9fb21e7c-e983-4553-9d7b-5e4dd15deaef', embedding=None, metadata={'page_label': '43', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Studio andOllama to power apps directly on users’ machines,\\nthus capitalizing on the open-source nature of some LLMs.\\nFor more dynamic or temporary setups, frameworks such as\\nGradio andStreamlit allow developers to prototype and share\\ndemos quickly, with hosting options like Hugging Face Spaces\\nproviding an accessible path to broader distribution. On the\\nindustrial scale, deploying LLMs can require robust server\\nsetups, utilizing cloud services or on-premises infrastructure\\nthat might leverage specialized frameworks for peak perfor-\\nmance and efficiency. Meanwhile, edge deployment strategies\\nbring LLM capabilities to devices with limited resources,\\nusing advanced, lightweight frameworks to integrate smart\\ncapabilities directly into mobile and web platforms, ensuring\\nresponsiveness and accessibility across a broad spectrum of\\nuser environments [232], [233].\\nCurrently, LLMs can be deployed on Phones. Microsoft\\n[138] propose phi-3-mini. This highly efficient 3.8 billion\\nparameter language model delivers robust performance on par\\nwith much larger models such as Mixtral 8x7B and GPT-\\n3.5, achieving impressive scores like 69% on the MMLU\\nand 8.38 on MT-bench. Remarkably, the phi-3-mini’s compact\\nsize allows for deployment on mobile devices, expanding\\nits accessibility and utility. This performance breakthrough\\nis primarily attributed to an innovative approach in training\\ndata selection—a significantly enhanced version of the dataset\\nused for phi-2, which integrates heavily filtered web data and\\nsynthetic data tailored for relevance and diversity. It has been\\nfurther aligned to ensure the model’s practicality in real-world\\napplications for enhanced robustness, safety, and optimization\\nfor chat formats. In addition, the research extends into larger\\nmodels, phi-3-small and phi-3-medium, which are trained on\\n4.8 trillion tokens with 7 billion and 14 billion parameters,\\nrespectively. These models retain the foundational strengths\\nof phi-3-mini and exhibit superior performance, scoring up to\\n78% on MMLU and 8.9 on MT-bench, illustrating significant\\nenhancements in language understanding capabilities with\\nscaling. In addition, AirLLM3enhances memory management\\nfor inference, enabling large language models, such as those\\nwith 70 billion parameters (e.g., Llama3 70B), to operate on a\\nsingle 4GB GPU card. This can be achieved without requiring\\nquantization, distillation, pruning, or any other form of model\\ncompression that could diminish performance.\\n8) Securing LLMs: Securing LLMs is essential due to their\\ninherent susceptibility to traditional software vulnerabilities\\nand unique risks stemming from their design and operational\\nmethods. Specifically, LLMs are prone to prompt hacking,\\nwhere techniques such as prompt injection can be used to\\nmanipulate model responses, prompt leaking that risks expo-\\nsure of training data, and jailbreaking intended to circumvent\\nbuilt-in safety mechanisms. These specific threats necessitate\\nimplementing comprehensive security measures that directly\\naddress the unique challenges LLMs pose. Additionally, in-\\nserting backdoors during training, either by poisoning the data\\nor embedding secret triggers, can significantly alter a model’s\\nbehavior during inference, posing severe risks to data integrity\\nand model reliability.\\n3https://pypi.org/project/airllm/As discussed in Section VIII, to mitigate these threats\\neffectively, organizations must adopt rigorous defensive strate-\\ngies as recommended by the OWASP LLM security checklist\\n4. This includes testing LLM applications against known\\nvulnerabilities using methods like red teaming and specific\\ntools such as garak [234] to identify and address security\\nflaws. In addition, deploying continuous monitoring systems\\nlike langfuse5in production environments helps detect and\\nrectify anomalous behaviors or potential breaches in real-\\ntime. The OWASP checklist also emphasizes the importance\\nof governance frameworks that ensure data used in training is\\nethically sourced and handled, maintaining transparency about\\ndata sources and model training methodologies. This struc-\\ntured approach to security and governance ensures that LLMs\\nare used responsibly and remain secure from conventional\\ncyber threats and those unique to their operational nature.\\n9) Optimizing LLMs: Optimizing LLMs for production\\nencompasses several crucial techniques to enhance speed,\\nreduce memory requirements, and maintain output quality.\\nOne pivotal strategy is model quantization, which significantly\\nreduces the precision of model weights—often to 4-bit or 8-\\nbit—thereby decreasing the GPU memory requirements. Table\\nXVII presents the optimization strategies for LLMs that can be\\nadopted for Cybersecurity use cases. For instance, quantizing\\na model to 4-bit can bring down the VRAM requirement\\nfrom 32 GB to just over 9 GB, allowing these models to\\nrun efficiently on consumer-level hardware like the RTX\\n3090 GPU. Therefore, advanced attention mechanisms such\\nas Flash Attention reduce computation times by optimizing\\nself-attention layers, which are integral to transformers [219].\\nThis optimization is especially beneficial for handling long\\ninput sequences, where traditional self-attention mechanisms\\ncould become prohibitively expensive regarding memory and\\nprocessing power [235], [236].\\nThe quantization methods include Bitsnbytes, 4-bit GPTQ,\\n2-bit GPTQ, and GGUF quantization. Bitsnbytes introduces a\\nk-bit quantization approach that significantly reduces mem-\\nory consumption while maintaining performance [220]. It\\nemploys an 8-bit optimization using block-wise quantization\\nto achieve 32-bit performance at a lower memory cost and\\nuses LLM.Int() for 8-bit quantization during inference, halving\\nthe required memory without performance loss. Furthermore,\\nQLoRA [202], or 4-bit quantization, enables the training\\nof LLMs using memory-saving techniques that include a\\nsmall set of trainable low-rank adaptation weights, allowing\\nfor performance preservation. In parallel, GPTQ is a novel\\nquantization method for GPT models, facilitating the reduction\\nof bit width to 3 or 4 bits per weight, enabling the operation\\nof models as large as 175 billion parameters on a single GPU\\nwith minimal accuracy loss. This method provides substantial\\ncompression and speed advantages, making high-performance\\nLLMs more accessible and cost-effective. Additionally, the\\nGGUF format, supported by Hugging Face Hub and optimized\\nfor quick model loading and saving, enhances the efficiency\\nof LLM inference.\\n4https://owasp.org/www-project-top-10-for-large-language-model-\\napplications/\\n5https://github.com/langfuse/langfuse\\n43', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4e8e19d-696c-4def-b3ed-18d4c0d2385c', embedding=None, metadata={'page_label': '44', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Another effective optimization is incorporating lower-\\nprecision data formats such as bfloat16 for training and\\ninference. This approach aligns with the training precision\\nand avoids the computational overhead associated with float32\\nprecision, optimizing resource usage without compromising\\nperformance accuracy. The potential VRAM requirements for\\ndifferent models using bfloat16 are substantial. For example,\\nGPT-3 might require up to 350 GB. In comparison, smaller\\nmodels like Llama-2-70b and Falcon-40b require 140 GB and\\n80 GB, respectively, illustrating the scale of resources needed\\neven with efficient data formats6.\\nRecently, FSDP-QLoRA7, a new technique combining data\\nparallelism, 4-bit quantization, and LoRA, was introduced by\\nAnswer.AI in collaboration with bitsandbytes. Utilizing Fully\\nSharded Data Parallelism (FSDP) to shard model parameters,\\noptimizer states, and gradients across GPUs, this approach\\nenables the training of LLMs up to 70 billion parameters\\non dual 24GB GPU systems. FSDP-QLoRA represents a\\nsignificant step forward in making the training of large-scale\\nLLMs.\\nCollectively, these techniques make it feasible to deploy\\npowerful LLMs on a wider range of hardware and enhance\\ntheir scalability and practicality in diverse applications, ensur-\\ning they can deliver high performance even under hardware\\nconstraints.\\nX. C ONCLUSION\\nIn this paper, we presented a comprehensive and in-depth\\nreview of the future of cybersecurity through the lens of\\nGenerative AI and Large Language Models (LLMs). Our\\nexploration covered a wide range of LLM applications in\\ncybersecurity, including hardware design security, intrusion\\ndetection, software engineering, design verification, cyber\\nthreat intelligence, malware detection, and phishing and spam\\ndetection, illustrating the broad potential of LLMs across\\nvarious domains.\\nWe provided a detailed examination of the evolution and\\ncurrent state of LLMs, highlighting advancements in 35\\nspecific models, such as GPT-4, GPT-3.5, BERT, Falcon,\\nand LLaMA. Our analysis included an in-depth look at the\\nvulnerabilities associated with LLMs, such as prompt injec-\\ntion, insecure output handling, training and inference data\\npoisoning, DDoS attacks, and adversarial natural language\\ninstructions. We discussed mitigation strategies to protect these\\nmodels, offering a thorough understanding of potential attack\\nscenarios and prevention techniques.\\nOur evaluation of 40 LLM models in terms of cybersecurity\\nknowledge and hardware security demonstrated their vary-\\ning strengths and weaknesses. We also conducted a detailed\\nassessment of cybersecurity datasets used for LLM training\\nand testing, from data creation to usage, identifying gaps and\\nopportunities for future research.\\nWe addressed the challenges and limitations of employing\\nLLMs in cybersecurity settings, including the difficulty of\\n6https://huggingface.co/docs/transformers/main/en/llm tutorial optimization\\n7https://huggingface.co/docs/bitsandbytes/main/en/fsdp qloradefending against adversarial attacks and ensuring model ro-\\nbustness. Additionally, we explored advanced techniques like\\nHalf-Quadratic Quantization (HQQ), Reinforcement Learning\\nwith Human Feedback (RLHF), Direct Preference Optimiza-\\ntion (DPO), Odds Ratio Preference Optimization (ORPO),\\nGPT-Generated Unified Format (GGUF), Quantized Low-\\nRank Adapters (QLoRA), and Retrieval-Augmented Genera-\\ntion (RAG) to enhance real-time cybersecurity defenses and\\nimprove the sophistication of LLM applications in threat\\ndetection and response.\\nOur findings underscore the significant potential of LLMs\\nin transforming cybersecurity practices. By integrating LLMs\\ninto future cybersecurity frameworks, we can leverage their\\ncapabilities to develop more robust and sophisticated defenses\\nagainst evolving cyber threats. The strategic direction outlined\\nin this paper aims to guide future research and deployment,\\nemphasizing the importance of innovation and resilience in\\nsafeguarding digital infrastructures.\\nREFERENCES\\n[1] R. Pascanu, C. Gulcehre, K. Cho, and Y . Bengio, “How to construct\\ndeep recurrent neural networks,” arXiv preprint arXiv:1312.6026 , 2013.\\n[2] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\\ncomputation , vol. 9, no. 8, pp. 1735–1780, 1997.\\n[3] R. Dey and F. M. Salem, “Gate-variants of gated recurrent unit (gru)\\nneural networks,” in 2017 IEEE 60th international midwest symposium\\non circuits and systems (MWSCAS) . IEEE, 2017, pp. 1597–1600.\\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\\nAdvances in neural information processing systems , vol. 30, 2017.\\n[5] B. Yan, K. Li, M. Xu, Y . Dong, Y . Zhang, Z. Ren, and X. Cheng, “On\\nprotecting the data privacy of large language models (llms): A survey,”\\narXiv preprint arXiv:2403.05156 , 2024.\\n[6] D. Myers, R. Mohawesh, V . I. Chellaboina, A. L. Sathvik, P. Venkatesh,\\nY .-H. Ho, H. Henshaw, M. Alhawawreh, D. Berdik, and Y . Jararweh,\\n“Foundation and large language models: fundamentals, challenges,\\nopportunities, and social impacts,” Cluster Computing , vol. 27, no. 1,\\npp. 1–26, 2024.\\n[7] S. Tonmoy, S. Zaman, V . Jain, A. Rani, V . Rawte, A. Chadha, and\\nA. Das, “A comprehensive survey of hallucination mitigation tech-\\nniques in large language models,” arXiv preprint arXiv:2401.01313 ,\\n2024.\\n[8] I. H. Sarker, H. Janicke, M. A. Ferrag, and A. Abuadbba, “Multi-\\naspect rule-based ai: Methods, taxonomy, challenges and directions\\ntoward automation, intelligence and transparent cybersecurity modeling\\nfor critical infrastructures,” Internet of Things , p. 101110, 2024.\\n[9] Y . Yao, J. Duan, K. Xu, Y . Cai, Z. Sun, and Y . Zhang, “A survey on\\nlarge language model (llm) security and privacy: The good, the bad,\\nand the ugly,” High-Confidence Computing , p. 100211, 2024.\\n[10] Y . Yan, Y . Zhang, and K. Huang, “Depending on yourself when\\nyou should: Mentoring llm with rl agents to become the master in\\ncybersecurity games,” arXiv preprint arXiv:2403.17674 , 2024.\\n[11] M. Sladi ´c, V . Valeros, C. Catania, and S. Garcia, “Llm in the shell:\\nGenerative honeypots,” arXiv preprint arXiv:2309.00155 , 2023.\\n[12] W. Tann, Y . Liu, J. H. Sim, C. M. Seah, and E.-C. Chang, “Using\\nlarge language models for cybersecurity capture-the-flag challenges and\\ncertification questions,” arXiv preprint arXiv:2308.10443 , 2023.\\n[13] O. G. Lira, A. Marroquin, and M. A. To, “Harnessing the advanced\\ncapabilities of llm for adaptive intrusion detection systems,” in In-\\nternational Conference on Advanced Information Networking and\\nApplications . Springer, 2024, pp. 453–464.\\n[14] C. Ebert and M. Beck, “Artificial intelligence for cybersecurity,” IEEE\\nSoftware , vol. 40, no. 6, pp. 27–34, 2023.\\n[15] J. Wang, Y . Huang, C. Chen, Z. Liu, S. Wang, and Q. Wang, “Software\\ntesting with large language models: Survey, landscape, and vision,”\\nIEEE Transactions on Software Engineering , 2024.\\n[16] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojo-\\ncaru, M. Debbah, ´E. Goffinet, D. Hesslow, J. Launay, Q. Malartic\\net al. , “The falcon series of open language models,” arXiv preprint\\narXiv:2311.16867 , 2023.\\n44', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eeff3fb7-eb09-4b20-9482-3bd86cf5f48c', embedding=None, metadata={'page_label': '45', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[17] H. Zhou, C. Hu, Y . Yuan, Y . Cui, Y . Jin, C. Chen, H. Wu, D. Yuan,\\nL. Jiang, D. Wu, X. Liu, C. Zhang, X. Wang, and J. Liu, “Large\\nlanguage model (llm) for telecommunications: A comprehensive survey\\non principles, key techniques, and opportunities,” 2024.\\n[18] H. Lai and M. Nissim, “A survey on automatic generation of figurative\\nlanguage: From rule-based systems to large language models,” ACM\\nComputing Surveys , 2024.\\n[19] M. A. Ferrag, M. Ndhlovu, N. Tihanyi, L. C. Cordeiro, M. Deb-\\nbah, T. Lestable, and N. S. Thandi, “Revolutionizing cyber threat\\ndetection with large language models: A privacy-preserving bert-based\\nlightweight model for iot/iiot devices,” IEEE Access , 2024.\\n[20] Z. Liu, “A review of advancements and applications of pre-trained lan-\\nguage models in cybersecurity,” in 2024 12th International Symposium\\non Digital Forensics and Security (ISDFS) , 2024, pp. 1–10.\\n[21] S. Jamal, H. Wimmer, and I. H. Sarker, “An improved transformer-\\nbased model for detecting phishing, spam and ham emails: A large\\nlanguage model approach,” Security and Privacy , p. e402, 2024.\\n[Online]. Available: https://doi.org/10.1002/spy2.402\\n[22] F. R. Alzaabi and A. Mehmood, “A review of recent advances,\\nchallenges, and opportunities in malicious insider threat detection using\\nmachine learning methods,” IEEE Access , vol. 12, pp. 30 907–30 927,\\n2024.\\n[23] R. Fang, R. Bindu, A. Gupta, and D. Kang, “Llm agents\\ncan autonomously exploit one-day vulnerabilities,” arXiv preprint\\narXiv:2404.08144 , 2024.\\n[24] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min,\\nB. Zhang, J. Zhang, Z. Dong et al. , “A survey of large language\\nmodels,” arXiv preprint arXiv:2303.18223 , 2023.\\n[25] M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib,\\nM. M. J. Mim, J. Ahmad, M. E. Ali, and S. Azam, “A review on large\\nlanguage models: Architectures, applications, taxonomies, open issues\\nand challenges,” IEEE Access , vol. 12, pp. 26 839–26 874, 2024.\\n[26] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\\nC. Wang, Y . Wang et al. , “A survey on evaluation of large language\\nmodels,” ACM Transactions on Intelligent Systems and Technology ,\\n2023.\\n[27] D. Saha, S. Tarek, K. Yahyaei, S. K. Saha, J. Zhou, M. Tehranipoor,\\nand F. Farahmandi, “Llm for soc security: A paradigm shift,” arXiv\\npreprint arXiv:2310.06046 , 2023.\\n[28] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\\nE. Agirre, I. Heintz, and D. Roth, “Recent advances in natural language\\nprocessing via large pre-trained language models: A survey,” ACM\\nComputing Surveys , vol. 56, no. 2, pp. 1–40, 2023.\\n[29] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\\nT. Zhang, F. Wu et al. , “Instruction tuning for large language models:\\nA survey,” arXiv preprint arXiv:2308.10792 , 2023.\\n[30] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo,\\nand J. M. Zhang, “Large language models for software engineering:\\nSurvey and open problems,” arXiv preprint arXiv:2310.03533 , 2023.\\n[31] J. Wu, W. Gan, Z. Chen, S. Wan, and P. S. Yu, “Multimodal large\\nlanguage models: A survey,” arXiv preprint arXiv:2311.13165 , 2023.\\n[32] Y . Liu, Y . Yao, J.-F. Ton, X. Zhang, R. G. H. Cheng, Y . Klochkov,\\nM. F. Taufiq, and H. Li, “Trustworthy llms: a survey and guideline\\nfor evaluating large language models’ alignment,” arXiv preprint\\narXiv:2308.05374 , 2023.\\n[33] L. Hu, Z. Liu, Z. Zhao, L. Hou, L. Nie, and J. Li, “A survey of\\nknowledge enhanced pre-trained language models,” IEEE Transactions\\non Knowledge and Data Engineering , 2023.\\n[34] H. Zhang, H. Song, S. Li, M. Zhou, and D. Song, “A survey of con-\\ntrollable text generation using transformer-based pre-trained language\\nmodels,” ACM Computing Surveys , vol. 56, no. 3, pp. 1–37, 2023.\\n[35] Z. He, Z. Li, and S. Yang, “Large language models for blockchain secu-\\nrity: A systematic literature review,” arXiv preprint arXiv:2403.14280 ,\\n2024.\\n[36] Y . Yigit, M. A. Ferrag, I. H. Sarker, L. A. Maglaras, C. Chrysoulas,\\nN. Moradpoor, and H. Janicke, “Critical infrastructure protec-\\ntion: Generative ai, challenges, and opportunities,” arXiv preprint\\narXiv:2405.04874 , 2024.\\n[37] J. Wang, Y . Huang, C. Chen, Z. Liu, S. Wang, and Q. Wang, “Software\\ntesting with large language models: Survey, landscape, and vision,”\\nIEEE Transactions on Software Engineering , pp. 1–27, 2024.\\n[38] H. Xu, S. Wang, N. Li, Y . Zhao, K. Chen, K. Wang, Y . Liu, T. Yu,\\nand H. Wang, “Large language models for cyber security: A systematic\\nliterature review,” arXiv preprint arXiv:2405.04760 , 2024.\\n[39] Z. Han, C. Gao, J. Liu, S. Q. Zhang et al. , “Parameter-efficient fine-\\ntuning for large models: A comprehensive survey,” arXiv preprint\\narXiv:2403.14608 , 2024.[40] J. Zhang, H. Bu, H. Wen, Y . Chen, L. Li, and H. Zhu, “When llms\\nmeet cybersecurity: A systematic literature review,” arXiv preprint\\narXiv:2405.03644 , 2024.\\n[41] C. Cui, Y . Ma, X. Cao, W. Ye, Y . Zhou, K. Liang, J. Chen, J. Lu,\\nZ. Yang, K.-D. Liao et al. , “A survey on multimodal large language\\nmodels for autonomous driving,” in Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vision , 2024, pp. 958–\\n979.\\n[42] G. Bai, Z. Chai, C. Ling, S. Wang, J. Lu, N. Zhang, T. Shi,\\nZ. Yu, M. Zhu, Y . Zhang et al. , “Beyond efficiency: A systematic\\nsurvey of resource-efficient large language models,” arXiv preprint\\narXiv:2401.00625 , 2024.\\n[43] S. Tian, Q. Jin, L. Yeganova, P.-T. Lai, Q. Zhu, X. Chen, Y . Yang,\\nQ. Chen, W. Kim, D. C. Comeau et al. , “Opportunities and challenges\\nfor chatgpt and large language models in biomedicine and health,”\\nBriefings in Bioinformatics , vol. 25, no. 1, p. bbad493, 2024.\\n[44] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning repre-\\nsentations by back-propagating errors,” nature , vol. 323, no. 6088, pp.\\n533–536, 1986.\\n[45] S. M. Kasongo, “A deep learning technique for intrusion detection\\nsystem using a recurrent neural networks based framework,” Computer\\nCommunications , vol. 199, pp. 113–125, 2023.\\n[46] S. M. Sohi, J.-P. Seifert, and F. Ganji, “Rnnids: Enhancing network\\nintrusion detection systems through deep learning,” Computers &\\nSecurity , vol. 102, p. 102151, 2021.\\n[47] K. Cho, B. Van Merri ¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\\nH. Schwenk, and Y . Bengio, “Learning phrase representations using\\nrnn encoder-decoder for statistical machine translation,” arXiv preprint\\narXiv:1406.1078 , 2014.\\n[48] H. Sedjelmaci, F. Guenab, S.-M. Senouci, H. Moustafa, J. Liu, and\\nS. Han, “Cyber security based on artificial intelligence for cyber-\\nphysical systems,” IEEE Network , vol. 34, no. 3, pp. 6–7, 2020.\\n[49] P. Dixit and S. Silakari, “Deep learning algorithms for cybersecurity\\napplications: A technological and status review,” Computer Science\\nReview , vol. 39, p. 100317, 2021.\\n[50] S. Gaba, I. Budhiraja, V . Kumar, S. Martha, J. Khurmi, A. Singh,\\nK. K. Singh, S. Askar, and M. Abouhawwash, “A systematic analysis\\nof enhancing cyber security using deep learning for cyber physical\\nsystems,” IEEE Access , 2024.\\n[51] G. Parra, L. Selvera, J. Khoury, H. Irizarry, E. Bou-Harb, and P. Rad,\\n“Interpretable federated transformer log learning for cloud threat foren-\\nsics,” in Proceedings of the Network and Distributed Systems Security\\n(NDSS) Symposium , 2022.\\n[52] N. Ziems and S. Wu, “Security vulnerability detection using deep\\nlearning natural language processing,” in IEEE INFOCOM 2021-IEEE\\nConference on Computer Communications Workshops (INFOCOM\\nWKSHPS) . IEEE, 2021, pp. 1–6.\\n[53] Z. Wu, H. Zhang, P. Wang, and Z. Sun, “Rtids: A robust transformer-\\nbased approach for intrusion detection system,” IEEE Access , vol. 10,\\npp. 64 375–64 387, 2022.\\n[54] F. Demirkıran, A. C ¸ ayır, U. ¨Unal, and H. Da ˘g, “An ensemble of\\npre-trained transformer models for imbalanced multiclass malware\\nclassification,” Computers & Security , vol. 121, p. 102846, 2022.\\n[55] C. Yin, Y . Zhu, J. Fei, and X. He, “A deep learning approach for\\nintrusion detection using recurrent neural networks,” Ieee Access ,\\nvol. 5, pp. 21 954–21 961, 2017.\\n[56] D. G ¨uera and E. J. Delp, “Deepfake video detection using recurrent\\nneural networks,” in 2018 15th IEEE international conference on\\nadvanced video and signal based surveillance (AVSS) . IEEE, 2018,\\npp. 1–6.\\n[57] S. Althubiti, W. Nick, J. Mason, X. Yuan, and A. Esterline, “Applying\\nlong short-term memory recurrent neural network for intrusion detec-\\ntion,” in SoutheastCon 2018 . IEEE, 2018, pp. 1–5.\\n[58] C. Xu, J. Shen, X. Du, and F. Zhang, “An intrusion detection system\\nusing a deep neural network with gated recurrent units,” IEEE Access ,\\nvol. 6, pp. 48 697–48 707, 2018.\\n[59] M. A. Ferrag and L. Maglaras, “Deepcoin: A novel deep learning and\\nblockchain-based energy exchange framework for smart grids,” IEEE\\nTransactions on Engineering Management , vol. 67, no. 4, pp. 1285–\\n1297, 2019.\\n[60] A. Ghourabi, “A security model based on lightgbm and transformer to\\nprotect healthcare systems from cyberattacks,” IEEE Access , vol. 10,\\npp. 48 890–48 903, 2022.\\n[61] C. Thapa, S. I. Jang, M. E. Ahmed, S. Camtepe, J. Pieprzyk, and\\nS. Nepal, “Transformer-based language models for software vulnera-\\nbility detection,” in Proceedings of the 38th Annual Computer Security\\nApplications Conference , 2022, pp. 481–496.\\n45', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='981f9f48-11a0-438b-a7f4-a8a32b1f823e', embedding=None, metadata={'page_label': '46', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[62] P. Ranade, A. Piplai, S. Mittal, A. Joshi, and T. Finin, “Generating\\nfake cyber threat intelligence using transformer-based models,” in 2021\\nInternational Joint Conference on Neural Networks (IJCNN) . IEEE,\\n2021, pp. 1–9.\\n[63] M. Fu and C. Tantithamthavorn, “Linevul: a transformer-based line-\\nlevel vulnerability prediction,” in Proceedings of the 19th International\\nConference on Mining Software Repositories , 2022, pp. 608–620.\\n[64] C. Mamede, E. Pinconschi, and R. Abreu, “A transformer-based ide\\nplugin for vulnerability detection,” in 37th IEEE/ACM International\\nConference on Automated Software Engineering , 2022, pp. 1–4.\\n[65] P. Evangelatos, C. Iliou, T. Mavropoulos, K. Apostolou, T. Tsikrika,\\nS. Vrochidis, and I. Kompatsiaris, “Named entity recognition in cyber\\nthreat intelligence using transformer-based models,” in 2021 IEEE\\nInternational Conference on Cyber Security and Resilience (CSR) .\\nIEEE, 2021, pp. 348–353.\\n[66] F. Hashemi Chaleshtori and I. Ray, “Automation of vulnerability\\ninformation extraction using transformer-based language models,” in\\nComputer Security. ESORICS 2022 International Workshops . Springer,\\n2023, pp. 645–665.\\n[67] A. Chawla, B. Lee, S. Fallon, and P. Jacob, “Host based intrusion\\ndetection system with combined cnn/rnn model,” in ECML PKDD 2018\\nWorkshops: Nemesis 2018, UrbReas 2018, SoGood 2018, IWAISe 2018,\\nand Green Data Mining 2018, Dublin, Ireland, September 10-14, 2018,\\nProceedings 18 . Springer, 2019, pp. 149–158.\\n[68] I. Ullah and Q. H. Mahmoud, “Design and development of rnn anomaly\\ndetection model for iot networks,” IEEE Access , vol. 10, pp. 62 722–\\n62 750, 2022.\\n[69] A. A. E.-B. Donkol, A. G. Hafez, A. I. Hussein, and M. M. Mabrook,\\n“Optimization of intrusion detection using likely point pso and en-\\nhanced lstm-rnn hybrid technique in communication networks,” IEEE\\nAccess , vol. 11, pp. 9469–9482, 2023.\\n[70] Z. Zhao, Z. Li, J. Jiang, F. Yu, F. Zhang, C. Xu, X. Zhao, R. Zhang,\\nand S. Guo, “Ernn: Error-resilient rnn for encrypted traffic detection\\ntowards network-induced phenomena,” IEEE Transactions on Depend-\\nable and Secure Computing , 2023.\\n[71] X. Wang, S. Wang, P. Feng, K. Sun, S. Jajodia, S. Benchaaboun, and\\nF. Geck, “Patchrnn: A deep learning-based system for security patch\\nidentification,” in MILCOM 2021-2021 IEEE Military Communications\\nConference (MILCOM) . IEEE, 2021, pp. 595–600.\\n[72] H. Polat, M. T ¨urko ˘glu, O. Polat, and A. S ¸eng ¨ur, “A novel approach for\\naccurate detection of the ddos attacks in sdn-based scada systems based\\non deep recurrent neural networks,” Expert Systems with Applications ,\\nvol. 197, p. 116748, 2022.\\n[73] S. Liu, Y . Li, and Y . Liu, “Commitbart: A large pre-trained model for\\ngithub commits,” arXiv preprint arXiv:2208.08100 , 2022.\\n[74] B. Ahmad, S. Thakur, B. Tan, R. Karri, and H. Pearce, “On hardware\\nsecurity bug code fixes by prompting large language models,” IEEE\\nTransactions on Information Forensics and Security , pp. 1–1, 2024.\\n[75] L. J. Wan, Y . Huang, Y . Li, H. Ye, J. Wang, X. Zhang, and D. Chen,\\n“Invited paper: Software/hardware co-design for llm and its application\\nfor design verification,” in 2024 29th Asia and South Pacific Design\\nAutomation Conference (ASP-DAC) , 2024, pp. 435–441.\\n[76] E. Jang, J. Cui, D. Yim, Y . Jin, J.-W. Chung, S. Shin, and Y . Lee,\\n“Ignore me but don’t replace me: Utilizing non-linguistic elements\\nfor pretraining on the cybersecurity domain,” arXiv preprint , 2024, to\\nappear in NAACL Findings 2024.\\n[77] M. Bayer, P. Kuehn, R. Shanehsaz, and C. Reuter, “Cysecbert: A\\ndomain-adapted language model for the cybersecurity domain,” ACM\\nTransactions on Privacy and Security , vol. 27, no. 2, pp. 1–20, 2024.\\n[78] A. Shestov, R. Levichev, R. Mussabayev, E. Maslov, A. Cheshkov, and\\nP. Zadorozhny, “Finetuning large language models for vulnerability\\ndetection,” arXiv preprint , 2024, version 4.\\n[79] F. He, F. Li, and P. Liang, “Enhancing smart contract security: Leverag-\\ning pre-trained language models for advanced vulnerability detection,”\\nIET Blockchain , 2024, first published: 29 March 2024.\\n[80] Y . Ding, Y . Fu, O. Ibrahim, C. Sitawarin, X. Chen, B. Alomair,\\nD. Wagner, B. Ray, and Y . Chen, “Vulnerability detection with code\\nlanguage models: How far are we?” arXiv preprint arXiv:2403.18624 ,\\n2024.\\n[81] T. Koide, N. Fukushi, H. Nakano, and D. Chiba, “Chatspamdetector:\\nLeveraging large language models for effective phishing email detec-\\ntion,” arXiv preprint arXiv:2402.18093 , 2024.\\n[82] F. Heiding, B. Schneier, A. Vishwanath, J. Bernstein, and P. S. Park,\\n“Devising and detecting phishing emails using large language models,”\\nIEEE Access , 2024.\\n[83] R. Chataut, P. K. Gyawali, and Y . Usman, “Can ai keep you safe? a\\nstudy of large language models for phishing detection,” in 2024 IEEE14th Annual Computing and Communication Workshop and Conference\\n(CCWC) . IEEE, 2024, pp. 0548–0554.\\n[84] M. Rostami, M. Chilese, S. Zeitouni, R. Kande, J. Rajendran, and A.-R.\\nSadeghi, “Beyond random inputs: A novel ml-based hardware fuzzing,”\\n2024.\\n[85] Z. Zhang, G. Chadwick, H. McNally, Y . Zhao, and R. Mullins,\\n“Llm4dv: Using large language models for hardware test stimuli\\ngeneration,” 2023.\\n[86] M. Nair, R. Sadhukhan, and D. Mukhopadhyay, “Generating\\nsecure hardware using chatgpt resistant to cwes,” Cryptology\\nePrint Archive, Paper 2023/212, 2023, https://eprint.iacr.org/2023/212.\\n[Online]. Available: https://eprint.iacr.org/2023/212\\n[87] L. J. Wan, Y . Huang, Y . Li, H. Ye, J. Wang, X. Zhang,\\nand D. Chen, “Software/hardware co-design for llm and its\\napplication for design verification,” in Proceedings of the 29th Asia\\nand South Pacific Design Automation Conference , ser. ASPDAC\\n’24. IEEE Press, 2024, p. 435–441. [Online]. Available: https:\\n//doi.org/10.1109/ASP-DAC58780.2024.10473893\\n[88] M. Liu, N. Pinckney, B. Khailany, and H. Ren, “Verilogeval: Evaluating\\nlarge language models for verilog code generation,” 2023.\\n[89] N. Tihanyi, M. A. Ferrag, R. Jain, and M. Debbah, “Cybermetric: A\\nbenchmark dataset for evaluating large language models knowledge in\\ncybersecurity,” arXiv preprint arXiv:2402.07688 , 2024.\\n[90] R. Meng, M. Mirchev, M. B ¨ohme, and A. Roychoudhury, “Large\\nlanguage model guided protocol fuzzing,” in Proceedings of the 31st\\nAnnual Network and Distributed System Security Symposium (NDSS) ,\\n2024.\\n[91] V .-T. Pham, M. B ¨ohme, and A. Roychoudhury, “Aflnet: A greybox\\nfuzzer for network protocols,” in 2020 IEEE 13th International Con-\\nference on Software Testing, Validation and Verification (ICST) , 2020,\\npp. 460–465.\\n[92] S. Qin, F. Hu, Z. Ma, B. Zhao, T. Yin, and C. Zhang, “Nsfuzz:\\nTowards efficient and state-aware network service fuzzing,” ACM\\nTrans. Softw. Eng. Methodol. , vol. 32, no. 6, sep 2023. [Online].\\nAvailable: https://doi.org/10.1145/3580598\\n[93] J. Wang, L. Yu, and X. Luo, “Llmif: Augmented large language model\\nfor fuzzing iot devices,” in 2024 IEEE Symposium on Security and\\nPrivacy (SP) . IEEE Computer Society, 2024, pp. 196–196.\\n[94] M. Ren, X. Ren, H. Feng, J. Ming, and Y . Lei, “Z-fuzzer: device-\\nagnostic fuzzing of zigbee protocol implementation,” in Proceedings\\nof the 14th ACM Conference on Security and Privacy in Wireless and\\nMobile Networks , ser. WiSec ’21. New York, NY , USA: Association\\nfor Computing Machinery, 2021, p. 347–358. [Online]. Available:\\nhttps://doi.org/10.1145/3448300.3468296\\n[95] J. Pereyda, “Boofuzz: Network protocol fuzzing for humans,” https:\\n//boofuzz.readthedocs.io/en/stable, 2020.\\n[96] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language mod-\\nels are few-shot learners,” Advances in neural information processing\\nsystems , vol. 33, pp. 1877–1901, 2020.\\n[97] OpenAI, “Gpt-4 technical report,” 2023.\\n[98] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learn-\\ning with a unified text-to-text transformer,” The Journal of Machine\\nLearning Research , vol. 21, no. 1, pp. 5485–5551, 2020.\\n[99] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv\\npreprint arXiv:1810.04805 , 2018.\\n[100] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n“Albert: A lite bert for self-supervised learning of language represen-\\ntations,” arXiv preprint arXiv:1909.11942 , 2019.\\n[101] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\\npretraining approach,” arXiv preprint arXiv:1907.11692 , 2019.\\n[102] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\\nQ. V . Le, “Xlnet: Generalized autoregressive pretraining for language\\nunderstanding,” Advances in neural information processing systems ,\\nvol. 32, 2019.\\n[103] W. Qi, Y . Yan, Y . Gong, D. Liu, N. Duan, J. Chen, R. Zhang,\\nand M. Zhou, “Prophetnet: Predicting future n-gram for sequence-to-\\nsequence pre-training,” arXiv preprint arXiv:2001.04063 , 2020.\\n[104] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refined-\\nweb dataset for falcon llm: outperforming curated corpora with web\\ndata, and web data only,” arXiv preprint arXiv:2306.01116 , 2023.\\n[105] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efficient\\ntransformer,” arXiv preprint arXiv:2001.04451 , 2020.\\n46', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8e539b3-db1b-48ae-92af-7df04ea2bc74', embedding=None, metadata={'page_label': '47', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[106] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al. , “Palm: Scaling\\nlanguage modeling with pathways,” Journal of Machine Learning\\nResearch , vol. 24, no. 240, pp. 1–113, 2023.\\n[107] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al. , “Palm 2 technical\\nreport,” arXiv preprint arXiv:2305.10403 , 2023.\\n[108] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. , “Llama:\\nOpen and efficient foundation language models,” arXiv preprint\\narXiv:2302.13971 , 2023.\\n[109] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al. , “Llama\\n2: Open foundation and fine-tuned chat models,” arXiv preprint\\narXiv:2307.09288 , 2023.\\n[110] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun,\\nN. Shazeer, and Z. Chen, “Gshard: Scaling giant models with\\nconditional computation and automatic sharding,” arXiv preprint\\narXiv:2006.16668 , 2020.\\n[111] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre-\\ntraining text encoders as discriminators rather than generators,” arXiv\\npreprint arXiv:2003.10555 , 2020.\\n[112] The MosaicML NLP Team, “Mpt-30b: Raising the bar for open-\\nsource foundation models,” June 2023, accessed: 2023-12-10. [Online].\\nAvailable: https://www.mosaicml.com/blog/mpt-30b\\n[113] 01.AI, “Yi-34b,” https://huggingface.co/01-ai/Yi-34B, 2023, accessed:\\n2023-12-10.\\n[114] TIIUAE, “Falcon-11b,” https://huggingface.co/tiiuae/falcon-11B, 2024,\\naccessed: 2024-05-01.\\n[115] P. Haller, J. Golde, and A. Akbik, “Pecc: Problem extraction and coding\\nchallenges,” arXiv preprint arXiv:2404.18766 , 2024.\\n[116] A. Z. Yang, Y . Takashima, B. Paulsen, J. Dodds, and D. Kroening,\\n“Vert: Verified equivalent rust transpilation with few-shot learning,”\\narXiv preprint arXiv:2404.18852 , 2024.\\n[117] D. Nichols, P. Polasam, H. Menon, A. Marathe, T. Gamblin, and\\nA. Bhatele, “Performance-aligned llms for generating fast code,” arXiv\\npreprint arXiv:2404.18864 , 2024.\\n[118] Z. Ma, A. R. Chen, D. J. Kim, T.-H. Chen, and S. Wang, “Llmparser:\\nAn exploratory study on using large language models for log parsing,”\\ninProceedings of the IEEE/ACM 46th International Conference on\\nSoftware Engineering , 2024, pp. 1–13.\\n[119] T. H. Le, M. A. Babar, and T. H. Thai, “Software vulnerability\\nprediction in low-resource languages: An empirical study of codebert\\nand chatgpt,” arXiv preprint arXiv:2404.17110 , 2024.\\n[120] B. Guan, Y . Wan, Z. Bi, Z. Wang, H. Zhang, Y . Sui, P. Zhou, and\\nL. Sun, “Codeip: A grammar-guided multi-bit watermark for large\\nlanguage models of code,” arXiv preprint arXiv:2404.15639 , 2024.\\n[121] X.-C. Wen, X. Wang, Y . Chen, R. Hu, D. Lo, and C. Gao, “Vuleval: To-\\nwards repository-level evaluation of software vulnerability detection,”\\narXiv preprint arXiv:2404.15596 , 2024.\\n[122] Z. Zhang, C. Chen, B. Liu, C. Liao, Z. Gong, H. Yu, J. Li, and R. Wang,\\n“Unifying the perspectives of nlp and software engineering: A survey\\non language models for code,” arXiv preprint arXiv:2311.07989 , 2023.\\n[123] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis,\\nN. Muennighoff, M. Mishra, A. Gu, M. Dey et al. , “Santacoder: don’t\\nreach for the stars!” arXiv preprint arXiv:2301.03988 , 2023.\\n[124] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim et al. , “Starcoder: may the source\\nbe with you!” arXiv preprint arXiv:2305.06161 , 2023.\\n[125] Hugging Face & ServiceNow, “Huggingfaceh4/starchat-alpha,” https://\\nhuggingface.co/HuggingFaceH4/starchat-alpha, 2023, accessed: 2023-\\n12-10.\\n[126] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y . Zhou,\\n“Codegen2: Lessons for training llms on programming and natural\\nlanguages,” arXiv preprint arXiv:2305.02309 , 2023.\\n[127] Salesforce AI Research, “Codegen2.5: Small, but mighty,”\\n2023, accessed: 2023-12-10. [Online]. Available: https://blog.\\nsalesforceairesearch.com/codegen25/\\n[128] Y . Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi,\\n“Codet5+: Open code large language models for code understanding\\nand generation,” arXiv preprint arXiv:2305.07922 , 2023.\\n[129] E. Nijkamp, T. Xie, H. Hayashi, B. Pang, C. Xia, C. Xing, J. Vig,\\nS. Yavuz, P. Laban, B. Krause et al. , “Xgen-7b technical report,” arXiv\\npreprint arXiv:2309.03450 , 2023.\\n[130] Replit, Inc., “replit-code-v1-3b,” 2023, accessed: 2023-12-10. [Online].\\nAvailable: https://huggingface.co/replit/replit-code-v1-3b[131] Deci AI, “Introducing decicoder: The new gold standard\\nin efficient and accurate code generation,” August 2023,\\naccessed: 2023-12-10. [Online]. Available: https://deci.ai/blog/\\ndecicoder-efficient-and-accurate-code-generation-llm/\\n[132] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,\\nJ. Liu, T. Remez, J. Rapin et al. , “Code llama: Open foundation models\\nfor code,” arXiv preprint arXiv:2308.12950 , 2023.\\n[133] J. Bai, S. Bai, Y . Chu, Z. Cui, K. Dang, X. Deng, Y . Fan, W. Ge,\\nY . Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu,\\nC. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu,\\nP. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang,\\nJ. Yang, S. Yang, Y . Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang,\\nY . Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and T. Zhu, “Qwen\\ntechnical report,” arXiv preprint, Tech. Rep., 2023, 59 pages, 5 figures.\\n[134] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen,\\nX. Bi, Y . Wu, Y . Li, F. Luo, Y . Xiong, and W. Liang, “Deepseek-\\ncoder: When the large language model meets programming – the rise\\nof code intelligence,” arXiv preprint , 2024, submitted on 25 Jan 2024,\\nLast revised 26 Jan 2024.\\n[135] C. Team, A. J. Hartman, A. Hu, C. A. Choquette-Choo, H. Zhao,\\nJ. Fine, J. Hui, J. Shen, J. Kelley, J. Howland, K. Bansal, L. Vilnis,\\nM. Wirth, N. Nguyen, P. Michel, P. Choy, P. Joshi, R. Kumar,\\nS. Hashmi, S. Agrawal, S. Zuo, T. Warkentin, and Z. Gong,\\n“Codegemma: Open code models based on gemma,” 2024. [Online].\\nAvailable: https://goo.gle/codegemma\\n[136] M. Mishra, M. Stallone, G. Zhang, Y . Shen, A. Prasad, A. Meza Soria,\\nM. Merler, P. Selvam, S. Surendran, S. Singh, M. Sethi, X.-H. Dang,\\nP. Li, K.-L. Wu, S. Zawad, A. Coleman, M. White, M. Lewis,\\nR. Pavuluri, Y . Koyfman, B. Lublinsky, M. de Bayser, I. Abdelaziz,\\nK. Basu, M. Agarwal, Y . Zhou, C. Johnson, A. Goyal, H. Patel,\\nY . Shah, P. Zerfos, H. Ludwig, A. Munawar, M. Crouse, P. Kapanipathi,\\nS. Salaria, B. Calio, S. Wen, S. Seelam, B. Belgodere, C. Fonseca,\\nA. Singhee, N. Desai, D. D. Cox, R. Puri, and R. Panda, “Granite code\\nmodels: A family of open foundation models for code intelligence,”\\narXiv preprint arXiv:2405.04324 , May 2024.\\n[137] DeepSeek-AI, “Deepseek-v2: A strong, economical, and efficient\\nmixture-of-experts language model,” arXiv preprint arXiv:2405.04434 ,\\nMay 2024, submitted on 7 May 2024 (v1), last revised 8 May 2024\\n(this version, v2). [Online]. Available: https://arxiv.org/abs/2405.04434\\n[138] M. A. et al., “Phi-3 technical report: A highly capable language model\\nlocally on your phone,” arXiv preprint arXiv:2404.14219 , 2024.\\n[139] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S.\\nChaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample,\\nL. Saulnier, L. Renard Lavaud, M.-A. Lachaux, P. Stock, T. Le Scao,\\nT. Lavril, T. Wang, T. Lacroix, and W. El Sayed, “Mistral 7b,” arXiv\\npreprint arXiv:2310.06825 , 2023, submitted on 10 Oct 2023. [Online].\\nAvailable: https://doi.org/10.48550/arXiv.2310.06825\\n[140] N. Dey, G. Gosal, Z. C. Chen, H. Khachane, W. Marshall, R. Pathria,\\nM. Tom, and J. Hestness, “Cerebras-gpt: Open compute-optimal\\nlanguage models trained on the cerebras wafer-scale cluster,” arXiv\\npreprint arXiv:2304.03208 , 2023, submitted on 6 Apr 2023. [Online].\\nAvailable: https://doi.org/10.48550/arXiv.2304.03208\\n[141] ZySec-AI, “Zysec-ai: Project zysec,” Webpage, accessed: 2024-05-01.\\n[Online]. Available: https://github.com/ZySec-AI/project-zysec\\n[142] DeciAI Research Team, “Decilm-7b,” 2023. [Online]. Available:\\nhttps://huggingface.co/Deci/DeciLM-7B\\n[143] L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y . Belkada,\\nS. Huang, L. von Werra, C. Fourrier, N. Habib, N. Sarrazin, O. San-\\nseviero, A. M. Rush, and T. Wolf, “Zephyr: Direct distillation of lm\\nalignment,” 2023.\\n[144] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah,\\nA. Ghodsi, P. Wendell, M. Zaharia, and R. Xin. (2023) Free\\ndolly: Introducing the world’s first truly open instruction-tuned\\nllm. [Online]. Available: https://www.databricks.com/blog/2023/04/12/\\ndolly-first-open-commercially-viable-instruction-tuned-llm\\n[145] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\\n“Codesearchnet challenge: Evaluating the state of semantic code\\nsearch,” arXiv preprint arXiv:1909.09436 , 2019.\\n[146] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\\nJ. Phang, H. He, A. Thite, N. Nabeshima et al. , “The pile: An\\n800gb dataset of diverse text for language modeling,” arXiv preprint\\narXiv:2101.00027 , 2020.\\n[147] D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis,\\nY . Jernite, M. Mitchell, S. Hughes, T. Wolf et al. , “The stack: 3 tb of\\npermissively licensed source code,” arXiv preprint arXiv:2211.15533 ,\\n2022.\\n47', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='29fd4545-3ef9-430a-bfc5-c804a1c23389', embedding=None, metadata={'page_label': '48', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[148] H. Laurenc ¸on, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\\nT. Le Scao, L. V on Werra, C. Mou, E. Gonz ´alez Ponferrada, H. Nguyen\\net al. , “The bigscience roots corpus: A 1.6 tb composite multilingual\\ndataset,” Advances in Neural Information Processing Systems , vol. 35,\\npp. 31 809–31 826, 2022.\\n[149] A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi,\\nA. Tang, D. Pykhtar, J. Liu, Y . Wei et al. , “Starcoder 2 and the stack\\nv2: The next generation,” arXiv preprint arXiv:2402.19173 , 2024.\\n[150] R. Schuster, C. Song, E. Tromer, and V . Shmatikov, “You autocomplete\\nme: Poisoning vulnerabilities in neural code completion,” in 30th\\nUSENIX Security Symposium (USENIX Security 21) , 2021, pp. 1559–\\n1575.\\n[151] O. Asare, M. Nagappan, and N. Asokan, “Is github’s copilot as bad\\nas humans at introducing vulnerabilities in code?” Empirical Software\\nEngineering , vol. 28, no. 6, p. 129, 2023.\\n[152] G. Sandoval, H. Pearce, T. Nys, R. Karri, S. Garg, and B. Dolan-Gavitt,\\n“Lost at c: A user study on the security implications of large language\\nmodel code assistants,” in 32nd USENIX Security Symposium (USENIX\\nSecurity 23) , 2023, pp. 2205–2222.\\n[153] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do users write\\nmore insecure code with ai assistants?” in Proceedings of the 2023\\nACM SIGSAC Conference on Computer and Communications Security ,\\n2023, pp. 2785–2799.\\n[154] S. Hamer, M. d’Amorim, and L. Williams, “Just another copy and\\npaste? comparing the security vulnerabilities of chatgpt generated code\\nand stackoverflow answers,” arXiv preprint arXiv:2403.15600 , 2024.\\n[155] D. Cotroneo, R. De Luca, and P. Liguori, “Devaic: A tool for security\\nassessment of ai-generated code,” arXiv preprint arXiv:2404.07548 ,\\n2024.\\n[156] R. T ´oth, T. Bisztray, and L. Erdodi, “Llms in web-development: Evalu-\\nating llm-generated php code unveiling vulnerabilities and limitations,”\\narXiv preprint arXiv:2404.14459 , 2024.\\n[157] N. Tihanyi, T. Bisztray, M. A. Ferrag, R. Jain, and L. C. Cordeiro, “Do\\nneutral prompts produce insecure code? formai-v2 dataset: Labelling\\nvulnerabilities in code generated by large language models,” 2024.\\n[158] N. S. Harzevili, A. B. Belle, J. Wang, S. Wang, Z. Ming, N. Nagappan\\net al. , “A survey on automated software vulnerability detection using\\nmachine learning and deep learning,” arXiv preprint arXiv:2306.11673 ,\\n2023.\\n[159] M. A. Ferrag, O. Friha, D. Hamouda, L. Maglaras, and H. Janicke,\\n“Edge-iiotset: A new comprehensive realistic cyber security dataset of\\niot and iiot applications for centralized and federated learning,” IEEE\\nAccess , vol. 10, pp. 40 281–40 306, 2022.\\n[160] N. Tihanyi, T. Bisztray, R. Jain, M. A. Ferrag, L. C. Cordeiro, and\\nV . Mavroeidis, “The formai dataset: Generative ai in software security\\nthrough the lens of formal verification,” in Proceedings of the 19th\\nInternational Conference on Predictive Models and Data Analytics in\\nSoftware Engineering , 2023, pp. 33–43.\\n[161] Y . Zheng, S. Pujar, B. Lewis, L. Buratti, E. Epstein, B. Yang, J. Laredo,\\nA. Morari, and Z. Su, “D2a: A dataset built for ai-based vulnerability\\ndetection methods using differential analysis,” in 2021 IEEE/ACM\\n43rd International Conference on Software Engineering: Software\\nEngineering in Practice (ICSE-SEIP) , 2021, pp. 111–120.\\n[162] Y . Zhou, S. Liu, J. Siow, X. Du, and Y . Liu, “Devign: Effective Vulner-\\nability Identification by Learning Comprehensive Program Semantics\\nvia Graph Neural Networks,” arXiv e-prints , p. arXiv:1909.03496, Sep.\\n2019.\\n[163] H. Hanif, M. H. N. M. Nasir, M. F. Ab Razak, A. Firdaus, and N. B.\\nAnuar, “The rise of software vulnerability: Taxonomy of software\\nvulnerabilities detection and machine learning approaches,” Journal of\\nNetwork and Computer Applications , vol. 179, p. 103009, 2021.\\n[164] R. Russell, L. Kim, L. Hamilton, T. Lazovich, J. Harer, O. Ozdemir,\\nP. Ellingwood, and M. McConley, “Automated vulnerability detection\\nin source code using deep representation learning,” in 2018 17th\\nIEEE International Conference on Machine Learning and Applications\\n(ICMLA) , 2018, pp. 757–762.\\n[165] ——, “Automated vulnerability detection in source code using deep\\nrepresentation learning,” in 2018 17th IEEE International Conference\\non Machine Learning and Applications (ICMLA) , 2018, pp. 757–762.\\n[166] Y . Zhou and A. Sharma, “Automated identification of security issues\\nfrom commit messages and bug reports,” in Proceedings of the 2017\\n11th joint meeting on foundations of software engineering , 2017, pp.\\n914–919.\\n[167] L. Wartschinski, Y . Noller, T. V ogel, T. Kehrer, and L. Grunske,\\n“Vudenc: Vulnerability detection with deep learning on a natural\\ncodebase for python,” Information and Software Technology , vol.144, p. 106809, 2022, arXiv preprint arXiv:2201.08441. [Online].\\nAvailable: https://doi.org/10.48550/arXiv.2201.08441\\n[168] J. Fan, Y . Li, S. Wang, and T. N. Nguyen, “A c/c++ code\\nvulnerability dataset with code changes and cve summaries,” in\\nProceedings of the 17th International Conference on Mining Software\\nRepositories , ser. MSR ’20. New York, NY , USA: Association\\nfor Computing Machinery, 2020, p. 508–512. [Online]. Available:\\nhttps://doi.org/10.1145/3379597.3387501\\n[169] G. Bhandari, A. Naseer, and L. Moonen, “Cvefixes: automated collec-\\ntion of vulnerabilities and their fixes from open-source software,” in\\nProceedings of the 17th International Conference on Predictive Models\\nand Data Analytics in Software Engineering , 2021, pp. 30–39.\\n[170] G. Nikitopoulos, K. Dritsa, P. Louridas, and D. Mitropoulos, “Crossvul:\\na cross-language vulnerability dataset with commit data,” in Proceed-\\nings of the 29th ACM Joint Meeting on European Software Engineering\\nConference and Symposium on the Foundations of Software Engineer-\\ning, 2021, pp. 1565–1569.\\n[171] Z. Li, D. Zou, S. Xu, H. Jin, Y . Zhu, and Z. Chen, “Sysevr: A\\nframework for using deep learning to detect software vulnerabilities,”\\nIEEE Transactions on Dependable and Secure Computing , vol. 19,\\nno. 4, pp. 2244–2258, 2022.\\n[172] Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S. Wang, Z. Deng, and Y . Zhong,\\n“Vuldeepecker: A deep learning-based system for vulnerability detec-\\ntion,” arXiv preprint arXiv:1801.01681 , 2018.\\n[173] Y . Chen, Z. Ding, L. Alowain, X. Chen, and D. Wagner, “DiverseVul:\\nA New Vulnerable Source Code Dataset for Deep Learning Based\\nVulnerability Detection,” arXiv e-prints , p. arXiv:2304.00409, Apr.\\n2023.\\n[174] D. N. Gadde, A. Kumar, T. Nalapat, E. Rezunov, and F. Cappellini,\\n“All artificial, less intelligence: Genai through the lens of formal\\nverification,” Infineon Technologies , 2024.\\n[175] OWASP Foundation, “Owasp top 10 for large\\nlanguage model applications,” https://owasp.org/\\nwww-project-top-10-for-large-language-model-applications/, 2023,\\naccessed: 2023-12-26.\\n[176] F. Perez and I. Ribeiro, “Ignore previous prompt: Attack techniques\\nfor language models,” arXiv preprint arXiv:2211.09527 , 2022.\\n[177] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and\\nM. Fritz, “More than you’ve asked for: A comprehensive analysis of\\nnovel prompt injection threats to application-integrated large language\\nmodels,” arXiv e-prints , pp. arXiv–2302, 2023.\\n[178] J. Yan, V . Yadav, S. Li, L. Chen, Z. Tang, H. Wang, V . Srinivasan,\\nX. Ren, and H. Jin, “Virtual prompt injection for instruction-tuned\\nlarge language models,” arXiv preprint arXiv:2307.16888 , 2023.\\n[179] R. Pedro, D. Castro, P. Carreira, and N. Santos, “From prompt\\ninjections to sql injection attacks: How protected is your llm-integrated\\nweb application?” arXiv preprint arXiv:2308.01990 , 2023.\\n[180] S. Abdelnabi, K. Greshake, S. Mishra, C. Endres, T. Holz, and M. Fritz,\\n“Not what you’ve signed up for: Compromising real-world llm-\\nintegrated applications with indirect prompt injection,” in Proceedings\\nof the 16th ACM Workshop on Artificial Intelligence and Security , 2023,\\npp. 79–90.\\n[181] Y . Liu, G. Deng, Y . Li, K. Wang, T. Zhang, Y . Liu, H. Wang,\\nY . Zheng, and Y . Liu, “Prompt injection attack against llm-integrated\\napplications,” arXiv preprint arXiv:2306.05499 , 2023.\\n[182] J. Yan, V . Yadav, S. Li, L. Chen, Z. Tang, H. Wang, V . Srinivasan,\\nX. Ren, and H. Jin, “Backdooring instruction-tuned large language\\nmodels with virtual prompt injection,” in NeurIPS 2023 Workshop on\\nBackdoors in Deep Learning-The Good, the Bad, and the Ugly , 2023.\\n[183] D. Glukhov, I. Shumailov, Y . Gal, N. Papernot, and V . Papyan, “Llm\\ncensorship: A machine learning challenge or a computer security\\nproblem?” arXiv preprint arXiv:2307.10719 , 2023.\\n[184] F. Wu, X. Liu, and C. Xiao, “Deceptprompt: Exploiting llm-driven\\ncode generation via adversarial natural language instructions,” arXiv\\npreprint arXiv:2312.04730 , 2023.\\n[185] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson, “Universal and\\ntransferable adversarial attacks on aligned language models,” arXiv\\npreprint arXiv:2307.15043 , 2023.\\n[186] Z. Yang, X. He, Z. Li, M. Backes, M. Humbert, P. Berrang, and\\nY . Zhang, “Data poisoning attacks against multimodal encoders,” in\\nInternational Conference on Machine Learning . PMLR, 2023, pp.\\n39 299–39 313.\\n[187] A. E. Cin `a, K. Grosse, A. Demontis, S. Vascon, W. Zellinger, B. A.\\nMoser, A. Oprea, B. Biggio, M. Pelillo, and F. Roli, “Wild patterns\\nreloaded: A survey of machine learning security against training data\\npoisoning,” ACM Computing Surveys , vol. 55, no. 13s, pp. 1–39, 2023.\\n48', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c371784c-359a-437e-9de9-bf91d30142c2', embedding=None, metadata={'page_label': '49', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[188] P. Gupta, K. Yadav, B. B. Gupta, M. Alazab, and T. R. Gadekallu, “A\\nnovel data poisoning attack in federated learning based on inverted loss\\nfunction,” Computers & Security , vol. 130, p. 103270, 2023.\\n[189] J. He, W. Jiang, G. Hou, W. Fan, R. Zhang, and H. Li, “Talk too much:\\nPoisoning large language models under token limit,” arXiv preprint\\narXiv:2404.14795 , 2024.\\n[190] A. B. de Neira, B. Kantarci, and M. Nogueira, “Distributed denial of\\nservice attack prediction: Challenges, open issues and opportunities,”\\nComputer Networks , vol. 222, 2023.\\n[191] N. Hoque, D. K. Bhattacharyya, and J. K. Kalita, “Botnet in ddos\\nattacks: Trends and challenges,” IEEE Communications Surveys and\\nTutorials , vol. 17, 2015.\\n[192] O. Osanaiye, K. K. R. Choo, and M. Dlodlo, “Distributed denial of\\nservice (ddos) resilience in cloud: Review and conceptual cloud ddos\\nmitigation framework,” 2016.\\n[193] Q. Yan, F. R. Yu, Q. Gong, and J. Li, “Software-defined networking\\n(sdn) and distributed denial of service (ddos) attacks in cloud com-\\nputing environments: A survey, some research issues, and challenges,”\\nIEEE Communications Surveys and Tutorials , vol. 18, 2016.\\n[194] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\\nand M. Du, “Explainability for large language models: A survey,” ACM\\nTransactions on Intelligent Systems and Technology , vol. 15, no. 2, pp.\\n1–38, 2024.\\n[195] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and\\nD. Jiang, “Wizardlm: Empowering large language models to follow\\ncomplex instructions,” arXiv preprint arXiv:2304.12244 , 2023.\\n[196] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nand T. Hashimoto, “Alpaca: a strong, replicable instruction-following\\nmodel; 2023,” URL https://crfm. stanford. edu/2023/03/13/alpaca. html .\\n[197] V . A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\\nM. Shoeybi, and B. Catanzaro, “Reducing activation recomputation\\nin large transformer models,” Proceedings of Machine Learning and\\nSystems , vol. 5, 2023.\\n[198] A. Andonian, Q. Anthony, S. Biderman, S. Black, P. Gali, L. Gao,\\nE. Hallahan, J. Levy-Kramer, C. Leahy, L. Nestler, K. Parker,\\nM. Pieler, J. Phang, S. Purohit, H. Schoelkopf, D. Stander, T. Songz,\\nC. Tigges, B. Th ´erien, P. Wang, and S. Weinbach, “GPT-NeoX:\\nLarge Scale Autoregressive Language Modeling in PyTorch,” 9 2023.\\n[Online]. Available: https://www.github.com/eleutherai/gpt-neox\\n[199] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361 , 2020.\\n[200] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer\\nlearning for nlp,” in International conference on machine learning .\\nPMLR, 2019, pp. 2790–2799.\\n[201] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\\nand W. Chen, “Lora: Low-rank adaptation of large language models,”\\narXiv preprint arXiv:2106.09685 , 2021.\\n[202] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora:\\nEfficient finetuning of quantized llms,” Advances in Neural Information\\nProcessing Systems , vol. 36, 2024.\\n[203] X. Wu, H. Xia, S. Youn, Z. Zheng, S. Chen, A. Bakhtiari, M. Wyatt,\\nR. Y . Aminabadi, Y . He, O. Ruwase, L. Song et al. , “Zeroquant(4+2):\\nRedefining llms quantization with a new fp6-centric strategy for diverse\\ngenerative tasks,” arXiv preprint arXiv:2312.08583 , 2023.\\n[204] H. Xia, Z. Zheng, X. Wu, S. Chen, Z. Yao, S. Youn, A. Bakhtiari,\\nM. Wyatt, D. Zhuang, Z. Zhou et al. , “Fp6-llm: Efficiently serving large\\nlanguage models through fp6-centric algorithm-system co-design,”\\narXiv preprint arXiv:2401.14112 , 2024.\\n[205] M. Bhatt, S. Chennabasappa, C. Nikolaidis, S. Wan, I. Evtimov,\\nD. Gabi, D. Song, F. Ahmad, C. Aschermann, L. Fontana et al. , “Purple\\nllama cyberseceval: A secure coding benchmark for language models,”\\narXiv preprint arXiv:2312.04724 , 2023.\\n[206] Z. Liu, “Secqa: A concise question-answering dataset for evalu-\\nating large language models in computer security,” arXiv preprint\\narXiv:2312.15838 , 2023.\\n[207] M. Bhatt, S. Chennabasappa, Y . Li, C. Nikolaidis, D. Song, S. Wan,\\nF. Ahmad, C. Aschermann, Y . Chen, D. Kapil, D. Molnar, S. Whitman,\\nand J. Saxe, “Cyberseceval 2: A wide-ranging cybersecurity evaluation\\nsuite for large language models,” 2024.\\n[208] N. Li, A. Pan, A. Gopal, S. Yue, D. Berrios, A. Gatti, J. D. Li, A.-\\nK. Dombrowski, S. Goel, L. Phan et al. , “The wmdp benchmark:\\nMeasuring and reducing malicious use with unlearning,” arXiv preprint\\narXiv:2403.03218 , 2024.[209] Y . Sun, D. Wu, Y . Xue, H. Liu, W. Ma, L. Zhang, M. Shi, and\\nY . Liu, “Llm4vuln: A unified evaluation framework for decoupling and\\nenhancing llms’ vulnerability reasoning,” 2024.\\n[210] Z. Liu, J. Shi, and J. F. Buford, “Cyberbench: A multi-task benchmark\\nfor evaluating large language models in cybersecurity.” [Online].\\nAvailable: http://aics.site/AICS2024/AICS CyberBench.pdf\\n[211] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457 , 2018.\\n[212] Y . Lai, C. Li, Y . Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-\\nt. Yih, D. Fried, S. Wang, and T. Yu, “Ds-1000: A natural and\\nreliable benchmark for data science code generation,” in International\\nConference on Machine Learning . PMLR, 2023, pp. 18 319–18 345.\\n[213] J. Liu, C. S. Xia, Y . Wang, and L. Zhang, “Is your code generated by\\nchatgpt really correct? rigorous evaluation of large language models for\\ncode generation,” Advances in Neural Information Processing Systems ,\\nvol. 36, 2024.\\n[214] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830 , 2019.\\n[215] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300 , 2020.\\n[216] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al. , “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168 , 2021.\\n[217] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\\nD. Song, and J. Steinhardt, “Measuring mathematical problem solving\\nwith the math dataset,” arXiv preprint arXiv:2103.03874 , 2021.\\n[218] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B.\\nClement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou,\\nM. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng,\\nS. Fu, and S. Liu, “Codexglue: A machine learning benchmark dataset\\nfor code understanding and generation,” CoRR , vol. abs/2102.04664,\\n2021.\\n[219] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R ´e, “Flashattention: Fast\\nand memory-efficient exact attention with io-awareness,” Advances in\\nNeural Information Processing Systems , vol. 35, pp. 16 344–16 359,\\n2022.\\n[220] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “Gptq: Accu-\\nrate post-training quantization for generative pre-trained transformers,”\\narXiv preprint arXiv:2210.17323 , 2022.\\n[221] H. Badri and A. Shaji, “Half-quadratic quantization of large\\nmachine learning models,” November 2023. [Online]. Available:\\nhttps://mobiusml.github.io/hqq blog/\\n[222] F. Gloeckle, B. Youbi Idrissi, B. Rozi `ere, D. Lopez-Paz, and G. Syn-\\nnaeve, “Better & Faster Large Language Models via Multi-token\\nPrediction,” arXiv e-prints , p. arXiv:2404.19737, Apr. 2024.\\n[223] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\\nregion policy optimization,” in Proceedings of the 32nd International\\nConference on Machine Learning , ser. Proceedings of Machine\\nLearning Research, F. Bach and D. Blei, Eds., vol. 37. Lille,\\nFrance: PMLR, 07–09 Jul 2015, pp. 1889–1897. [Online]. Available:\\nhttps://proceedings.mlr.press/v37/schulman15.html\\n[224] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\\n“Proximal policy optimization algorithms,” 2017.\\n[225] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning,\\nS. Ermon, and C. Finn, “Direct preference optimization: Your\\nlanguage model is secretly a reward model,” in Advances in\\nNeural Information Processing Systems , A. Oh, T. Naumann,\\nA. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,\\nvol. 36. Curran Associates, Inc., 2023, pp. 53 728–53 741.\\n[Online]. Available: https://proceedings.neurips.cc/paper files/paper/\\n2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf\\n[226] J. Hong, N. Lee, and J. Thorne, “Orpo: Monolithic preference opti-\\nmization without reference model,” 2024.\\n[227] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel, S. Riedel, and D. Kiela,\\n“Retrieval-augmented generation for knowledge-intensive nlp tasks,”\\ninAdvances in Neural Information Processing Systems , H. Larochelle,\\nM. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran\\nAssociates, Inc., 2020, pp. 9459–9474.\\n[228] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun,\\nM. Wang, and H. Wang, “Retrieval-augmented generation for large\\nlanguage models: A survey,” 2024.\\n49', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='73e1416e-c2de-4fa3-9db2-b0a0b25cc6f4', embedding=None, metadata={'page_label': '50', 'file_name': '2405.12750v1.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/2405.12750v1.pdf', 'file_type': 'application/pdf', 'file_size': 8849747, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[229] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and\\nC. Finn, “Direct preference optimization: Your language model is\\nsecretly a reward model,” 2023.\\n[230] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y . Geng, F. Fu, L. Yang, W. Zhang,\\nJ. Jiang, and B. Cui, “Retrieval-augmented generation for ai-generated\\ncontent: A survey,” 2024.\\n[231] Y . Huang and J. Huang, “A survey on retrieval-augmented text gener-\\nation for large language models,” 2024.\\n[232] M. team, “MLC-LLM,” 2023. [Online]. Available: https://github.com/\\nmlc-ai/mlc-llm\\n[233] ——, “MNN-LLM,” 2023. [Online]. Available: https://github.com/\\nwangzhaode/mnn-llm/\\n[234] L. Derczynski, E. Galinkin, and S. Majumdar, “garak: A Framework\\nfor Large Language Model Red Teaming,” https://garak.ai, 2024.\\n[235] N. Shazeer, “Fast transformer decoding: One write-head is all you\\nneed,” 2019.\\n[236] J. Ainslie, J. Lee-Thorp, M. de Jong, Y . Zemlyanskiy, F. Lebr ´on, and\\nS. Sanghai, “Gqa: Training generalized multi-query transformer models\\nfrom multi-head checkpoints,” 2023.\\n50', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5292ef5e-e5c6-428e-b103-b8f4e9f5370c', embedding=None, metadata={'page_label': '1', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e45ca0ee-cb68-4bff-89d3-461f7b414de8', embedding=None, metadata={'page_label': '2', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='77c772bd-fbba-4cdb-bd59-8e8abe0cc02d', embedding=None, metadata={'page_label': '3', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='75043e77-c4f6-45f3-afa5-52477b488dd7', embedding=None, metadata={'page_label': '4', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc87e2ae-8859-4c38-a348-a89a4075f14f', embedding=None, metadata={'page_label': '5', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6779f72d-c525-47cc-bc7b-19ff3475cb3a', embedding=None, metadata={'page_label': '6', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8f8cc1cc-1927-4cd6-a79a-87f2e060eb67', embedding=None, metadata={'page_label': '7', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='be2f948b-38f3-45e8-99a0-aa7a3def7503', embedding=None, metadata={'page_label': '8', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aab88e3d-c2a5-4869-9cba-6edd568c7ac9', embedding=None, metadata={'page_label': '9', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='34ca9069-ff6c-4019-987c-edcb9242077a', embedding=None, metadata={'page_label': '10', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='37593621-4822-4c50-ab22-65c9586ea8e8', embedding=None, metadata={'page_label': '11', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='036d1f53-1458-42e7-80be-01f51b7ca58a', embedding=None, metadata={'page_label': '12', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5fd1dbe0-490d-43bd-8fc8-4ae0604ca42d', embedding=None, metadata={'page_label': '13', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='09cabad7-2aef-4cc8-8a27-995dc9e17c26', embedding=None, metadata={'page_label': '14', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5c9eb13f-6971-48c4-8e16-950f6bb88cb5', embedding=None, metadata={'page_label': '15', 'file_name': 'Attention.pdf', 'file_path': '//Users/amitsarang/LlamaIndex/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-30', 'last_modified_date': '2024-09-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amitsarang/LlamaIndex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 65/65 [00:00<00:00, 195.92it/s]\n",
      "Generating embeddings: 100%|██████████| 121/121 [00:04<00:00, 26.69it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x169ee94e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index,similarity_top_k=4)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever,node_postprocessors=[postprocessor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"what is Generative AI and Large Language Models for Cyber Security: All Insights You Need is about.please explain in simple terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need proposes a new network architecture called the Transformer, which is based solely on attention mechanisms, eliminating the need for complex recurrent or convolutional neural networks. The Transformer model connects the encoder and decoder through an attention mechanism, achieving superior quality in tasks like machine translation while being more parallelizable and requiring less training time compared to existing models.\n"
     ]
    }
   ],
   "source": [
    "# response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Generative AI and Large Language Models for Cyber\n",
      "Security: All Insights You Need discusses how advanced technologies\n",
      "like Large Language Models (LLMs) and Generative AI can be used to\n",
      "enhance cybersecurity defenses. It explores the applications of LLMs\n",
      "in various cybersecurity areas such as hardware security, intrusion\n",
      "detection, malware detection, and more. The paper also covers the\n",
      "evolution of LLMs, vulnerabilities associated with them, mitigation\n",
      "strategies, performance evaluation of different LLM models, challenges\n",
      "in using LLMs for cybersecurity, and new strategies for leveraging\n",
      "LLMs effectively. Overall, it aims to provide a comprehensive\n",
      "understanding of integrating LLMs into cybersecurity frameworks to\n",
      "improve threat detection and response capabilities.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: e29eae44-2029-4b7f-8714-a5ff66cfceb4\n",
      "Similarity: 0.8676278220228448\n",
      "Text: Generative AI and Large Language Models for Cyber Security: All\n",
      "Insights You Need Mohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah,\n",
      "Bilel Cherif, Abdechakour Mechri, and Norbert Tihanyi Abstract —The\n",
      "rapid evolution of cyber threats requires in- novative approaches to\n",
      "enhance cybersecurity defenses. In this paper, we provide a\n",
      "comprehensive a...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 41b60099-9f6e-493f-a4cb-5007fe84e2aa\n",
      "Similarity: 0.847229790288911\n",
      "Text: Employing synthetic data generation via the OpenAI API allows\n",
      "for diverse cybersecurity scenarios, while advanced tools like Evol-\n",
      "Instruct [195] enhance dataset quality byadding complexity and\n",
      "removing outdated threats. Techniques such as regex filtering and\n",
      "removing near-duplicates ensure the data’s uniqueness and relevance.\n",
      "In addition, famili...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: ce1114da-0681-4941-9054-5dc002eea301\n",
      "Similarity: 0.8365791603129226\n",
      "Text: TABLE XV: LLM Cybersecurity Insights. Aspect Details\n",
      "Tools/Methods Applications Architecture Focus on model components such\n",
      "as tokenization, attention mechanisms, and output generation.•Paper:\n",
      "Attention Is All You NeedThreat Detection and Analysis, Security Au-\n",
      "tomation, Cyber Forensics, Penetration Test- ing, Security Training\n",
      "and Awareness, an...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: b6a6612a-dfc5-4838-ad17-6d32701f02aa\n",
      "Similarity: 0.8359519609375706\n",
      "Text: variant of RNN, was introduced in 1997, which addressed the\n",
      "vanishing gradient problem and allowed for longer-term memory in NLP\n",
      "models. GRU, another variant of RNN, was introduced in 2014, which\n",
      "reduced the number of parame- ters and improved computational\n",
      "efficiency [5]. The latest breakthrough in NLP was the introduction of\n",
      "Transformers in 20...\n",
      "Generative AI and Large Language Models for Cyber Security: All Insights You Need discusses how advanced technologies like Large Language Models (LLMs) and Generative AI can be used to enhance cybersecurity defenses. It explores the applications of LLMs in various cybersecurity areas such as hardware security, intrusion detection, malware detection, and more. The paper also covers the evolution of LLMs, vulnerabilities associated with them, mitigation strategies, performance evaluation of different LLM models, challenges in using LLMs for cybersecurity, and new strategies for leveraging LLMs effectively. Overall, it aims to provide a comprehensive understanding of integrating LLMs into cybersecurity frameworks to improve threat detection and response capabilities.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "pprint_response(response,show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persistant Director for Storing the data in vector form for retieval of complex data and increasing the effieciency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are a model architecture that relies entirely on an attention mechanism to draw global dependencies between input and output. They allow for significantly more parallelization compared to recurrent models, as they do not use recurrence and instead focus on self-attention to compute representations of input and output sequences.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "from llama_index.core import(\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "PERSIST_DIR = \"./storage\"\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    documents = SimpleDirectoryReader(\"//Users//amitsarang//LlamaIndex//data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are Transformers?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
